<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TestIQ Analysis Report</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f7fa;
            padding: 20px;
        }
        .container {
            max-width: 1600px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            padding: 40px;
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.5em;
        }
        .timestamp {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 40px;
        }
        .stat-card {
            background: linear-gradient(135deg, #4a5568 0%, #2d3748 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            cursor: pointer;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }
        .stat-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 12px rgba(0,0,0,0.15);
        }
        .stat-card.danger {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }
        .stat-card.success {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        }
        .stat-card.warning {
            background: linear-gradient(135deg, #f7971e 0%, #ffd200 100%);
        }
        .stat-card.info {
            background: linear-gradient(135deg, #00c6ff 0%, #0072ff 100%);
        }
        .stat-value {
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 5px;
        }
        .stat-label {
            font-size: 0.9em;
            opacity: 0.9;
        }
        h2 {
            color: #2c3e50;
            margin-top: 40px;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #00c6ff;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 30px;
            background: white;
        }
        th {
            background: #00c6ff;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
        }
        td {
            padding: 12px;
            border-bottom: 1px solid #ecf0f1;
            word-break: break-word;
            overflow-wrap: break-word;
            max-width: 400px;
        }
        tr:hover {
            background: #f8f9fa;
        }
        .badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .badge-danger {
            background: #fee;
            color: #c33;
        }
        .badge-warning {
            background: #ffeaa7;
            color: #d63031;
        }
        .badge-info {
            background: #dfe6e9;
            color: #2d3436;
        }
        .test-group {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 15px;
            border-left: 4px solid #00c6ff;
        }
        .test-name {
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            background: #ecf0f1;
            padding: 2px 6px;
            border-radius: 3px;
            display: inline-block;
            max-width: 100%;
            line-height: 1.6;
        }
        .test-name .test-part {
            display: inline;
        }
        .test-name .test-separator {
            color: #3498db;
            font-weight: bold;
            margin: 0 2px;
        }
        .action {
            color: #27ae60;
            font-weight: 600;
        }
        .footer {
            margin-top: 60px;
            padding-top: 20px;
            border-top: 1px solid #ecf0f1;
            text-align: center;
            color: #7f8c8d;
            font-size: 0.9em;
        }
        .clickable-row {
            cursor: pointer;
            transition: all 0.2s ease;
        }
        .clickable-row:hover {
            background: #e8f4f8 !important;
            transform: translateX(3px);
        }
        .modal {
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.7);
            animation: fadeIn 0.3s;
        }
        .modal-content {
            background-color: white;
            margin: 10px auto;
            padding: 0;
            width: calc(100% - 20px);
            height: calc(100vh - 20px);
            border-radius: 8px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.3);
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }
        .modal-header {
            padding: 20px;
            background: linear-gradient(135deg, #00c6ff 0%, #764ba2 100%);
            color: white;
            border-radius: 8px 8px 0 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-shrink: 0;
        }
        .close {
            color: white;
            font-size: 32px;
            font-weight: bold;
            cursor: pointer;
            line-height: 1;
            transition: transform 0.2s;
        }
        .close:hover {
            transform: scale(1.2);
        }
        .split-view {
            display: flex;
            flex: 1;
            overflow-y: auto;
            overflow-x: hidden;
            min-height: 0;
        }
        .split-view.independent {
            overflow: hidden;
        }
        .file-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            border-right: 2px solid #ecf0f1;
            min-width: 0;
        }
        .file-panel.independent {
            overflow-y: auto;
        }
        .file-panel:last-child {
            border-right: none;
        }
        .panel-header {
            padding: 15px;
            background: #f8f9fa;
            border-bottom: 2px solid #ecf0f1;
            font-weight: 600;
            color: #2c3e50;
            position: sticky;
            top: 0;
            z-index: 10;
            flex-shrink: 0;
        }
        .file-content {
            padding: 20px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.8;
            background: #fafafa;
        }
        .code-line {
            padding: 2px 8px;
            border-radius: 3px;
            margin: 1px 0;
            white-space: pre;
        }
        .covered-both {
            background: #c8e6c9;
            border-left: 3px solid #4caf50;
            font-weight: 600;
        }
        .covered-single {
            background: #fff9c4;
            border-left: 3px solid #fbc02d;
            font-weight: 500;
        }
        .covered {
            background: #d4edda;
            border-left: 3px solid #28a745;
            font-weight: 600;
        }
        .not-covered {
            opacity: 0.6;
        }
        .file-path {
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            color: #7f8c8d;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .coverage-info {
            background: #e8f4f8;
            padding: 15px;
            margin: 10px 20px;
            border-left: 4px solid #00c6ff;
            border-radius: 4px;
            display: flex;
            align-items: center;
            gap: 20px;
            flex-wrap: wrap;
            flex-shrink: 0;
        }
        .filter-section {
            display: flex;
            align-items: center;
            gap: 5px;
        }
        .filter-select {
            padding: 6px 10px;
            border: 1px solid rgba(255,255,255,0.5);
            border-radius: 4px;
            font-size: 0.85em;
            background: rgba(255,255,255,0.95);
            cursor: pointer;
            min-width: 180px;
            color: #2c3e50;
        }
        .filter-select:hover {
            background: white;
            border-color: white;
        }
        .sync-toggle {
            display: flex;
            align-items: center;
            gap: 6px;
            padding: 6px 12px;
            background: rgba(255,255,255,0.95);
            border: 1px solid rgba(255,255,255,0.5);
            border-radius: 4px;
            cursor: pointer;
            transition: all 0.2s;
            font-size: 0.85em;
            color: #2c3e50;
        }
        .sync-toggle:hover {
            background: white;
            border-color: white;
            background: #f0f0ff;
        }
        .sync-toggle.active {
            background: #00c6ff;
            color: white;
        }
        .sync-checkbox {
            width: 18px;
            height: 18px;
            cursor: pointer;
        }
            border-left: 4px solid #00c6ff;
            border-radius: 4px;
        }
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
        .progress-bar {
            height: 30px;
            background: #ecf0f1;
            border-radius: 15px;
            overflow: hidden;
            margin: 20px 0;
        }
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #00c6ff 0%, #0072ff 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: 600;
            transition: width 0.3s ease;
        }
        .tabs {
            display: flex;
            gap: 10px;
            margin: 30px 0 20px 0;
            border-bottom: 2px solid #ecf0f1;
        }
        .tab {
            padding: 12px 24px;
            background: #f8f9fa;
            border: none;
            border-radius: 8px 8px 0 0;
            cursor: pointer;
            font-weight: 600;
            font-size: 16px;
            color: #7f8c8d;
            transition: all 0.3s ease;
            position: relative;
            bottom: -2px;
        }
        .tab:hover {
            background: #e9ecef;
            color: #495057;
        }
        .tab.active {
            background: white;
            color: #00c6ff;
            border-bottom: 2px solid #00c6ff;
        }
        .tab-content {
            display: none;
            animation: fadeIn 0.3s;
        }
        .tab-content.active {
            display: block;
        }
        .pagination {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 10px;
            margin: 20px 0;
        }
        .pagination-controls {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-bottom: 20px;
            justify-content: space-between;
            flex-wrap: wrap;
        }
        .page-size-selector {
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 0.9em;
            color: #2c3e50;
        }
        .page-size-selector label {
            font-weight: 600;
        }
        .page-size-selector select {
            padding: 6px 12px;
            border: 2px solid #3498db;
            border-radius: 6px;
            background: white;
            color: #2c3e50;
            font-size: 0.9em;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        .page-size-selector select:hover {
            border-color: #00c6ff;
            box-shadow: 0 2px 8px rgba(0, 198, 255, 0.2);
        }
        .page-size-selector select:focus {
            outline: none;
            border-color: #00c6ff;
            box-shadow: 0 0 0 3px rgba(0, 198, 255, 0.1);
        }
        .page-btn {
            padding: 8px 12px;
            background: #00c6ff;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.2s ease;
        }
        .page-btn:hover:not(:disabled) {
            background: #0088cc;
            transform: translateY(-2px);
        }
        .page-btn:disabled {
            background: #bdc3c7;
            cursor: not-allowed;
            opacity: 0.6;
        }
        .page-info {
            color: #7f8c8d;
            font-weight: 600;
        }
        .loading {
            text-align: center;
            padding: 20px;
            color: #00c6ff;
        }
        .spinner {
            border: 3px solid #f3f3f3;
            border-top: 3px solid #00c6ff;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 20px auto;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        
        /* Responsive: Hide text on small screens, keep only icon */
        @media (max-width: 768px) {
            .view-coverage-text {
                display: none;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üß™ TestIQ Analysis Report</h1>
        <div class="timestamp">Generated on 2026-01-14 17:29:43</div>
        
        <div class="stats">
            <div class="stat-card" onclick="switchTab('exact')">
                <div class="stat-value">190</div>
                <div class="stat-label">Total Test Methods</div>
            </div>
            <div class="stat-card danger" onclick="switchTab('exact')">
                <div class="stat-value">0</div>
                <div class="stat-label">Duplicates</div>
            </div>
            <div class="stat-card info" onclick="switchTab('similar')">
                <div class="stat-value">446</div>
                <div class="stat-label">Similar Test Pairs</div>
            </div>
            <div class="stat-card warning" onclick="switchTab('subset')">
                <div class="stat-value">0</div>
                <div class="stat-label">Subset Duplicates</div>
            </div>
        </div>



        <div class="tabs">
            <button class="tab active" onclick="switchTab('exact')">üéØ Exact Duplicates (0)</button>
            <button class="tab" onclick="switchTab('similar')">üîç Similar Tests (446)</button>
            <button class="tab" onclick="switchTab('subset')">üìä Subset Duplicates (0)</button>
        </div>

        <div id="exact-content" class="tab-content active">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                <h2 style="margin: 0;">üéØ Exact Duplicates</h2>
                <div class="page-size-selector">
                    <label for="exact-page-size">Items per page:</label>
                    <select id="exact-page-size" onchange="changePageSize('exact', parseInt(this.value))">
                        <option value="10">10</option>
                        <option value="20" selected>20</option>
                        <option value="50">50</option>
                        <option value="100">100</option>
                        <option value="999999">All</option>
                    </select>
                </div>
            </div>
            <p>Tests with identical code coverage that can be safely removed.</p>
            <div class="pagination-controls">
                <div id="exact-pagination" class="pagination"></div>
            </div>
            <div id="exact-table"></div>
        </div>

        <div id="similar-content" class="tab-content">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                <h2 style="margin: 0;">üîç Similar Tests (‚â•30% overlap)</h2>
                <div class="page-size-selector">
                    <label for="similar-page-size">Items per page:</label>
                    <select id="similar-page-size" onchange="changePageSize('similar', parseInt(this.value))">
                        <option value="10">10</option>
                        <option value="20" selected>20</option>
                        <option value="50">50</option>
                        <option value="100">100</option>
                        <option value="999999">All</option>
                    </select>
                </div>
            </div>
            <p>Test pairs with significant code coverage overlap that may indicate redundancy.</p>
            <div class="pagination-controls">
                <div id="similar-pagination" class="pagination"></div>
            </div>
            <div id="similar-table"></div>
        </div>

        <div id="subset-content" class="tab-content">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                <h2 style="margin: 0;">üìä Subset Duplicates</h2>
                <div class="page-size-selector">
                    <label for="subset-page-size">Items per page:</label>
                    <select id="subset-page-size" onchange="changePageSize('subset', parseInt(this.value))">
                        <option value="10">10</option>
                        <option value="20" selected>20</option>
                        <option value="50">50</option>
                        <option value="100">100</option>
                        <option value="999999">All</option>
                    </select>
                </div>
            </div>
            <p>Tests that are subsets of other tests and may be redundant.</p>
            <div class="pagination-controls">
                <div id="subset-pagination" class="pagination"></div>
            </div>
            <div id="subset-table"></div>
        </div>

        <script>
        // Utility function for escaping HTML to prevent XSS
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        // Utility function to format test names with separators
        function formatTestName(testName) {
            // Split test name at :: for better readability
            const parts = testName.split('::');
            if (parts.length === 1) return testName;
            
            return parts.map((part, idx) => {
                if (idx === parts.length - 1) {
                    return '<span class="test-part">' + part + '</span>';
                }
                return '<span class="test-part">' + part + '</span><span class="test-separator">::</span><wbr>';
            }).join('');
        }
        
        // Data for pagination
        // Note: Coverage data is limited to first 20 items for similar and subset
        const maxSimilar = 20;
        const maxSubset = 20;
        const exactDupsData = [];
        const similarData = [["tests/test_cli.py::TestCLI::test_analyze_output_formats", "tests/test_cli.py::TestCLIIntegration::test_cli_workflow_with_thresholds", 0.6540178571428571, 0], ["tests/test_analysis.py::TestRecommendationEngine::test_recommendation_engine_scenarios", "tests/test_analysis.py::TestRecommendationEngine::test_comprehensive_recommendation_workflow", 0.6538461538461539, 1], ["tests/test_cli.py::TestCLI::test_analyze_command_variations", "tests/test_cli.py::TestCLIConfig::test_custom_config_file", 0.6484560570071259, 2], ["tests/test_cli.py::TestCLI::test_analyze_markdown_format", "tests/test_cli.py::TestCLIIntegration::test_cli_workflow_with_thresholds", 0.6437346437346437, 3], ["tests/test_cli.py::TestCLIQualityGate::test_quality_gate_pass_and_fail", "tests/test_cli.py::TestCLIConfig::test_custom_config_file", 0.6271186440677966, 4], ["tests/test_plugins.py::TestPluginManager::test_trigger_hook", "tests/test_plugins.py::TestPluginManager::test_trigger_multiple_hooks", 0.625, 5], ["tests/test_cli.py::TestCLI::test_analyze_command_variations", "tests/test_cli.py::TestCLIQualityGate::test_quality_gate_pass_and_fail", 0.6242424242424243, 6], ["tests/test_analysis.py::TestQualityAnalyzer::test_quality_scores_across_all_grades", "tests/test_analysis.py::TestRecommendationEngine::test_comprehensive_recommendation_workflow", 0.6229508196721312, 7], ["tests/test_cicd.py::TestBaselineManager::test_save_and_load_baseline", "tests/test_cicd.py::TestBaselineManager::test_list_baselines", 0.6144578313253012, 8], ["tests/test_analysis.py::TestQualityAnalyzer::test_quality_scores_across_all_grades", "tests/test_analysis.py::TestRecommendationEngine::test_recommendation_engine_scenarios", 0.6047297297297297, 9], ["tests/test_security.py::TestValidateFilePath::test_dangerous_path_patterns", "tests/test_security.py::TestValidateFilePath::test_dangerous_pattern_windows_backslash", 0.6, 10], ["tests/test_cli.py::TestCLIIntegration::test_cli_workflow_with_thresholds", "tests/test_cli.py::TestCLIConfig::test_custom_config_file", 0.5994897959183674, 11], ["tests/test_plugins.py::TestGlobalFunctions::test_hook_with_data", "tests/test_plugins.py::TestPluginIntegration::test_conditional_hook_execution", 0.59375, 12], ["tests/test_security.py::TestValidateCoverageData::test_non_string_test_name", "tests/test_security.py::TestValidateCoverageData::test_empty_test_name", 0.5925925925925926, 13], ["tests/test_cli.py::TestCLI::test_analyze_markdown_format", "tests/test_cli.py::TestCLIConfig::test_custom_config_file", 0.5894736842105263, 14], ["tests/test_cli.py::TestCLI::test_analyze_output_formats", "tests/test_cli.py::TestCLI::test_analyze_markdown_format", 0.588495575221239, 15], ["tests/test_security.py::TestValidateCoverageData::test_empty_test_name", "tests/test_security.py::TestValidateCoverageData::test_non_dict_coverage", 0.5862068965517241, 16], ["tests/test_plugins.py::TestPluginManager::test_trigger_hook", "tests/test_plugins.py::TestPluginManager::test_hook_error_handling", 0.5833333333333334, 17], ["tests/test_plugins.py::TestPluginManager::test_unregister_hook", "tests/test_plugins.py::TestPluginManager::test_clear_hooks", 0.58, 18], ["tests/test_plugins.py::TestPluginManager::test_trigger_multiple_hooks", "tests/test_plugins.py::TestPluginManager::test_hook_error_handling", 0.5737704918032787, 19]];
        const subsetData = [];
        
        // Build coverage data per file
        const coverageByFile = {};
        coverageByFile["/Users/kkotari/github/TestIQ/tests/test_coverage_converter.py"] = {lines: 8, tests: 1};
        coverageByFile["src/testiq/analysis.py"] = {lines: 142, tests: 6};
        coverageByFile["src/testiq/analyzer.py"] = {lines: 137, tests: 30};
        coverageByFile["src/testiq/cicd.py"] = {lines: 142, tests: 19};
        coverageByFile["src/testiq/cli.py"] = {lines: 278, tests: 13};
        coverageByFile["src/testiq/config.py"] = {lines: 86, tests: 19};
        coverageByFile["src/testiq/coverage_converter.py"] = {lines: 62, tests: 13};
        coverageByFile["src/testiq/exceptions.py"] = {lines: 14, tests: 25};
        coverageByFile["src/testiq/logging_config.py"] = {lines: 34, tests: 44};
        coverageByFile["src/testiq/performance.py"] = {lines: 102, tests: 67};
        coverageByFile["src/testiq/plugins.py"] = {lines: 50, tests: 14};
        coverageByFile["src/testiq/pytest_plugin.py"] = {lines: 2, tests: 189};
        coverageByFile["src/testiq/reporting.py"] = {lines: 209, tests: 9};
        coverageByFile["src/testiq/security.py"] = {lines: 82, tests: 45};
        coverageByFile["src/testiq/source_reader.py"] = {lines: 22, tests: 17};
        coverageByFile["tests/test_analysis.py"] = {lines: 143, tests: 7};
        coverageByFile["tests/test_analyzer.py"] = {lines: 80, tests: 7};
        coverageByFile["tests/test_cicd.py"] = {lines: 186, tests: 16};
        coverageByFile["tests/test_cli.py"] = {lines: 251, tests: 15};
        coverageByFile["tests/test_coverage_converter.py"] = {lines: 130, tests: 13};
        coverageByFile["tests/test_enterprise.py"] = {lines: 85, tests: 13};
        coverageByFile["tests/test_init.py"] = {lines: 28, tests: 7};
        coverageByFile["tests/test_performance.py"] = {lines: 225, tests: 37};
        coverageByFile["tests/test_plugins.py"] = {lines: 149, tests: 14};
        coverageByFile["tests/test_reporting.py"] = {lines: 121, tests: 7};
        coverageByFile["tests/test_security.py"] = {lines: 197, tests: 39};
        coverageByFile["tests/test_source_reader.py"] = {lines: 100, tests: 14};
        
        let itemsPerPage = { exact: 20, similar: 20, subset: 20 };
        let currentPages = { exact: 1, similar: 1, subset: 1, coverage: 1 };
        
        function changePageSize(type, newSize) {
            itemsPerPage[type] = newSize;
            currentPages[type] = 1; // Reset to first page
            
            // Re-render the appropriate section
            if (type === 'exact') {
                renderExactDuplicates(1);
            } else if (type === 'similar') {
                renderSimilarTests(1);
            } else if (type === 'subset') {
                renderSubsetDuplicates(1);
            }
        }
        
        function truncateTestName(testName) {
            if (!testName || typeof testName !== 'string') {
                return '';
            }
            
            // Extract just the meaningful parts of the test name
            const parts = testName.split('::');
            if (parts.length <= 2) {
                return testName;
            }
            
            try {
                // Get the file name (without path)
                const filePart = parts[0].split('/').pop() || parts[0];
                // Get the class name (if exists) and test name
                const classPart = parts.length > 2 ? parts[parts.length - 2] : '';
                const testPart = parts[parts.length - 1];
                
                // Format: FileName::Class::test_name
                if (classPart) {
                    return filePart + '::' + classPart + '::' + testPart;
                } else {
                    return filePart + '::' + testPart;
                }
            } catch (e) {
                console.error('Error truncating test name:', e);
                return testName;
            }
        }
        
        function switchTab(tabName) {
            // Hide all tabs
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
            
            // Show selected tab
            event.target.classList.add('active');
            document.getElementById(tabName + '-content').classList.add('active');
        }
        
        function renderExactDuplicates(page) {
            const pageSize = itemsPerPage['exact'];
            const start = (page - 1) * pageSize;
            const end = start + pageSize;
            const pageData = exactDupsData.slice(start, end);
            
            let html = '';
            if (pageData.length === 0) {
                html = '<p style="color: #27ae60; text-align: center; padding: 20px;">‚úì No exact duplicates found!</p>';
            } else {
                html = `
                <table>
                    <thead>
                        <tr>
                            <th>Group</th>
                            <th>Tests</th>
                            <th>Count</th>
                            <th>Action</th>
                        </tr>
                    </thead>
                    <tbody>`;
                
                pageData.forEach(([group, coverageIdx], idx) => {
                    const groupNum = start + idx + 1;
                    const testList = group.map(test => {
                        if (!test) return '';
                        const truncated = truncateTestName(test);
                        return `<span class="test-name" title="${escapeHtml(test)}" style="cursor: help;">${escapeHtml(truncated)}</span>`;
                    }).filter(t => t).join('<br>');
                    html += `
                        <tr class="clickable-row" onclick="showComparison(${coverageIdx})">
                            <td><strong>Group ${groupNum}</strong></td>
                            <td>${testList}</td>
                            <td><span class="badge badge-danger">${group.length} tests</span></td>
                            <td><span style="color: #00c6ff; font-weight: 600;">üîç <span class="view-coverage-text">View Coverage</span></span></td>
                        </tr>`;
                });
                
                html += '</tbody></table>';
            }
            
            document.getElementById('exact-table').innerHTML = html;
            renderPagination('exact', exactDupsData.length, page, pageSize);
            formatTestNames();
        }
        
        function renderSimilarTests(page) {
            const pageSize = itemsPerPage['similar'];
            const start = (page - 1) * pageSize;
            const end = start + pageSize;
            const pageData = similarData.slice(start, end);
            
            let html = '';
            if (pageData.length === 0) {
                html = '<p style="color: #27ae60; text-align: center; padding: 20px;">‚úì No similar tests found!</p>';
            } else {
                html = `
                <table>
                    <thead>
                        <tr>
                            <th>Test 1</th>
                            <th>Test 2</th>
                            <th>Similarity</th>
                            <th>Action</th>
                        </tr>
                    </thead>
                    <tbody>`;
                
                pageData.forEach(([test1, test2, similarity, coverageIdx]) => {
                    if (!test1 || !test2) return;
                    const t1 = escapeHtml(truncateTestName(test1));
                    const t2 = escapeHtml(truncateTestName(test2));
                    const simPercent = (similarity * 100).toFixed(1);
                    html += `
                        <tr class="clickable-row" onclick="showComparison(${coverageIdx})">
                            <td><span class="test-name" title="${escapeHtml(test1)}" style="cursor: help;">${t1}</span></td>
                            <td><span class="test-name" title="${escapeHtml(test2)}" style="cursor: help;">${t2}</span></td>
                            <td><span class="badge badge-info">${simPercent}%</span></td>
                            <td><span style="color: #00c6ff; font-weight: 600;">üîç <span class="view-coverage-text">View Coverage</span></span></td>
                        </tr>`;
                });
                
                html += '</tbody></table>';
            }
            
            document.getElementById('similar-table').innerHTML = html;
            renderPagination('similar', similarData.length, page, pageSize);
            formatTestNames();
        }
        
        function renderSubsetDuplicates(page) {
            const pageSize = itemsPerPage['subset'];
            const start = (page - 1) * pageSize;
            const end = start + pageSize;
            const pageData = subsetData.slice(start, end);
            
            let html = '';
            if (pageData.length === 0) {
                html = '<p style="color: #27ae60; text-align: center; padding: 20px;">‚úì No subset duplicates found!</p>';
            } else {
                html = `
                <table>
                    <thead>
                        <tr>
                            <th>Subset Test</th>
                            <th>Superset Test</th>
                            <th>Coverage Ratio</th>
                            <th>Action</th>
                        </tr>
                    </thead>
                    <tbody>`;
                
                pageData.forEach(([subsetTest, supersetTest, ratio, coverageIdx]) => {
                    if (!subsetTest || !supersetTest) return;
                    const sub = escapeHtml(truncateTestName(subsetTest));
                    const sup = escapeHtml(truncateTestName(supersetTest));
                    const ratioPercent = (ratio * 100).toFixed(1);
                    html += `
                        <tr class="clickable-row" onclick="showComparison(${coverageIdx})">
                            <td><span class="test-name" title="${escapeHtml(subsetTest)}" style="cursor: help;">${sub}</span></td>
                            <td><span class="test-name" title="${escapeHtml(supersetTest)}" style="cursor: help;">${sup}</span></td>
                            <td><span class="badge badge-warning">${ratioPercent}%</span></td>
                            <td><span style="color: #00c6ff; font-weight: 600;">üîç <span class="view-coverage-text">View Coverage</span></span></td>
                        </tr>`;
                });
                
                html += '</tbody></table>';
            }
            
            document.getElementById('subset-table').innerHTML = html;
            renderPagination('subset', subsetData.length, page, pageSize);
            formatTestNames();
        }
        
        function renderPagination(type, totalItems, currentPage, pageSize) {
            const totalPages = Math.ceil(totalItems / pageSize);
            
            if (totalPages <= 1) {
                document.getElementById(type + '-pagination').innerHTML = '';
                return;
            }
            
            const start = (currentPage - 1) * pageSize + 1;
            const end = Math.min(currentPage * pageSize, totalItems);
            
            let html = '<button class="page-btn" onclick="changePage(\'' + type + '\', ' + (currentPage - 1) + ')" ' +
                (currentPage === 1 ? 'disabled' : '') + '>‚Üê Previous</button>' +
                '<span class="page-info">' + start + '-' + end + ' of ' + totalItems + ' | Page ' + currentPage + '/' + totalPages + '</span>' +
                '<button class="page-btn" onclick="changePage(\'' + type + '\', ' + (currentPage + 1) + ')" ' +
                (currentPage === totalPages ? 'disabled' : '') + '>Next ‚Üí</button>';
            
            document.getElementById(type + '-pagination').innerHTML = html;
        }
        
        function changePage(type, newPage) {
            currentPages[type] = newPage;
            
            if (type === 'exact') {
                renderExactDuplicates(newPage);
            } else if (type === 'similar') {
                renderSimilarTests(newPage);
            } else if (type === 'subset') {
                renderSubsetDuplicates(newPage);
            }
            
            // Scroll to top of table
            document.getElementById(type + '-content').scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
        
        function formatTestNames() {
            const testNames = document.querySelectorAll('.test-name');
            testNames.forEach(el => {
                const originalText = el.textContent;
                if (originalText.includes('::')) {
                    el.innerHTML = formatTestName(originalText);
                }
            });
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', function() {
            renderExactDuplicates(1);
            renderSimilarTests(1);
            renderSubsetDuplicates(1);
        });
        </script>

        <!-- Modal for split-screen coverage view -->
        <div id="comparisonModal" class="modal">
            <div class="modal-content">
                <div class="modal-header">
                    <h2 style="margin: 0;">üìä Coverage Comparison: Execution Paths</h2>
                    <div style="display: flex; align-items: center; gap: 15px;">
                        <div class="filter-section">
                            <label for="fileFilter" style="font-weight: 600; margin-right: 8px;">üìÅ</label>
                            <select id="fileFilter" class="filter-select" onchange="applyFileFilter()">
                                <option value="">All Files</option>
                            </select>
                        </div>
                        <div class="sync-toggle" id="syncToggle" onclick="toggleSync()">
                            <input type="checkbox" id="syncCheckbox" class="sync-checkbox" checked>
                            <label for="syncCheckbox" style="cursor: pointer; user-select: none;">üîó Sync Scroll</label>
                        </div>
                        <span class="close" onclick="closeModal()">&times;</span>
                    </div>
                </div>
                <div style="background: #e3f2fd; border-left: 4px solid #2196F3; padding: 12px 16px; margin: 0 20px 16px; border-radius: 4px; font-size: 14px;">
                    <strong>‚ÑπÔ∏è Note:</strong> Source code is identical. Highlighting shows <strong>which lines each test executed</strong>. 
                    Different execution paths are normal due to conditional branches (if/elif/else), early returns, and functions called with different parameters.
                </div>
                <div class="coverage-info">
                    <div>
                        <strong>Subset Test:</strong> <span id="subsetName" class="test-name"></span>
                        &nbsp;&nbsp;|&nbsp;&nbsp;
                        <strong>Superset Test:</strong> <span id="supersetName" class="test-name"></span>
                        &nbsp;&nbsp;|&nbsp;&nbsp;
                        <strong>Coverage Ratio:</strong> <span id="coverageRatio" class="badge badge-warning"></span>
                    </div>
                    <div style="margin-top: 10px; padding: 8px; background: #f5f5f5; border-radius: 4px; display: inline-flex; gap: 20px; font-size: 13px;">
                        <span><span style="display: inline-block; width: 16px; height: 16px; background: #c8e6c9; border-radius: 3px; vertical-align: middle;"></span> Both tests executed</span>
                        <span><span style="display: inline-block; width: 16px; height: 16px; background: #fff9c4; border-radius: 3px; vertical-align: middle;"></span> Only one test executed</span>
                        <span><span style="display: inline-block; width: 16px; height: 16px; background: #ffffff; border: 1px solid #ddd; border-radius: 3px; vertical-align: middle;"></span> Neither test executed</span>
                    </div>
                </div>
                <div class="split-view">
                    <div class="file-panel">
                        <div class="panel-header">üìÑ Subset Test Coverage</div>
                        <div id="subsetContent" class="file-content"></div>
                    </div>
                    <div class="file-panel">
                        <div class="panel-header">üìÑ Superset Test Coverage</div>
                        <div id="supersetContent" class="file-content"></div>
                    </div>
                </div>
            </div>
        </div>

        <script>
        const coverageData = [{"subset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 270, 273, 274, 275, 277, 279, 281, 282, 283, 284, 285, 286, 287, 289, 290, 292, 294, 295, 296, 298, 300, 302, 303, 304, 305, 307, 324, 326, 330, 334], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 191, 198, 199, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 261, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142, 149, 163, 164, 167, 168, 169, 173, 186], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 112, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143, 144, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 169, 170, 171, 172, 174, 175], "src/testiq/reporting.py": [1590, 1592, 1664, 1672, 1673, 1675, 1676, 1678, 1679, 1680, 1681, 1683, 1684, 1685, 1687, 1688, 1691, 1692, 1693, 1694, 1695, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1717, 1718, 1719, 1721, 1724, 1725, 1726, 1727, 1729], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "superset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 279, 281, 282, 283, 284, 285, 286, 287, 289, 290, 292, 294, 296, 298, 300, 302, 303, 304, 305, 324, 326, 330, 334], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142, 149, 163, 164, 167, 168, 169, 173, 186], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 229, 232, 233, 234, 235, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 284], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "ratio": 0.6540178571428571, "subsetName": "tests/test_cli.py::TestCLI::test_analyze_output_formats", "supersetName": "tests/test_cli.py::TestCLIIntegration::test_cli_workflow_with_thresholds"}, {"subset": {"src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 166, 170, 171, 173, 174, 175, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "src/testiq/analysis.py": [43, 45, 47, 57, 59, 60, 61, 63, 64, 74, 77, 78, 81, 82, 85, 86, 89, 90, 91, 92, 96, 98, 100, 102, 103, 104, 107, 110, 111, 114, 115, 116, 117, 118, 119, 120, 123, 124, 126, 130, 133, 134, 135, 136, 147, 151, 152, 154, 155, 160, 161, 164, 168, 169, 180, 186, 191, 194, 199, 204, 212, 215, 216, 217, 218, 219, 222, 237, 239, 240, 242, 252, 254, 255, 256, 257, 258, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 292, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319], "tests/test_analysis.py": [28, 30, 33, 34, 37, 38, 40, 144, 147, 148, 151, 154, 155, 158, 159, 160, 163, 164, 167, 168, 169, 170, 171, 173, 174, 176, 177, 178], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "tests/test_analysis.py": [12, 14, 17, 18, 19, 20, 22, 24, 44, 46, 49, 50, 51, 54, 55, 57, 180, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228], "src/testiq/analysis.py": [43, 45, 47, 57, 59, 60, 61, 63, 64, 74, 77, 78, 81, 82, 85, 86, 89, 90, 91, 92, 96, 97, 98, 100, 101, 102, 104, 107, 110, 111, 114, 115, 116, 117, 118, 119, 120, 123, 124, 126, 130, 131, 133, 134, 135, 136, 138, 139, 140, 147, 151, 152, 164, 168, 169, 180, 186, 187, 188, 191, 194, 199, 204, 212, 215, 216, 217, 218, 219, 222, 237, 239, 240, 242, 252, 254, 255, 256, 257, 258, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 278, 292, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.6538461538461539, "subsetName": "tests/test_analysis.py::TestRecommendationEngine::test_recommendation_engine_scenarios", "supersetName": "tests/test_analysis.py::TestRecommendationEngine::test_comprehensive_recommendation_workflow"}, {"subset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 110, 123, 126, 128, 129, 130, 131, 140, 143, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 255, 256, 259, 260, 263, 266, 268, 269, 270, 273, 274], "src/testiq/cli.py": [72, 81, 82, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 105, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 239, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 256, 257, 258, 261, 269, 277, 309, 320, 321, 322, 324, 326, 330, 334, 340, 342, 343, 344, 345, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 362, 363, 366, 367, 384, 385, 386, 387, 388, 389, 391, 392, 394, 398, 399, 402, 403, 404, 405, 407, 408, 409, 410, 411, 413], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 261, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 57, 60, 61, 64, 65, 68, 69, 72, 73, 74, 75, 77, 78, 81, 82, 83, 85, 88, 89, 90, 92, 93, 96, 104, 105, 106, 107, 108, 110], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 80, 82, 83, 84, 85, 86, 87, 92, 93, 94, 95, 97, 98, 100], "src/testiq/cicd.py": [52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 209, 211, 212, 213, 215, 217, 219, 220, 222, 223]}, "superset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 110, 123, 126, 128, 129, 130, 131, 140, 143, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 254, 255, 256, 259, 260, 263, 266, 268, 269, 270, 273, 274], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 263, 264, 266, 267, 268, 269], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 105, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 309, 320, 322, 324, 326, 330, 334, 340, 342, 343, 344, 345, 350, 351, 366, 367, 384, 385, 402, 403, 404, 405, 407, 408, 409, 410, 411, 413], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100], "tests/test_cli.py": [12, 14, 393, 395, 403, 404, 405, 407, 408, 410], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.6484560570071259, "subsetName": "tests/test_cli.py::TestCLI::test_analyze_command_variations", "supersetName": "tests/test_cli.py::TestCLIConfig::test_custom_config_file"}, {"subset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 309, 310, 312, 313, 314, 315, 324, 326, 330, 334], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142, 149, 163, 164, 167, 168, 169, 173, 186], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 177, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 193, 194, 196, 197], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 191, 198, 199, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 261, 263, 264, 266, 267, 268, 269, 275, 285, 286, 288, 289, 290, 291, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 308, 309, 310, 312, 318, 322, 323, 324, 326, 327, 328, 330, 334, 335, 336, 337, 339, 340, 342], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "superset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 279, 281, 282, 283, 284, 285, 286, 287, 289, 290, 292, 294, 296, 298, 300, 302, 303, 304, 305, 324, 326, 330, 334], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142, 149, 163, 164, 167, 168, 169, 173, 186], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 229, 232, 233, 234, 235, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 284], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "ratio": 0.6437346437346437, "subsetName": "tests/test_cli.py::TestCLI::test_analyze_markdown_format", "supersetName": "tests/test_cli.py::TestCLIIntegration::test_cli_workflow_with_thresholds"}, {"subset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 215, 216, 217, 219, 222, 223, 227, 229, 230, 231, 232, 233, 235, 238, 261, 269, 277, 309, 320, 322, 324, 326, 330, 334, 340, 342, 343, 344, 345, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 362, 363, 366, 367, 384, 385, 402, 403, 404, 405, 407, 408, 409, 410, 411, 413], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142], "src/testiq/cicd.py": [29, 31, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 85, 87, 88, 90, 107, 109, 110, 111, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 135, 136, 138, 139, 141, 144, 153, 162, 171, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 198, 199, 201, 203], "src/testiq/pytest_plugin.py": [138, 141], "tests/test_cli.py": [12, 14, 319, 322, 323, 324, 325, 327, 328, 329, 330, 332, 333, 336, 337, 338, 340, 341, 342, 343, 345, 346], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "superset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 110, 123, 126, 128, 129, 130, 131, 140, 143, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 254, 255, 256, 259, 260, 263, 266, 268, 269, 270, 273, 274], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 263, 264, 266, 267, 268, 269], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 105, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 309, 320, 322, 324, 326, 330, 334, 340, 342, 343, 344, 345, 350, 351, 366, 367, 384, 385, 402, 403, 404, 405, 407, 408, 409, 410, 411, 413], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100], "tests/test_cli.py": [12, 14, 393, 395, 403, 404, 405, 407, 408, 410], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.6271186440677966, "subsetName": "tests/test_cli.py::TestCLIQualityGate::test_quality_gate_pass_and_fail", "supersetName": "tests/test_cli.py::TestCLIConfig::test_custom_config_file"}, {"subset": {"src/testiq/logging_config.py": [25, 27, 33, 35, 38], "src/testiq/plugins.py": [39, 41, 42, 44, 59, 60, 81, 92, 95, 96, 99, 101, 102, 103, 114, 121, 125, 126, 127, 139, 142, 144, 176, 178], "tests/test_plugins.py": [20, 22, 23, 24, 101, 103, 104, 106, 107, 109, 110, 112, 113, 114], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"tests/test_plugins.py": [20, 22, 23, 24, 116, 118, 119, 121, 122, 124, 125, 127, 128, 129, 131], "src/testiq/logging_config.py": [25, 27, 33, 35, 38], "src/testiq/plugins.py": [39, 41, 42, 44, 59, 60, 81, 92, 95, 96, 99, 101, 102, 103, 114, 121, 125, 126, 127, 139, 142, 144, 176, 178], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.625, "subsetName": "tests/test_plugins.py::TestPluginManager::test_trigger_hook", "supersetName": "tests/test_plugins.py::TestPluginManager::test_trigger_multiple_hooks"}, {"subset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 110, 123, 126, 128, 129, 130, 131, 140, 143, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 255, 256, 259, 260, 263, 266, 268, 269, 270, 273, 274], "src/testiq/cli.py": [72, 81, 82, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 105, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 239, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 256, 257, 258, 261, 269, 277, 309, 320, 321, 322, 324, 326, 330, 334, 340, 342, 343, 344, 345, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 362, 363, 366, 367, 384, 385, 386, 387, 388, 389, 391, 392, 394, 398, 399, 402, 403, 404, 405, 407, 408, 409, 410, 411, 413], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 261, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 57, 60, 61, 64, 65, 68, 69, 72, 73, 74, 75, 77, 78, 81, 82, 83, 85, 88, 89, 90, 92, 93, 96, 104, 105, 106, 107, 108, 110], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 80, 82, 83, 84, 85, 86, 87, 92, 93, 94, 95, 97, 98, 100], "src/testiq/cicd.py": [52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 209, 211, 212, 213, 215, 217, 219, 220, 222, 223]}, "superset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 215, 216, 217, 219, 222, 223, 227, 229, 230, 231, 232, 233, 235, 238, 261, 269, 277, 309, 320, 322, 324, 326, 330, 334, 340, 342, 343, 344, 345, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 362, 363, 366, 367, 384, 385, 402, 403, 404, 405, 407, 408, 409, 410, 411, 413], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142], "src/testiq/cicd.py": [29, 31, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 85, 87, 88, 90, 107, 109, 110, 111, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 135, 136, 138, 139, 141, 144, 153, 162, 171, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 198, 199, 201, 203], "src/testiq/pytest_plugin.py": [138, 141], "tests/test_cli.py": [12, 14, 319, 322, 323, 324, 325, 327, 328, 329, 330, 332, 333, 336, 337, 338, 340, 341, 342, 343, 345, 346], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "ratio": 0.6242424242424243, "subsetName": "tests/test_cli.py::TestCLI::test_analyze_command_variations", "supersetName": "tests/test_cli.py::TestCLIQualityGate::test_quality_gate_pass_and_fail"}, {"subset": {"src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 166, 170, 171, 173, 174, 175, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "tests/test_analysis.py": [12, 14, 17, 18, 19, 20, 22, 24, 28, 30, 33, 34, 37, 38, 40, 44, 46, 49, 50, 51, 54, 55, 57, 95, 98, 99, 100, 101, 102, 103, 104, 105, 106, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 123, 126], "src/testiq/analysis.py": [43, 45, 47, 57, 59, 60, 61, 63, 64, 74, 77, 78, 81, 82, 85, 86, 89, 90, 91, 92, 96, 97, 98, 100, 101, 102, 104, 107, 110, 111, 114, 115, 116, 117, 118, 119, 120, 123, 124, 126, 130, 131, 133, 134, 135, 136, 138, 139, 140, 147, 151, 152, 154, 155, 160, 161, 164, 168, 169, 180, 186, 187, 188, 191, 194, 199, 204, 212, 215, 216, 217, 218, 219, 222], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "tests/test_analysis.py": [12, 14, 17, 18, 19, 20, 22, 24, 44, 46, 49, 50, 51, 54, 55, 57, 180, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228], "src/testiq/analysis.py": [43, 45, 47, 57, 59, 60, 61, 63, 64, 74, 77, 78, 81, 82, 85, 86, 89, 90, 91, 92, 96, 97, 98, 100, 101, 102, 104, 107, 110, 111, 114, 115, 116, 117, 118, 119, 120, 123, 124, 126, 130, 131, 133, 134, 135, 136, 138, 139, 140, 147, 151, 152, 164, 168, 169, 180, 186, 187, 188, 191, 194, 199, 204, 212, 215, 216, 217, 218, 219, 222, 237, 239, 240, 242, 252, 254, 255, 256, 257, 258, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 278, 292, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.6229508196721312, "subsetName": "tests/test_analysis.py::TestQualityAnalyzer::test_quality_scores_across_all_grades", "supersetName": "tests/test_analysis.py::TestRecommendationEngine::test_comprehensive_recommendation_workflow"}, {"subset": {"src/testiq/cicd.py": [52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 209, 211, 212, 213, 215, 217, 219, 220, 222, 223, 225, 227, 229, 233, 234, 236, 237], "tests/test_cicd.py": [41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 125, 127, 128, 131, 134, 136, 137, 138, 139], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/cicd.py": [52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 209, 211, 212, 213, 215, 217, 219, 220, 222, 223, 225, 227, 229, 233, 234, 236, 237, 239, 241, 242, 243, 244, 245, 246, 247, 248, 253, 254], "tests/test_cicd.py": [41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 149, 151, 152, 155, 156, 159, 161, 162, 163, 164, 166, 167], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.6144578313253012, "subsetName": "tests/test_cicd.py::TestBaselineManager::test_save_and_load_baseline", "supersetName": "tests/test_cicd.py::TestBaselineManager::test_list_baselines"}, {"subset": {"src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 166, 170, 171, 173, 174, 175, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "tests/test_analysis.py": [12, 14, 17, 18, 19, 20, 22, 24, 28, 30, 33, 34, 37, 38, 40, 44, 46, 49, 50, 51, 54, 55, 57, 95, 98, 99, 100, 101, 102, 103, 104, 105, 106, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 123, 126], "src/testiq/analysis.py": [43, 45, 47, 57, 59, 60, 61, 63, 64, 74, 77, 78, 81, 82, 85, 86, 89, 90, 91, 92, 96, 97, 98, 100, 101, 102, 104, 107, 110, 111, 114, 115, 116, 117, 118, 119, 120, 123, 124, 126, 130, 131, 133, 134, 135, 136, 138, 139, 140, 147, 151, 152, 154, 155, 160, 161, 164, 168, 169, 180, 186, 187, 188, 191, 194, 199, 204, 212, 215, 216, 217, 218, 219, 222], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 166, 170, 171, 173, 174, 175, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "src/testiq/analysis.py": [43, 45, 47, 57, 59, 60, 61, 63, 64, 74, 77, 78, 81, 82, 85, 86, 89, 90, 91, 92, 96, 98, 100, 102, 103, 104, 107, 110, 111, 114, 115, 116, 117, 118, 119, 120, 123, 124, 126, 130, 133, 134, 135, 136, 147, 151, 152, 154, 155, 160, 161, 164, 168, 169, 180, 186, 191, 194, 199, 204, 212, 215, 216, 217, 218, 219, 222, 237, 239, 240, 242, 252, 254, 255, 256, 257, 258, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 292, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319], "tests/test_analysis.py": [28, 30, 33, 34, 37, 38, 40, 144, 147, 148, 151, 154, 155, 158, 159, 160, 163, 164, 167, 168, 169, 170, 171, 173, 174, 176, 177, 178], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.6047297297297297, "subsetName": "tests/test_analysis.py::TestQualityAnalyzer::test_quality_scores_across_all_grades", "supersetName": "tests/test_analysis.py::TestRecommendationEngine::test_recommendation_engine_scenarios"}, {"subset": {"src/testiq/exceptions.py": [10, 11, 12, 13, 15, 16, 36, 37], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 46, 62], "tests/test_security.py": [64, 67, 68, 69, 72, 73, 74], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/exceptions.py": [10, 11, 12, 13, 15, 16, 36, 37], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 46, 62], "tests/test_security.py": [83, 86, 87, 88, 89], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.6, "subsetName": "tests/test_security.py::TestValidateFilePath::test_dangerous_path_patterns", "supersetName": "tests/test_security.py::TestValidateFilePath::test_dangerous_pattern_windows_backslash"}, {"subset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 279, 281, 282, 283, 284, 285, 286, 287, 289, 290, 292, 294, 296, 298, 300, 302, 303, 304, 305, 324, 326, 330, 334], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142, 149, 163, 164, 167, 168, 169, 173, 186], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 202, 213, 214, 216, 217, 219, 222, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 229, 232, 233, 234, 235, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 284], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "superset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 110, 123, 126, 128, 129, 130, 131, 140, 143, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 254, 255, 256, 259, 260, 263, 266, 268, 269, 270, 273, 274], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 263, 264, 266, 267, 268, 269], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 105, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 309, 320, 322, 324, 326, 330, 334, 340, 342, 343, 344, 345, 350, 351, 366, 367, 384, 385, 402, 403, 404, 405, 407, 408, 409, 410, 411, 413], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100], "tests/test_cli.py": [12, 14, 393, 395, 403, 404, 405, 407, 408, 410], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.5994897959183674, "subsetName": "tests/test_cli.py::TestCLIIntegration::test_cli_workflow_with_thresholds", "supersetName": "tests/test_cli.py::TestCLIConfig::test_custom_config_file"}, {"subset": {"src/testiq/plugins.py": [44, 59, 60, 81, 92, 95, 96, 99, 101, 102, 103, 114, 121, 125, 126, 127, 139, 142, 144, 148, 150, 158, 163, 164, 167, 176, 178], "tests/test_plugins.py": [20, 22, 23, 24, 252, 254, 256, 257, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270], "src/testiq/logging_config.py": [25, 27, 33, 35, 38], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/plugins.py": [44, 59, 60, 81, 92, 95, 96, 99, 101, 102, 103, 114, 121, 125, 126, 127, 139, 142, 144, 148, 150, 158, 163, 164, 167, 176, 178], "tests/test_plugins.py": [20, 22, 23, 24, 307, 309, 311, 312, 313, 314, 316, 319, 320, 323, 324, 325], "src/testiq/logging_config.py": [25, 27, 33, 35, 38], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.59375, "subsetName": "tests/test_plugins.py::TestGlobalFunctions::test_hook_with_data", "supersetName": "tests/test_plugins.py::TestPluginIntegration::test_conditional_hook_execution"}, {"subset": {"src/testiq/exceptions.py": [10, 11, 12, 13, 15, 16, 29, 30], "tests/test_security.py": [170, 172, 173, 174], "src/testiq/security.py": [87, 99, 102, 105, 109, 110, 111], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/exceptions.py": [10, 11, 12, 13, 15, 16, 29, 30], "tests/test_security.py": [176, 178, 179, 180], "src/testiq/security.py": [87, 99, 102, 105, 109, 110, 113, 114], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.5925925925925926, "subsetName": "tests/test_security.py::TestValidateCoverageData::test_non_string_test_name", "supersetName": "tests/test_security.py::TestValidateCoverageData::test_empty_test_name"}, {"subset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 309, 310, 312, 313, 314, 315, 324, 326, 330, 334], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142, 149, 163, 164, 167, 168, 169, 173, 186], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 177, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 193, 194, 196, 197], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 191, 198, 199, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 261, 263, 264, 266, 267, 268, 269, 275, 285, 286, 288, 289, 290, 291, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 308, 309, 310, 312, 318, 322, 323, 324, 326, 327, 328, 330, 334, 335, 336, 337, 339, 340, 342], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "superset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 110, 123, 126, 128, 129, 130, 131, 140, 143, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 254, 255, 256, 259, 260, 263, 266, 268, 269, 270, 273, 274], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 170, 171, 173, 174, 175, 181, 188, 189, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 263, 264, 266, 267, 268, 269], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 105, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 309, 320, 322, 324, 326, 330, 334, 340, 342, 343, 344, 345, 350, 351, 366, 367, 384, 385, 402, 403, 404, 405, 407, 408, 409, 410, 411, 413], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100], "tests/test_cli.py": [12, 14, 393, 395, 403, 404, 405, 407, 408, 410], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.5894736842105263, "subsetName": "tests/test_cli.py::TestCLI::test_analyze_markdown_format", "supersetName": "tests/test_cli.py::TestCLIConfig::test_custom_config_file"}, {"subset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 270, 273, 274, 275, 277, 279, 281, 282, 283, 284, 285, 286, 287, 289, 290, 292, 294, 295, 296, 298, 300, 302, 303, 304, 305, 307, 324, 326, 330, 334], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 191, 198, 199, 201, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 261, 263, 264, 266, 267, 268, 269], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142, 149, 163, 164, 167, 168, 169, 173, 186], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 112, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143, 144, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 169, 170, 171, 172, 174, 175], "src/testiq/reporting.py": [1590, 1592, 1664, 1672, 1673, 1675, 1676, 1678, 1679, 1680, 1681, 1683, 1684, 1685, 1687, 1688, 1691, 1692, 1693, 1694, 1695, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1717, 1718, 1719, 1721, 1724, 1725, 1726, 1727, 1729], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "superset": {"src/testiq/config.py": [33, 39, 71, 73, 74, 75, 76, 77, 153, 163, 164, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 181, 184, 200, 203, 205, 209, 211, 215, 219, 223, 228, 231, 247, 250, 251, 254, 259, 260, 263, 266, 268, 269, 274], "src/testiq/cli.py": [72, 81, 82, 85, 87, 91, 92, 93, 94, 95, 96, 100, 101, 103, 104, 155, 171, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 201, 202, 203, 204, 205, 209, 210, 213, 214, 238, 261, 269, 277, 309, 310, 312, 313, 314, 315, 324, 326, 330, 334], "src/testiq/security.py": [23, 38, 40, 43, 44, 45, 50, 54, 60, 66, 77, 78, 79, 87, 99, 102, 105, 109, 110, 113, 116, 122, 123, 124, 127, 132, 135, 136, 138, 142, 149, 163, 164, 167, 168, 169, 173, 186], "tests/test_cli.py": [12, 14, 18, 20, 21, 22, 23, 26, 27, 29, 177, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 193, 194, 196, 197], "src/testiq/analyzer.py": [40, 56, 57, 58, 60, 61, 62, 63, 66, 77, 80, 83, 84, 85, 86, 90, 91, 93, 95, 96, 102, 112, 116, 117, 119, 120, 122, 123, 124, 127, 129, 130, 131, 137, 147, 151, 152, 154, 155, 156, 158, 159, 160, 161, 163, 166, 170, 171, 173, 174, 175, 181, 188, 189, 191, 198, 199, 225, 239, 242, 246, 247, 249, 250, 251, 253, 254, 256, 257, 260, 261, 263, 264, 266, 267, 268, 269, 275, 285, 286, 288, 289, 290, 291, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 308, 309, 310, 312, 318, 322, 323, 324, 326, 327, 328, 330, 334, 335, 336, 337, 339, 340, 342], "src/testiq/pytest_plugin.py": [138, 141], "src/testiq/performance.py": [24, 32, 33, 36, 38, 39, 40, 142, 153, 154, 155, 156, 157, 158, 228, 236, 237, 238, 239, 241, 248, 249, 252, 253, 254], "src/testiq/logging_config.py": [25, 27, 33, 35, 38, 41, 61, 62, 63, 66, 69, 70, 71, 72, 73, 75, 76, 79, 100]}, "ratio": 0.588495575221239, "subsetName": "tests/test_cli.py::TestCLI::test_analyze_output_formats", "supersetName": "tests/test_cli.py::TestCLI::test_analyze_markdown_format"}, {"subset": {"src/testiq/exceptions.py": [10, 11, 12, 13, 15, 16, 29, 30], "tests/test_security.py": [176, 178, 179, 180], "src/testiq/security.py": [87, 99, 102, 105, 109, 110, 113, 114], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/exceptions.py": [10, 11, 12, 13, 15, 16, 29, 30], "src/testiq/security.py": [87, 99, 102, 105, 109, 110, 113, 116, 117, 118], "tests/test_security.py": [182, 184, 185, 186], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.5862068965517241, "subsetName": "tests/test_security.py::TestValidateCoverageData::test_empty_test_name", "supersetName": "tests/test_security.py::TestValidateCoverageData::test_non_dict_coverage"}, {"subset": {"src/testiq/logging_config.py": [25, 27, 33, 35, 38], "src/testiq/plugins.py": [39, 41, 42, 44, 59, 60, 81, 92, 95, 96, 99, 101, 102, 103, 114, 121, 125, 126, 127, 139, 142, 144, 176, 178], "tests/test_plugins.py": [20, 22, 23, 24, 101, 103, 104, 106, 107, 109, 110, 112, 113, 114], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/plugins.py": [39, 41, 42, 44, 59, 60, 81, 92, 95, 96, 99, 101, 102, 103, 104, 105, 106, 107, 114, 121, 125, 126, 127, 139, 142, 144, 176, 178], "src/testiq/logging_config.py": [25, 27, 33, 35, 38], "tests/test_plugins.py": [20, 22, 23, 24, 168, 170, 171, 173, 174, 176, 177, 179, 180, 183, 186], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.5833333333333334, "subsetName": "tests/test_plugins.py::TestPluginManager::test_trigger_hook", "supersetName": "tests/test_plugins.py::TestPluginManager::test_hook_error_handling"}, {"subset": {"src/testiq/logging_config.py": [25, 27, 33, 35, 38], "src/testiq/plugins.py": [39, 41, 42, 44, 59, 60, 62, 75, 76, 77, 78, 114, 121, 125, 126, 127, 130, 132, 139, 142, 144, 176, 178], "tests/test_plugins.py": [20, 22, 23, 24, 135, 137, 139, 142, 143, 145, 146], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/logging_config.py": [25, 27, 33, 35, 38], "tests/test_plugins.py": [20, 22, 23, 24, 150, 152, 154, 157, 160, 161, 163, 165, 166], "src/testiq/plugins.py": [39, 41, 42, 44, 59, 60, 114, 121, 125, 126, 127, 130, 132, 139, 142, 144, 176, 178], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.58, "subsetName": "tests/test_plugins.py::TestPluginManager::test_unregister_hook", "supersetName": "tests/test_plugins.py::TestPluginManager::test_clear_hooks"}, {"subset": {"tests/test_plugins.py": [20, 22, 23, 24, 116, 118, 119, 121, 122, 124, 125, 127, 128, 129, 131], "src/testiq/logging_config.py": [25, 27, 33, 35, 38], "src/testiq/plugins.py": [39, 41, 42, 44, 59, 60, 81, 92, 95, 96, 99, 101, 102, 103, 114, 121, 125, 126, 127, 139, 142, 144, 176, 178], "src/testiq/pytest_plugin.py": [138, 141]}, "superset": {"src/testiq/plugins.py": [39, 41, 42, 44, 59, 60, 81, 92, 95, 96, 99, 101, 102, 103, 104, 105, 106, 107, 114, 121, 125, 126, 127, 139, 142, 144, 176, 178], "src/testiq/logging_config.py": [25, 27, 33, 35, 38], "tests/test_plugins.py": [20, 22, 23, 24, 168, 170, 171, 173, 174, 176, 177, 179, 180, 183, 186], "src/testiq/pytest_plugin.py": [138, 141]}, "ratio": 0.5737704918032787, "subsetName": "tests/test_plugins.py::TestPluginManager::test_trigger_multiple_hooks", "supersetName": "tests/test_plugins.py::TestPluginManager::test_hook_error_handling"}];
        const sourceCode = {"tests/test_security.py": {"1": "\"\"\"", "2": "Tests for security module.", "3": "\"\"\"", "4": "", "5": "import tempfile", "6": "from pathlib import Path", "7": "", "8": "import pytest", "9": "", "10": "from testiq.exceptions import SecurityError, ValidationError", "11": "from testiq.security import (", "12": "    ALLOWED_EXTENSIONS,", "13": "    DANGEROUS_PATTERNS,", "14": "    MAX_FILE_SIZE,", "15": "    MAX_LINES_PER_FILE,", "16": "    MAX_TESTS,", "17": "    check_file_size,", "18": "    compute_file_hash,", "19": "    sanitize_output_path,", "20": "    validate_coverage_data,", "21": "    validate_file_path,", "22": ")", "23": "", "24": "", "25": "class TestValidateFilePath:", "26": "    \"\"\"Test validate_file_path function.\"\"\"", "27": "", "28": "    def test_valid_json_file(self, tmp_path):", "29": "        \"\"\"Test valid JSON file path.\"\"\"", "30": "        test_file = tmp_path / \"test.json\"", "31": "        test_file.write_text(\"{}\")", "32": "        result = validate_file_path(test_file, check_exists=True)", "33": "        assert result.exists()", "34": "        assert result.suffix == \".json\"", "35": "", "36": "    def test_valid_yaml_file(self, tmp_path):", "37": "        \"\"\"Test valid YAML file path.\"\"\"", "38": "        test_file = tmp_path / \"test.yaml\"", "39": "        test_file.write_text(\"key: value\")", "40": "        result = validate_file_path(test_file, check_exists=True)", "41": "        assert result.exists()", "42": "        assert result.suffix == \".yaml\"", "43": "", "44": "    def test_valid_yml_file(self, tmp_path):", "45": "        \"\"\"Test valid YML file path.\"\"\"", "46": "        test_file = tmp_path / \"test.yml\"", "47": "        test_file.write_text(\"key: value\")", "48": "        result = validate_file_path(test_file, check_exists=True)", "49": "        assert result.exists()", "50": "        assert result.suffix == \".yml\"", "51": "", "52": "    def test_nonexistent_file_with_check(self, tmp_path):", "53": "        \"\"\"Test nonexistent file with check_exists=True.\"\"\"", "54": "        test_file = tmp_path / \"missing.json\"", "55": "        with pytest.raises(ValidationError, match=\"File does not exist\"):", "56": "            validate_file_path(test_file, check_exists=True)", "57": "", "58": "    def test_nonexistent_file_without_check(self, tmp_path):", "59": "        \"\"\"Test nonexistent file with check_exists=False.\"\"\"", "60": "        test_file = tmp_path / \"missing.json\"", "61": "        result = validate_file_path(test_file, check_exists=False)", "62": "        assert result.suffix == \".json\"", "63": "", "64": "    def test_dangerous_path_patterns(self, tmp_path):", "65": "        \"\"\"Test detection of various dangerous path patterns.\"\"\"", "66": "        # Test 1: Path traversal attack", "67": "        dangerous_file = tmp_path / \"../../../etc/passwd.json\"", "68": "        with pytest.raises(SecurityError, match=\"Dangerous path pattern detected\"):", "69": "            validate_file_path(dangerous_file, check_exists=False)", "70": "", "71": "        # Test 2: Tilde expansion pattern", "72": "        tilde_file = Path(\"~/test.json\")", "73": "        with pytest.raises(SecurityError, match=\"Dangerous path pattern detected\"):", "74": "            validate_file_path(tilde_file, check_exists=False)", "75": "", "76": "    def test_invalid_extension(self, tmp_path):", "77": "        \"\"\"Test rejection of invalid file extensions.\"\"\"", "78": "        test_file = tmp_path / \"test.txt\"", "79": "        test_file.write_text(\"content\")", "80": "        with pytest.raises(SecurityError, match=\"File extension not allowed\"):", "81": "            validate_file_path(test_file, check_exists=True)", "82": "", "83": "    def test_dangerous_pattern_windows_backslash(self, tmp_path):", "84": "        \"\"\"Test detection of Windows-style path traversal.\"\"\"", "85": "        # Create a file with backslash in name (Unix allows this)", "86": "        try:", "87": "            dangerous_file = tmp_path / \"..\\\\test.json\"", "88": "            with pytest.raises(SecurityError, match=\"Dangerous path pattern detected\"):", "89": "                validate_file_path(dangerous_file, check_exists=False)", "90": "        except (OSError, ValueError):", "91": "            # Some filesystems don't allow backslashes in filenames", "92": "            pytest.skip(\"Filesystem doesn't support backslashes in filenames\")", "93": "", "94": "", "95": "", "96": "", "97": "class TestCheckFileSize:", "98": "    \"\"\"Test check_file_size function.\"\"\"", "99": "", "100": "    def test_file_within_limit(self, tmp_path):", "101": "        \"\"\"Test file within size limit.\"\"\"", "102": "        test_file = tmp_path / \"small.json\"", "103": "        test_file.write_text(\"x\" * 1000)", "104": "        # Should not raise", "105": "        check_file_size(test_file, max_size=10000)", "106": "", "107": "    def test_file_exceeds_limit(self, tmp_path):", "108": "        \"\"\"Test file exceeding size limit.\"\"\"", "109": "        test_file = tmp_path / \"large.json\"", "110": "        test_file.write_bytes(b\"x\" * 10000)", "111": "        with pytest.raises(SecurityError, match=\"File too large\"):", "112": "            check_file_size(test_file, max_size=1000)", "113": "", "114": "    def test_file_at_exact_limit(self, tmp_path):", "115": "        \"\"\"Test file at exact size limit.\"\"\"", "116": "        test_file = tmp_path / \"exact.json\"", "117": "        test_file.write_bytes(b\"x\" * 1000)", "118": "        # Should not raise", "119": "        check_file_size(test_file, max_size=1000)", "120": "", "121": "    def test_default_max_size(self, tmp_path):", "122": "        \"\"\"Test default MAX_FILE_SIZE limit.\"\"\"", "123": "        test_file = tmp_path / \"test.json\"", "124": "        test_file.write_text(\"small\")", "125": "        # Should not raise with default", "126": "        check_file_size(test_file)", "127": "", "128": "    def test_nonexistent_file(self, tmp_path):", "129": "        \"\"\"Test checking size of nonexistent file.\"\"\"", "130": "        test_file = tmp_path / \"missing.json\"", "131": "        with pytest.raises(ValidationError, match=\"Cannot check file size\"):", "132": "            check_file_size(test_file)", "133": "", "134": "", "135": "class TestValidateCoverageData:", "136": "    \"\"\"Test validate_coverage_data function.\"\"\"", "137": "", "138": "    def test_valid_coverage_scenarios(self):", "139": "        \"\"\"Test valid coverage data in various scenarios.\"\"\"", "140": "        # Test 1: Normal valid coverage data", "141": "        data_normal = {", "142": "            \"test_one\": {\"file1.py\": [1, 2, 3], \"file2.py\": [10, 20]},", "143": "            \"test_two\": {\"file1.py\": [4, 5], \"file3.py\": [1, 2]},", "144": "        }", "145": "        # Should not raise", "146": "        validate_coverage_data(data_normal)", "147": "", "148": "        # Test 2: Valid coverage at line limit", "149": "        lines = list(range(1, MAX_LINES_PER_FILE + 1))", "150": "        data_limit = {\"test_one\": {\"file.py\": lines}}", "151": "        # Should not raise", "152": "        validate_coverage_data(data_limit)", "153": "", "154": "    def test_empty_coverage_data(self):", "155": "        \"\"\"Test empty coverage data.\"\"\"", "156": "        with pytest.raises(ValidationError, match=\"Coverage data is empty\"):", "157": "            validate_coverage_data({})", "158": "", "159": "    def test_non_dict_coverage_data(self):", "160": "        \"\"\"Test non-dict coverage data.\"\"\"", "161": "        with pytest.raises(ValidationError, match=\"Coverage data must be a dictionary\"):", "162": "            validate_coverage_data([])", "163": "", "164": "    def test_too_many_tests(self):", "165": "        \"\"\"Test exceeding max tests limit.\"\"\"", "166": "        data = {f\"test_{i}\": {\"file.py\": [1, 2]} for i in range(100)}", "167": "        with pytest.raises(SecurityError, match=\"Too many tests\"):", "168": "            validate_coverage_data(data, max_tests=50)", "169": "", "170": "    def test_non_string_test_name(self):", "171": "        \"\"\"Test non-string test name.\"\"\"", "172": "        data = {123: {\"file.py\": [1, 2]}}", "173": "        with pytest.raises(ValidationError, match=\"Test name must be string\"):", "174": "            validate_coverage_data(data)", "175": "", "176": "    def test_empty_test_name(self):", "177": "        \"\"\"Test empty test name.\"\"\"", "178": "        data = {\"   \": {\"file.py\": [1, 2]}}", "179": "        with pytest.raises(ValidationError, match=\"Test name cannot be empty\"):", "180": "            validate_coverage_data(data)", "181": "", "182": "    def test_non_dict_coverage(self):", "183": "        \"\"\"Test non-dict coverage for a test.\"\"\"", "184": "        data = {\"test_one\": [1, 2, 3]}", "185": "        with pytest.raises(ValidationError, match=\"must be a dictionary\"):", "186": "            validate_coverage_data(data)", "187": "", "188": "    def test_invalid_data_types_in_coverage(self):", "189": "        \"\"\"Test validation of invalid data types in coverage data.\"\"\"", "190": "        # Test 1: Non-string file name", "191": "        data_bad_filename = {\"test_one\": {123: [1, 2]}}", "192": "        with pytest.raises(ValidationError, match=\"File name must be string\"):", "193": "            validate_coverage_data(data_bad_filename)", "194": "", "195": "        # Test 2: Non-list lines", "196": "        data_bad_lines = {\"test_one\": {\"file.py\": \"not a list\"}}", "197": "        with pytest.raises(ValidationError, match=\"must be a list\"):", "198": "            validate_coverage_data(data_bad_lines)", "199": "", "200": "    def test_line_number_validation_scenarios(self):", "201": "        \"\"\"Test comprehensive line number validation scenarios.\"\"\"", "202": "        # Test 1: Non-integer line number", "203": "        data_non_int = {\"test_one\": {\"file.py\": [1, 2, \"3\"]}}", "204": "        with pytest.raises(ValidationError, match=\"Line number must be integer\"):", "205": "            validate_coverage_data(data_non_int)", "206": "", "207": "        # Test 2: Invalid line number (zero)", "208": "        data_zero = {\"test_one\": {\"file.py\": [1, 2, 0]}}", "209": "        with pytest.raises(ValidationError, match=\"Invalid line number: 0\"):", "210": "            validate_coverage_data(data_zero)", "211": "", "212": "        # Test 3: Negative line number", "213": "        data_negative = {\"test_one\": {\"file.py\": [1, 2, -5]}}", "214": "        with pytest.raises(ValidationError, match=\"Invalid line number: -5\"):", "215": "            validate_coverage_data(data_negative)", "216": "", "217": "    def test_too_many_lines(self):", "218": "        \"\"\"Test exceeding max lines limit.\"\"\"", "219": "        # Create coverage with many lines", "220": "        large_lines = list(range(1, 101000))", "221": "        data = {\"test_one\": {\"file.py\": large_lines}}", "222": "        with pytest.raises(SecurityError, match=\"covers too many lines\"):", "223": "            validate_coverage_data(data)", "224": "", "225": "", "226": "", "227": "class TestSanitizeOutputPath:", "228": "    \"\"\"Test sanitize_output_path function.\"\"\"", "229": "", "230": "    def test_valid_output_path(self, tmp_path):", "231": "        \"\"\"Test valid output path.\"\"\"", "232": "        output_path = tmp_path / \"output.html\"", "233": "        result = sanitize_output_path(output_path)", "234": "        assert result.is_absolute()", "235": "", "236": "    def test_dangerous_pattern_in_path(self, tmp_path):", "237": "        \"\"\"Test detection of dangerous patterns.\"\"\"", "238": "        output_path = tmp_path / \"../../../etc/output.html\"", "239": "        with pytest.raises(SecurityError, match=\"Dangerous path pattern detected\"):", "240": "            sanitize_output_path(output_path)", "241": "", "242": "    def test_allowed_directory(self, tmp_path):", "243": "        \"\"\"Test path within allowed directory.\"\"\"", "244": "        allowed_dir = tmp_path / \"allowed\"", "245": "        allowed_dir.mkdir()", "246": "        output_path = allowed_dir / \"output.html\"", "247": "        result = sanitize_output_path(output_path, allowed_dirs=[allowed_dir])", "248": "        assert result.is_absolute()", "249": "", "250": "    def test_disallowed_directory(self, tmp_path):", "251": "        \"\"\"Test path outside allowed directories.\"\"\"", "252": "        allowed_dir = tmp_path / \"allowed\"", "253": "        allowed_dir.mkdir()", "254": "        disallowed_dir = tmp_path / \"disallowed\"", "255": "        disallowed_dir.mkdir()", "256": "        output_path = disallowed_dir / \"output.html\"", "257": "        with pytest.raises(SecurityError, match=\"not in allowed directories\"):", "258": "            sanitize_output_path(output_path, allowed_dirs=[allowed_dir])", "259": "", "260": "    def test_no_allowed_dirs_restriction(self, tmp_path):", "261": "        \"\"\"Test path when no allowed_dirs restriction is set.\"\"\"", "262": "        output_path = tmp_path / \"anywhere\" / \"output.html\"", "263": "        result = sanitize_output_path(output_path, allowed_dirs=None)", "264": "        assert result.is_absolute()", "265": "", "266": "    def test_multiple_allowed_directories(self, tmp_path):", "267": "        \"\"\"Test path with multiple allowed directories.\"\"\"", "268": "        dir1 = tmp_path / \"dir1\"", "269": "        dir2 = tmp_path / \"dir2\"", "270": "        dir1.mkdir()", "271": "        dir2.mkdir()", "272": "        output_path = dir2 / \"output.html\"", "273": "        result = sanitize_output_path(output_path, allowed_dirs=[dir1, dir2])", "274": "        assert result.is_absolute()", "275": "", "276": "", "277": "class TestComputeFileHash:", "278": "    \"\"\"Test compute_file_hash function.\"\"\"", "279": "", "280": "    def test_hash_small_file(self, tmp_path):", "281": "        \"\"\"Test hash computation for small file.\"\"\"", "282": "        test_file = tmp_path / \"test.txt\"", "283": "        test_file.write_text(\"test content\")", "284": "        hash1 = compute_file_hash(test_file)", "285": "        assert len(hash1) == 64  # SHA-256 produces 64 hex characters", "286": "        assert isinstance(hash1, str)", "287": "", "288": "    def test_hash_consistency(self, tmp_path):", "289": "        \"\"\"Test hash is consistent for same content.\"\"\"", "290": "        test_file = tmp_path / \"test.txt\"", "291": "        test_file.write_text(\"test content\")", "292": "        hash1 = compute_file_hash(test_file)", "293": "        hash2 = compute_file_hash(test_file)", "294": "        assert hash1 == hash2", "295": "", "296": "    def test_hash_different_content(self, tmp_path):", "297": "        \"\"\"Test different content produces different hash.\"\"\"", "298": "        file1 = tmp_path / \"file1.txt\"", "299": "        file2 = tmp_path / \"file2.txt\"", "300": "        file1.write_text(\"content1\")", "301": "        file2.write_text(\"content2\")", "302": "        hash1 = compute_file_hash(file1)", "303": "        hash2 = compute_file_hash(file2)", "304": "        assert hash1 != hash2", "305": "", "306": "    def test_hash_large_file(self, tmp_path):", "307": "        \"\"\"Test hash computation for large file (> 4KB chunks).\"\"\"", "308": "        test_file = tmp_path / \"large.txt\"", "309": "        test_file.write_bytes(b\"x\" * 10000)", "310": "        hash_result = compute_file_hash(test_file)", "311": "        assert len(hash_result) == 64", "312": "", "313": "    def test_hash_empty_file(self, tmp_path):", "314": "        \"\"\"Test hash computation for empty file.\"\"\"", "315": "        test_file = tmp_path / \"empty.txt\"", "316": "        test_file.write_text(\"\")", "317": "        hash_result = compute_file_hash(test_file)", "318": "        assert len(hash_result) == 64", "319": "        # Empty file should have a specific known hash", "320": "        assert hash_result == \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"", "321": "", "322": "", "323": "class TestConstants:", "324": "    \"\"\"Test security constants.\"\"\"", "325": "", "326": "    def test_max_file_size(self):", "327": "        \"\"\"Test MAX_FILE_SIZE constant.\"\"\"", "328": "        assert MAX_FILE_SIZE == 100 * 1024 * 1024", "329": "        assert MAX_FILE_SIZE == 104857600  # 100MB", "330": "", "331": "    def test_max_tests(self):", "332": "        \"\"\"Test MAX_TESTS constant.\"\"\"", "333": "        assert MAX_TESTS == 50000", "334": "", "335": "    def test_max_lines_per_file(self):", "336": "        \"\"\"Test MAX_LINES_PER_FILE constant.\"\"\"", "337": "        assert MAX_LINES_PER_FILE == 100000", "338": "", "339": "    def test_allowed_extensions(self):", "340": "        \"\"\"Test ALLOWED_EXTENSIONS constant.\"\"\"", "341": "        assert \".json\" in ALLOWED_EXTENSIONS", "342": "        assert \".yaml\" in ALLOWED_EXTENSIONS", "343": "        assert \".yml\" in ALLOWED_EXTENSIONS", "344": "        assert len(ALLOWED_EXTENSIONS) == 3", "345": "", "346": "    def test_dangerous_patterns(self):", "347": "        \"\"\"Test DANGEROUS_PATTERNS constant.\"\"\"", "348": "        assert \"../\" in DANGEROUS_PATTERNS", "349": "        assert \"..\\\\\" in DANGEROUS_PATTERNS", "350": "        assert \"~\" in DANGEROUS_PATTERNS"}, "tests/test_performance.py": {"1": "\"\"\"", "2": "Tests for performance module.", "3": "\"\"\"", "4": "", "5": "import json", "6": "import tempfile", "7": "from pathlib import Path", "8": "", "9": "import pytest", "10": "", "11": "from testiq.exceptions import AnalysisError", "12": "from testiq.performance import (", "13": "    CacheManager,", "14": "    ParallelProcessor,", "15": "    ProgressTracker,", "16": "    StreamingJSONParser,", "17": "    batch_iterator,", "18": "    compute_similarity,", "19": ")", "20": "", "21": "", "22": "class TestCacheManager:", "23": "    \"\"\"Test CacheManager class.\"\"\"", "24": "", "25": "    def test_init_with_custom_dir(self, tmp_path):", "26": "        \"\"\"Test initialization with custom cache directory.\"\"\"", "27": "        cache_dir = tmp_path / \"custom_cache\"", "28": "        manager = CacheManager(cache_dir=cache_dir, enabled=True)", "29": "        assert manager.cache_dir == cache_dir", "30": "        assert cache_dir.exists()", "31": "        assert manager.enabled", "32": "", "33": "    def test_init_with_default_dir(self):", "34": "        \"\"\"Test initialization with default cache directory.\"\"\"", "35": "        manager = CacheManager(enabled=True)", "36": "        assert manager.cache_dir == Path.home() / \".testiq\" / \"cache\"", "37": "        assert manager.enabled", "38": "", "39": "    def test_init_disabled(self):", "40": "        \"\"\"Test initialization with caching disabled.\"\"\"", "41": "        manager = CacheManager(enabled=False)", "42": "        assert not manager.enabled", "43": "", "44": "    def test_get_cache_key_dict(self):", "45": "        \"\"\"Test cache key generation from dict.\"\"\"", "46": "        manager = CacheManager(enabled=False)", "47": "        data = {\"key\": \"value\", \"num\": 123}", "48": "        key1 = manager._get_cache_key(data)", "49": "        key2 = manager._get_cache_key(data)", "50": "        assert key1 == key2", "51": "        assert len(key1) == 16  # SHA-256 truncated to 16 chars", "52": "", "53": "    def test_get_cache_key_string(self):", "54": "        \"\"\"Test cache key generation from string.\"\"\"", "55": "        manager = CacheManager(enabled=False)", "56": "        key1 = manager._get_cache_key(\"test string\")", "57": "        key2 = manager._get_cache_key(\"test string\")", "58": "        assert key1 == key2", "59": "        assert len(key1) == 16", "60": "", "61": "    def test_get_cache_key_consistency(self):", "62": "        \"\"\"Test cache key is consistent for same data.\"\"\"", "63": "        manager = CacheManager(enabled=False)", "64": "        data = {\"b\": 2, \"a\": 1}  # Different order", "65": "        key1 = manager._get_cache_key({\"a\": 1, \"b\": 2})", "66": "        key2 = manager._get_cache_key(data)", "67": "        assert key1 == key2  # Should be same due to sort_keys=True", "68": "", "69": "    def test_get_miss(self, tmp_path):", "70": "        \"\"\"Test cache miss.\"\"\"", "71": "        manager = CacheManager(cache_dir=tmp_path, enabled=True)", "72": "        result = manager.get(\"nonexistent_key\")", "73": "        assert result is None", "74": "", "75": "    def test_set_and_get(self, tmp_path):", "76": "        \"\"\"Test setting and getting cached value.\"\"\"", "77": "        manager = CacheManager(cache_dir=tmp_path, enabled=True)", "78": "        data = {\"test\": \"value\", \"number\": 42}", "79": "        manager.set(\"test_key\", data)", "80": "        result = manager.get(\"test_key\")", "81": "        assert result == data", "82": "", "83": "    def test_get_disabled(self, tmp_path):", "84": "        \"\"\"Test get when caching is disabled.\"\"\"", "85": "        manager = CacheManager(cache_dir=tmp_path, enabled=False)", "86": "        result = manager.get(\"any_key\")", "87": "        assert result is None", "88": "", "89": "    def test_set_disabled(self, tmp_path):", "90": "        \"\"\"Test set when caching is disabled.\"\"\"", "91": "        manager = CacheManager(cache_dir=tmp_path, enabled=False)", "92": "        manager.set(\"key\", {\"value\": 123})", "93": "        # Check that no cache file was created", "94": "        cache_files = list(tmp_path.glob(\"*.cache\"))", "95": "        assert len(cache_files) == 0", "96": "", "97": "    def test_get_corrupted_cache(self, tmp_path):", "98": "        \"\"\"Test handling of corrupted cache file.\"\"\"", "99": "        manager = CacheManager(cache_dir=tmp_path, enabled=True)", "100": "        # Create a corrupted cache file", "101": "        cache_file = tmp_path / \"test_key.cache\"", "102": "        cache_file.write_text(\"corrupted data\")", "103": "        result = manager.get(\"test_key\")", "104": "        assert result is None  # Should return None on error", "105": "", "106": "    def test_clear_cache(self, tmp_path):", "107": "        \"\"\"Test clearing all cached data.\"\"\"", "108": "        manager = CacheManager(cache_dir=tmp_path, enabled=True)", "109": "        manager.set(\"key1\", {\"value\": 1})", "110": "        manager.set(\"key2\", {\"value\": 2})", "111": "        manager.set(\"key3\", {\"value\": 3})", "112": "", "113": "        # Verify files exist", "114": "        assert len(list(tmp_path.glob(\"*.cache\"))) == 3", "115": "", "116": "        # Clear cache", "117": "        manager.clear()", "118": "", "119": "        # Verify files are gone", "120": "        assert len(list(tmp_path.glob(\"*.cache\"))) == 0", "121": "", "122": "    def test_clear_disabled(self, tmp_path):", "123": "        \"\"\"Test clear when caching is disabled.\"\"\"", "124": "        manager = CacheManager(cache_dir=tmp_path, enabled=False)", "125": "        # Should not raise", "126": "        manager.clear()", "127": "", "128": "", "129": "class TestStreamingJSONParser:", "130": "    \"\"\"Test StreamingJSONParser class.\"\"\"", "131": "", "132": "    def test_parse_coverage_file(self, tmp_path):", "133": "        \"\"\"Test parsing a valid coverage JSON file.\"\"\"", "134": "        coverage_data = {", "135": "            \"test1\": {\"file1.py\": [1, 2, 3]},", "136": "            \"test2\": {\"file2.py\": [10, 20]},", "137": "            \"test3\": {\"file3.py\": [5, 6]},", "138": "        }", "139": "", "140": "        json_file = tmp_path / \"coverage.json\"", "141": "        json_file.write_text(json.dumps(coverage_data))", "142": "", "143": "        parser = StreamingJSONParser()", "144": "        results = list(parser.parse_coverage_file(json_file))", "145": "", "146": "        assert len(results) == 3", "147": "        assert results[0][0] in coverage_data", "148": "        assert results[1][0] in coverage_data", "149": "        assert results[2][0] in coverage_data", "150": "", "151": "    def test_parse_coverage_file_chunked(self, tmp_path):", "152": "        \"\"\"Test parsing with custom chunk size.\"\"\"", "153": "        coverage_data = {f\"test{i}\": {\"file.py\": [i]} for i in range(10)}", "154": "", "155": "        json_file = tmp_path / \"coverage.json\"", "156": "        json_file.write_text(json.dumps(coverage_data))", "157": "", "158": "        parser = StreamingJSONParser()", "159": "        results = list(parser.parse_coverage_file(json_file, chunk_size=3))", "160": "", "161": "        assert len(results) == 10", "162": "", "163": "    def test_parse_invalid_json(self, tmp_path):", "164": "        \"\"\"Test parsing invalid JSON.\"\"\"", "165": "        json_file = tmp_path / \"invalid.json\"", "166": "        json_file.write_text(\"{ invalid json }\")", "167": "", "168": "        parser = StreamingJSONParser()", "169": "        with pytest.raises(AnalysisError, match=\"Invalid JSON\"):", "170": "            list(parser.parse_coverage_file(json_file))", "171": "", "172": "    def test_parse_non_dict_json(self, tmp_path):", "173": "        \"\"\"Test parsing JSON that's not a dict.\"\"\"", "174": "        json_file = tmp_path / \"list.json\"", "175": "        json_file.write_text(\"[1, 2, 3]\")", "176": "", "177": "        parser = StreamingJSONParser()", "178": "        with pytest.raises(AnalysisError, match=\"must contain a dictionary\"):", "179": "            list(parser.parse_coverage_file(json_file))", "180": "", "181": "    def test_parse_empty_file(self, tmp_path):", "182": "        \"\"\"Test parsing empty coverage data.\"\"\"", "183": "        json_file = tmp_path / \"empty.json\"", "184": "        json_file.write_text(\"{}\")", "185": "", "186": "        parser = StreamingJSONParser()", "187": "        results = list(parser.parse_coverage_file(json_file))", "188": "", "189": "        assert len(results) == 0", "190": "", "191": "", "192": "class TestParallelProcessor:", "193": "    \"\"\"Test ParallelProcessor class.\"\"\"", "194": "", "195": "    def test_init_enabled(self):", "196": "        \"\"\"Test initialization with parallel processing enabled.\"\"\"", "197": "        processor = ParallelProcessor(max_workers=4, enabled=True)", "198": "        assert processor.max_workers == 4", "199": "        assert processor.enabled", "200": "", "201": "    def test_init_disabled(self):", "202": "        \"\"\"Test initialization with parallel processing disabled.\"\"\"", "203": "        processor = ParallelProcessor(enabled=False)", "204": "        assert not processor.enabled", "205": "", "206": "    def test_map_sequential(self):", "207": "        \"\"\"Test sequential processing when disabled.\"\"\"", "208": "        processor = ParallelProcessor(enabled=False)", "209": "        items = [1, 2, 3, 4, 5]", "210": "", "211": "        def square(x):", "212": "            return x * x", "213": "", "214": "        results = processor.map(square, items)", "215": "        assert results == [1, 4, 9, 16, 25]", "216": "", "217": "    def test_map_parallel_thread(self):", "218": "        \"\"\"Test parallel processing with threads.\"\"\"", "219": "        processor = ParallelProcessor(max_workers=2, use_processes=False, enabled=True)", "220": "        items = [1, 2, 3, 4, 5]", "221": "", "222": "        def square(x):", "223": "            return x * x", "224": "", "225": "        results = processor.map(square, items)", "226": "        assert sorted(results) == [1, 4, 9, 16, 25]", "227": "", "228": "    def test_parallel_map_scenarios(self):", "229": "        \"\"\"Test parallel mapping with processes and error handling.\"\"\"", "230": "        # Test 1: Parallel processing with processes", "231": "        processor = ParallelProcessor(max_workers=2, use_processes=True, enabled=True)", "232": "        items = [1, 2, 3, 4, 5]", "233": "", "234": "        def local_square(x):", "235": "            return x * x", "236": "", "237": "        results = processor.map(local_square, items)", "238": "        # May fail and fall back to sequential, so just check it completes", "239": "        assert results is not None", "240": "        assert len(results) == 5", "241": "", "242": "        # Test 2: Error handling in parallel processing", "243": "        processor2 = ParallelProcessor(max_workers=2, enabled=True)", "244": "", "245": "        def failing_func(x):", "246": "            if x == 3:", "247": "                raise ValueError(\"Test error\")", "248": "            return x * x", "249": "", "250": "        results2 = processor2.map(failing_func, items)", "251": "        # Should have None for failed item", "252": "        assert None in results2", "253": "        assert 1 in results2", "254": "        assert 4 in results2", "255": "", "256": "", "257": "", "258": "    def test_map_edge_cases(self):", "259": "        \"\"\"Test processing edge cases: empty list and single item.\"\"\"", "260": "        processor = ParallelProcessor(enabled=True)", "261": "", "262": "        # Test 1: Empty list", "263": "        results_empty = processor.map(lambda x: x, [])", "264": "        assert results_empty == []", "265": "", "266": "        # Test 2: Single item (uses sequential)", "267": "        results_single = processor.map(lambda x: x * 2, [5])", "268": "        assert results_single == [10]", "269": "", "270": "", "271": "class TestComputeSimilarity:", "272": "    \"\"\"Test compute_similarity function.\"\"\"", "273": "", "274": "    def test_identical_sets(self):", "275": "        \"\"\"Test similarity of identical sets.\"\"\"", "276": "        set1 = frozenset([1, 2, 3, 4])", "277": "        set2 = frozenset([1, 2, 3, 4])", "278": "        similarity = compute_similarity(set1, set2)", "279": "        assert similarity == 1.0", "280": "", "281": "    def test_no_overlap(self):", "282": "        \"\"\"Test similarity of disjoint sets.\"\"\"", "283": "        set1 = frozenset([1, 2, 3])", "284": "        set2 = frozenset([4, 5, 6])", "285": "        similarity = compute_similarity(set1, set2)", "286": "        assert similarity == 0.0", "287": "", "288": "    def test_partial_overlap(self):", "289": "        \"\"\"Test similarity of partially overlapping sets.\"\"\"", "290": "        set1 = frozenset([1, 2, 3, 4])", "291": "        set2 = frozenset([3, 4, 5, 6])", "292": "        similarity = compute_similarity(set1, set2)", "293": "        # Intersection: {3, 4} = 2 elements", "294": "        # Union: {1, 2, 3, 4, 5, 6} = 6 elements", "295": "        # Similarity: 2/6 = 0.333...", "296": "        assert abs(similarity - 0.333) < 0.01", "297": "", "298": "    def test_subset(self):", "299": "        \"\"\"Test similarity when one set is subset of another.\"\"\"", "300": "        set1 = frozenset([1, 2])", "301": "        set2 = frozenset([1, 2, 3, 4])", "302": "        similarity = compute_similarity(set1, set2)", "303": "        # Intersection: {1, 2} = 2 elements", "304": "        # Union: {1, 2, 3, 4} = 4 elements", "305": "        # Similarity: 2/4 = 0.5", "306": "        assert similarity == 0.5", "307": "", "308": "    def test_empty_sets(self):", "309": "        \"\"\"Test similarity of empty sets.\"\"\"", "310": "        set1 = frozenset()", "311": "        set2 = frozenset()", "312": "        similarity = compute_similarity(set1, set2)", "313": "        assert similarity == 0.0", "314": "", "315": "    def test_caching(self):", "316": "        \"\"\"Test that similarity computation is cached.\"\"\"", "317": "        set1 = frozenset([1, 2, 3])", "318": "        set2 = frozenset([2, 3, 4])", "319": "", "320": "        # First call", "321": "        result1 = compute_similarity(set1, set2)", "322": "        # Second call should use cache", "323": "        result2 = compute_similarity(set1, set2)", "324": "", "325": "        assert result1 == result2", "326": "", "327": "", "328": "class TestProgressTracker:", "329": "    \"\"\"Test ProgressTracker class.\"\"\"", "330": "", "331": "    def test_init(self):", "332": "        \"\"\"Test initialization.\"\"\"", "333": "        tracker = ProgressTracker(total=100, desc=\"Testing\")", "334": "        assert tracker.total == 100", "335": "        assert tracker.current == 0", "336": "        assert tracker.desc == \"Testing\"", "337": "        assert tracker.last_logged_percent == -1", "338": "", "339": "    def test_progress_tracking_scenarios(self):", "340": "        \"\"\"Test progress tracking including updates, percentages, and completion.\"\"\"", "341": "        # Test 1: Update progress", "342": "        tracker = ProgressTracker(total=100)", "343": "        tracker.update(10)", "344": "        assert tracker.current == 10", "345": "        tracker.update(15)", "346": "        assert tracker.current == 25", "347": "", "348": "        # Test 2: Progress percentage calculation", "349": "        tracker2 = ProgressTracker(total=100)", "350": "        tracker2.update(25)", "351": "        percent = (tracker2.current / tracker2.total) * 100", "352": "        assert percent == 25.0", "353": "", "354": "        # Test 3: Complete 100% progress", "355": "        tracker3 = ProgressTracker(total=10)", "356": "        tracker3.update(10)", "357": "        assert tracker3.current == 10", "358": "        percent_complete = (tracker3.current / tracker3.total) * 100", "359": "        assert percent_complete == 100.0", "360": "", "361": "", "362": "class TestBatchIterator:", "363": "    \"\"\"Test batch_iterator function.\"\"\"", "364": "", "365": "    def test_exact_batches(self):", "366": "        \"\"\"Test batching with exact multiples.\"\"\"", "367": "        items = list(range(10))", "368": "        batches = list(batch_iterator(items, batch_size=2))", "369": "        assert len(batches) == 5", "370": "        assert batches[0] == [0, 1]", "371": "        assert batches[4] == [8, 9]", "372": "", "373": "    def test_uneven_batches(self):", "374": "        \"\"\"Test batching with remainder.\"\"\"", "375": "        items = list(range(10))", "376": "        batches = list(batch_iterator(items, batch_size=3))", "377": "        assert len(batches) == 4", "378": "        assert batches[0] == [0, 1, 2]", "379": "        assert batches[3] == [9]  # Last batch has only 1 item", "380": "", "381": "    def test_single_batch(self):", "382": "        \"\"\"Test when batch size >= item count.\"\"\"", "383": "        items = [1, 2, 3]", "384": "        batches = list(batch_iterator(items, batch_size=10))", "385": "        assert len(batches) == 1", "386": "        assert batches[0] == [1, 2, 3]", "387": "", "388": "    def test_empty_list(self):", "389": "        \"\"\"Test batching empty list.\"\"\"", "390": "        items = []", "391": "        batches = list(batch_iterator(items, batch_size=5))", "392": "        assert len(batches) == 0", "393": "", "394": "    def test_batch_size_one(self):", "395": "        \"\"\"Test batch size of 1.\"\"\"", "396": "        items = [1, 2, 3]", "397": "        batches = list(batch_iterator(items, batch_size=1))", "398": "        assert len(batches) == 3", "399": "        assert batches[0] == [1]", "400": "        assert batches[1] == [2]", "401": "        assert batches[2] == [3]"}, "src/testiq/config.py": {"1": "\"\"\"", "2": "Configuration management for TestIQ.", "3": "Supports YAML, TOML config files and environment variables.", "4": "\"\"\"", "5": "", "6": "import os", "7": "from dataclasses import dataclass, field", "8": "from pathlib import Path", "9": "from typing import Any, Optional", "10": "", "11": "try:", "12": "    import tomllib", "13": "except ImportError:", "14": "    import tomli as tomllib  # Python < 3.11", "15": "", "16": "import yaml", "17": "", "18": "from testiq.exceptions import ConfigurationError", "19": "", "20": "", "21": "@dataclass", "22": "class LogConfig:", "23": "    \"\"\"Logging configuration.\"\"\"", "24": "", "25": "    level: str = \"INFO\"", "26": "    file: Optional[str] = None", "27": "    enable_rotation: bool = True", "28": "    max_bytes: int = 10 * 1024 * 1024  # 10MB", "29": "    backup_count: int = 5", "30": "", "31": "", "32": "@dataclass", "33": "class SecurityConfig:", "34": "    \"\"\"Security configuration.\"\"\"", "35": "", "36": "    max_file_size: int = 100 * 1024 * 1024  # 100MB", "37": "    max_tests: int = 50000", "38": "    max_lines_per_file: int = 100000", "39": "    allowed_extensions: list[str] = field(default_factory=lambda: [\".json\", \".yaml\", \".yml\"])", "40": "", "41": "", "42": "@dataclass", "43": "class PerformanceConfig:", "44": "    \"\"\"Performance configuration.\"\"\"", "45": "", "46": "    enable_parallel: bool = True", "47": "    max_workers: int = 4", "48": "    enable_caching: bool = True", "49": "    cache_dir: Optional[str] = None", "50": "", "51": "", "52": "@dataclass", "53": "class AnalysisConfig:", "54": "    \"\"\"Analysis configuration.\"\"\"", "55": "", "56": "    similarity_threshold: float = 0.3", "57": "    min_coverage_lines: int = 1", "58": "    max_results: int = 1000", "59": "", "60": "", "61": "@dataclass", "62": "class Config:", "63": "    \"\"\"Main TestIQ configuration.\"\"\"", "64": "", "65": "    log: LogConfig = field(default_factory=LogConfig)", "66": "    security: SecurityConfig = field(default_factory=SecurityConfig)", "67": "    performance: PerformanceConfig = field(default_factory=PerformanceConfig)", "68": "    analysis: AnalysisConfig = field(default_factory=AnalysisConfig)", "69": "", "70": "    @classmethod", "71": "    def from_dict(cls, data: dict[str, Any]) -> \"Config\":", "72": "        \"\"\"Create config from dictionary.\"\"\"", "73": "        return cls(", "74": "            log=LogConfig(**data.get(\"log\", {})),", "75": "            security=SecurityConfig(**data.get(\"security\", {})),", "76": "            performance=PerformanceConfig(**data.get(\"performance\", {})),", "77": "            analysis=AnalysisConfig(**data.get(\"analysis\", {})),", "78": "        )", "79": "", "80": "    def to_dict(self) -> dict[str, Any]:", "81": "        \"\"\"Convert config to dictionary.\"\"\"", "82": "        return {", "83": "            \"log\": {", "84": "                \"level\": self.log.level,", "85": "                \"file\": self.log.file,", "86": "                \"enable_rotation\": self.log.enable_rotation,", "87": "                \"max_bytes\": self.log.max_bytes,", "88": "                \"backup_count\": self.log.backup_count,", "89": "            },", "90": "            \"security\": {", "91": "                \"max_file_size\": self.security.max_file_size,", "92": "                \"max_tests\": self.security.max_tests,", "93": "                \"max_lines_per_file\": self.security.max_lines_per_file,", "94": "                \"allowed_extensions\": self.security.allowed_extensions,", "95": "            },", "96": "            \"performance\": {", "97": "                \"enable_parallel\": self.performance.enable_parallel,", "98": "                \"max_workers\": self.performance.max_workers,", "99": "                \"enable_caching\": self.performance.enable_caching,", "100": "                \"cache_dir\": self.performance.cache_dir,", "101": "            },", "102": "            \"analysis\": {", "103": "                \"similarity_threshold\": self.analysis.similarity_threshold,", "104": "                \"min_coverage_lines\": self.analysis.min_coverage_lines,", "105": "                \"max_results\": self.analysis.max_results,", "106": "            },", "107": "        }", "108": "", "109": "", "110": "def load_config_file(config_path: Path) -> dict[str, Any]:", "111": "    \"\"\"", "112": "    Load configuration from file.", "113": "", "114": "    Args:", "115": "        config_path: Path to config file (.yaml, .yml, or .toml)", "116": "", "117": "    Returns:", "118": "        Configuration dictionary", "119": "", "120": "    Raises:", "121": "        ConfigurationError: If file cannot be loaded", "122": "    \"\"\"", "123": "    if not config_path.exists():", "124": "        raise ConfigurationError(f\"Config file not found: {config_path}\")", "125": "", "126": "    suffix = config_path.suffix.lower()", "127": "", "128": "    try:", "129": "        with open(config_path, \"rb\") as f:", "130": "            if suffix in (\".yaml\", \".yml\"):", "131": "                data = yaml.safe_load(f)", "132": "            elif suffix == \".toml\":", "133": "                data = tomllib.load(f)", "134": "            else:", "135": "                raise ConfigurationError(", "136": "                    f\"Unsupported config file format: {suffix}. \"", "137": "                    \"Supported formats: .yaml, .yml, .toml\"", "138": "                )", "139": "", "140": "        if not isinstance(data, dict):", "141": "            raise ConfigurationError(\"Config file must contain a dictionary\")", "142": "", "143": "        return data", "144": "", "145": "    except yaml.YAMLError as e:", "146": "        raise ConfigurationError(f\"Invalid YAML in config file: {e}\")", "147": "    except tomllib.TOMLDecodeError as e:", "148": "        raise ConfigurationError(f\"Invalid TOML in config file: {e}\")", "149": "    except Exception as e:", "150": "        raise ConfigurationError(f\"Error reading config file: {e}\")", "151": "", "152": "", "153": "def find_config_file(start_path: Path = None) -> Optional[Path]:", "154": "    \"\"\"", "155": "    Find config file in current directory or parent directories.", "156": "", "157": "    Args:", "158": "        start_path: Starting directory (default: current directory)", "159": "", "160": "    Returns:", "161": "        Path to config file or None if not found", "162": "    \"\"\"", "163": "    if start_path is None:", "164": "        start_path = Path.cwd()", "165": "", "166": "    config_names = [\".testiq.yaml\", \".testiq.yml\", \".testiq.toml\", \"testiq.yaml\", \"testiq.yml\"]", "167": "", "168": "    # Search current directory and parents", "169": "    current = start_path.resolve()", "170": "    for _ in range(10):  # Limit search depth", "171": "        for config_name in config_names:", "172": "            config_path = current / config_name", "173": "            if config_path.exists():", "174": "                return config_path", "175": "", "176": "        parent = current.parent", "177": "        if parent == current:  # Reached root", "178": "            break", "179": "        current = parent", "180": "", "181": "    return None", "182": "", "183": "", "184": "def load_config_from_env() -> dict[str, Any]:", "185": "    \"\"\"", "186": "    Load configuration from environment variables.", "187": "", "188": "    Environment variables:", "189": "        TESTIQ_LOG_LEVEL: Log level", "190": "        TESTIQ_LOG_FILE: Log file path", "191": "        TESTIQ_MAX_FILE_SIZE: Maximum file size in bytes", "192": "        TESTIQ_MAX_TESTS: Maximum number of tests", "193": "        TESTIQ_ENABLE_PARALLEL: Enable parallel processing (true/false)", "194": "        TESTIQ_MAX_WORKERS: Maximum number of workers", "195": "        TESTIQ_SIMILARITY_THRESHOLD: Similarity threshold (0.0-1.0)", "196": "", "197": "    Returns:", "198": "        Configuration dictionary", "199": "    \"\"\"", "200": "    config: dict[str, Any] = {}", "201": "", "202": "    # Log config", "203": "    if \"TESTIQ_LOG_LEVEL\" in os.environ:", "204": "        config.setdefault(\"log\", {})[\"level\"] = os.environ[\"TESTIQ_LOG_LEVEL\"]", "205": "    if \"TESTIQ_LOG_FILE\" in os.environ:", "206": "        config.setdefault(\"log\", {})[\"file\"] = os.environ[\"TESTIQ_LOG_FILE\"]", "207": "", "208": "    # Security config", "209": "    if \"TESTIQ_MAX_FILE_SIZE\" in os.environ:", "210": "        config.setdefault(\"security\", {})[\"max_file_size\"] = int(os.environ[\"TESTIQ_MAX_FILE_SIZE\"])", "211": "    if \"TESTIQ_MAX_TESTS\" in os.environ:", "212": "        config.setdefault(\"security\", {})[\"max_tests\"] = int(os.environ[\"TESTIQ_MAX_TESTS\"])", "213": "", "214": "    # Performance config", "215": "    if \"TESTIQ_ENABLE_PARALLEL\" in os.environ:", "216": "        config.setdefault(\"performance\", {})[\"enable_parallel\"] = (", "217": "            os.environ[\"TESTIQ_ENABLE_PARALLEL\"].lower() == \"true\"", "218": "        )", "219": "    if \"TESTIQ_MAX_WORKERS\" in os.environ:", "220": "        config.setdefault(\"performance\", {})[\"max_workers\"] = int(os.environ[\"TESTIQ_MAX_WORKERS\"])", "221": "", "222": "    # Analysis config", "223": "    if \"TESTIQ_SIMILARITY_THRESHOLD\" in os.environ:", "224": "        config.setdefault(\"analysis\", {})[\"similarity_threshold\"] = float(", "225": "            os.environ[\"TESTIQ_SIMILARITY_THRESHOLD\"]", "226": "        )", "227": "", "228": "    return config", "229": "", "230": "", "231": "def load_config(config_path: Optional[Path] = None) -> Config:", "232": "    \"\"\"", "233": "    Load configuration from file and environment.", "234": "", "235": "    Priority (highest to lowest):", "236": "        1. Environment variables", "237": "        2. Specified config file", "238": "        3. Auto-discovered config file", "239": "        4. Default values", "240": "", "241": "    Args:", "242": "        config_path: Path to config file (optional)", "243": "", "244": "    Returns:", "245": "        TestIQ configuration", "246": "    \"\"\"", "247": "    config_data: dict[str, Any] = {}", "248": "", "249": "    # 1. Load from auto-discovered file", "250": "    if config_path is None:", "251": "        config_path = find_config_file()", "252": "", "253": "    # 2. Load from specified or discovered file", "254": "    if config_path:", "255": "        file_config = load_config_file(config_path)", "256": "        config_data = _deep_merge(config_data, file_config)", "257": "", "258": "    # 3. Override with environment variables", "259": "    env_config = load_config_from_env()", "260": "    config_data = _deep_merge(config_data, env_config)", "261": "", "262": "    # 4. Create config with defaults", "263": "    return Config.from_dict(config_data)", "264": "", "265": "", "266": "def _deep_merge(base: dict[str, Any], override: dict[str, Any]) -> dict[str, Any]:", "267": "    \"\"\"Deep merge two dictionaries.\"\"\"", "268": "    result = base.copy()", "269": "    for key, value in override.items():", "270": "        if key in result and isinstance(result[key], dict) and isinstance(value, dict):", "271": "            result[key] = _deep_merge(result[key], value)", "272": "        else:", "273": "            result[key] = value", "274": "    return result"}, "tests/test_init.py": {"1": "\"\"\"Tests for main package __init__.py.\"\"\"", "2": "", "3": "import testiq", "4": "", "5": "", "6": "class TestPackageInit:", "7": "    \"\"\"Tests for package initialization.\"\"\"", "8": "", "9": "    def test_version_exists(self):", "10": "        \"\"\"Test that __version__ is defined.\"\"\"", "11": "        assert hasattr(testiq, \"__version__\")", "12": "        assert isinstance(testiq.__version__, str)", "13": "", "14": "    def test_version_format(self):", "15": "        \"\"\"Test version follows semantic versioning format.\"\"\"", "16": "        version = testiq.__version__", "17": "        parts = version.split(\".\")", "18": "        assert len(parts) >= 2  # At least major.minor", "19": "        assert parts[0].isdigit()", "20": "        assert parts[1].isdigit()", "21": "", "22": "    def test_exports_coverage_duplicate_finder(self):", "23": "        \"\"\"Test that CoverageDuplicateFinder is exported.\"\"\"", "24": "        assert hasattr(testiq, \"CoverageDuplicateFinder\")", "25": "        assert \"CoverageDuplicateFinder\" in testiq.__all__", "26": "", "27": "    def test_exports_coverage_data(self):", "28": "        \"\"\"Test that CoverageData is exported.\"\"\"", "29": "        assert hasattr(testiq, \"CoverageData\")", "30": "        assert \"CoverageData\" in testiq.__all__", "31": "", "32": "    def test_all_list_complete(self):", "33": "        \"\"\"Test that __all__ contains expected exports.\"\"\"", "34": "        expected = {\"CoverageDuplicateFinder\", \"CoverageData\", \"__version__\"}", "35": "        actual = set(testiq.__all__)", "36": "        assert expected == actual", "37": "", "38": "    def test_can_instantiate_finder(self):", "39": "        \"\"\"Test that we can instantiate CoverageDuplicateFinder from package.\"\"\"", "40": "        finder = testiq.CoverageDuplicateFinder()", "41": "        assert finder is not None", "42": "", "43": "    def test_can_instantiate_coverage_data(self):", "44": "        \"\"\"Test that we can instantiate CoverageData from package.\"\"\"", "45": "        test_cov = testiq.CoverageData(", "46": "            test_name=\"test_example\",", "47": "            covered_lines={(\"file.py\", 1), (\"file.py\", 2), (\"file.py\", 3)}", "48": "        )", "49": "        assert test_cov is not None", "50": "        assert test_cov.test_name == \"test_example\""}, "tests/test_analyzer.py": {"1": "\"\"\"Tests for TestIQ analyzer module.\"\"\"", "2": "", "3": "import pytest", "4": "", "5": "from testiq.analyzer import CoverageDuplicateFinder, CoverageData", "6": "", "7": "", "8": "class TestCoverageDuplicateFinder:", "9": "    \"\"\"Test suite for CoverageDuplicateFinder class.\"\"\"", "10": "", "11": "    def test_add_test_coverage(self):", "12": "        \"\"\"Test adding coverage data including single and multiple files.\"\"\"", "13": "        finder = CoverageDuplicateFinder()", "14": "", "15": "        # Test with multiple files", "16": "        finder.add_test_coverage(\"test_1\", {\"file1.py\": [1, 2, 3], \"file2.py\": [10, 20]})", "17": "", "18": "        assert len(finder.tests) == 1", "19": "        assert finder.tests[0].test_name == \"test_1\"", "20": "        assert (\"file1.py\", 1) in finder.tests[0].covered_lines", "21": "        assert (\"file2.py\", 10) in finder.tests[0].covered_lines", "22": "", "23": "        # Test with many files spanning different modules", "24": "        finder.add_test_coverage(", "25": "            \"test_multifile\", {\"auth.py\": [10, 11, 12], \"user.py\": [5, 6, 7], \"db.py\": [100, 101]}", "26": "        )", "27": "", "28": "        assert len(finder.tests) == 2", "29": "        assert finder.tests[1].test_name == \"test_multifile\"", "30": "        assert len(finder.tests[1].covered_lines) == 8  # 3 + 3 + 2 lines", "31": "", "32": "    def test_duplicate_detection_all_scenarios(self):", "33": "        \"\"\"Test exact duplicates, subset duplicates, and negative cases in one comprehensive test.\"\"\"", "34": "        # Scenario 1: Exact duplicates", "35": "        finder1 = CoverageDuplicateFinder()", "36": "        for i in range(1, 4):", "37": "            finder1.add_test_coverage(f\"test_{i}\", {\"file.py\": [1, 2, 3]})", "38": "        finder1.add_test_coverage(\"test_different\", {\"file.py\": [10, 20]})", "39": "        duplicates = finder1.find_exact_duplicates()", "40": "        assert len(duplicates) == 1", "41": "        assert len(duplicates[0]) == 3", "42": "        assert set(duplicates[0]) == {\"test_1\", \"test_2\", \"test_3\"}", "43": "", "44": "        # Scenario 2: No exact duplicates", "45": "        finder2 = CoverageDuplicateFinder()", "46": "        finder2.add_test_coverage(\"test_1\", {\"file.py\": [1, 2]})", "47": "        finder2.add_test_coverage(\"test_2\", {\"file.py\": [3, 4]})", "48": "        assert len(finder2.find_exact_duplicates()) == 0", "49": "", "50": "        # Scenario 3: Subset duplicates", "51": "        finder3 = CoverageDuplicateFinder()", "52": "        finder3.add_test_coverage(\"test_minimal\", {\"file.py\": [1, 2, 3]})", "53": "        finder3.add_test_coverage(\"test_complete\", {\"file.py\": [1, 2, 3, 4, 5, 6, 7, 8, 9]})", "54": "        subsets = finder3.find_subset_duplicates()", "55": "        assert len(subsets) == 1", "56": "        assert subsets[0][0] == \"test_minimal\"", "57": "        assert subsets[0][1] == \"test_complete\"", "58": "        assert subsets[0][2] == pytest.approx(0.333, rel=0.01)", "59": "", "60": "        # Scenario 4: No subset duplicates", "61": "        assert len(finder2.find_subset_duplicates()) == 0", "62": "", "63": "    def test_similarity_detection_scenarios(self):", "64": "        \"\"\"Test similarity detection, sorting, and threshold variations.\"\"\"", "65": "        finder = CoverageDuplicateFinder()", "66": "", "67": "        # Test 1: Finding and sorting similar coverage", "68": "        # Add test with ~67% similarity", "69": "        finder.add_test_coverage(\"test_a\", {\"file.py\": [1, 2, 3, 4, 5]})", "70": "        finder.add_test_coverage(\"test_b\", {\"file.py\": [1, 2, 3, 4, 10]})", "71": "", "72": "        # Add test with ~82% similarity (higher)", "73": "        finder.add_test_coverage(\"test_c\", {\"file.py\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})", "74": "        finder.add_test_coverage(\"test_d\", {\"file.py\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 20]})", "75": "", "76": "        similar = finder.find_similar_coverage(threshold=0.5)", "77": "", "78": "        # Should find both pairs", "79": "        assert len(similar) >= 2", "80": "", "81": "        # Results should be sorted by similarity (descending)", "82": "        if len(similar) >= 2:", "83": "            assert similar[0][2] >= similar[1][2]", "84": "            # Highest similarity should be ~82%", "85": "            assert similar[0][2] > 0.8", "86": "", "87": "        # Test 2: Threshold variations", "88": "        finder2 = CoverageDuplicateFinder()", "89": "        finder2.add_test_coverage(\"test_1\", {\"file.py\": [1, 2, 3, 4, 5]})", "90": "        finder2.add_test_coverage(\"test_2\", {\"file.py\": [1, 2, 3, 4, 10]})", "91": "", "92": "        # With high threshold, should not match (similarity ~67%)", "93": "        similar_high = finder2.find_similar_coverage(threshold=0.8)", "94": "        assert len(similar_high) == 0", "95": "", "96": "        # With matching threshold, should find them", "97": "        similar_low = finder2.find_similar_coverage(threshold=0.6)", "98": "        assert len(similar_low) == 1", "99": "        assert similar_low[0][2] == pytest.approx(0.667, rel=0.01)", "100": "", "101": "", "102": "    def test_generate_report(self):", "103": "        \"\"\"Test report generation.\"\"\"", "104": "        finder = CoverageDuplicateFinder()", "105": "", "106": "        finder.add_test_coverage(\"test_1\", {\"file.py\": [1, 2, 3]})", "107": "        finder.add_test_coverage(\"test_2\", {\"file.py\": [1, 2, 3]})", "108": "", "109": "        report = finder.generate_report()", "110": "", "111": "        assert \"Test Duplication Report\" in report", "112": "        assert \"Exact Duplicates\" in report", "113": "        assert \"test_1\" in report", "114": "        assert \"test_2\" in report", "115": "        assert \"Summary\" in report", "116": "", "117": "    def test_empty_finder(self):", "118": "        \"\"\"Test finder with no tests added.\"\"\"", "119": "        finder = CoverageDuplicateFinder()", "120": "", "121": "        assert len(finder.tests) == 0", "122": "        assert finder.find_exact_duplicates() == []", "123": "        assert finder.find_subset_duplicates() == []", "124": "        assert finder.find_similar_coverage() == []", "125": "", "126": "", "127": "", "128": "class TestCoverageDataClass:", "129": "    \"\"\"Test suite for CoverageData class.\"\"\"", "130": "", "131": "    def test_create_coverage_data(self):", "132": "        \"\"\"Test creating a CoverageData instance.\"\"\"", "133": "        coverage = CoverageData(", "134": "            test_name=\"test_example\", covered_lines={(\"file.py\", 1), (\"file.py\", 2)}", "135": "        )", "136": "", "137": "        assert coverage.test_name == \"test_example\"", "138": "        assert len(coverage.covered_lines) == 2", "139": "        assert (\"file.py\", 1) in coverage.covered_lines", "140": "", "141": "    def test_test_coverage_hash(self):", "142": "        \"\"\"Test that CoverageData instances are hashable.\"\"\"", "143": "        coverage1 = CoverageData(\"test_1\", {(\"file.py\", 1)})", "144": "        coverage2 = CoverageData(\"test_2\", {(\"file.py\", 2)})", "145": "", "146": "        # Should be able to add to set", "147": "        coverage_set = {coverage1, coverage2}", "148": "        assert len(coverage_set) == 2"}, "src/testiq/performance.py": {"1": "\"\"\"", "2": "Performance optimization utilities for TestIQ.", "3": "Provides parallel processing, caching, and streaming capabilities.", "4": "\"\"\"", "5": "", "6": "import hashlib", "7": "import json", "8": "import pickle", "9": "from collections.abc import Iterator", "10": "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed", "11": "from functools import lru_cache", "12": "from pathlib import Path", "13": "from typing import Any, Callable, Optional", "14": "", "15": "from testiq.exceptions import AnalysisError", "16": "from testiq.logging_config import get_logger", "17": "", "18": "logger = get_logger(__name__)", "19": "", "20": "", "21": "class CacheManager:", "22": "    \"\"\"Manages caching of analysis results.\"\"\"", "23": "", "24": "    def __init__(self, cache_dir: Optional[Path] = None, enabled: bool = True) -> None:", "25": "        \"\"\"", "26": "        Initialize cache manager.", "27": "", "28": "        Args:", "29": "            cache_dir: Directory for cache files (default: ~/.testiq/cache)", "30": "            enabled: Whether caching is enabled", "31": "        \"\"\"", "32": "        self.enabled = enabled", "33": "        if cache_dir:", "34": "            self.cache_dir = Path(cache_dir)", "35": "        else:", "36": "            self.cache_dir = Path.home() / \".testiq\" / \"cache\"", "37": "", "38": "        if self.enabled:", "39": "            self.cache_dir.mkdir(parents=True, exist_ok=True)", "40": "            logger.debug(f\"Cache directory: {self.cache_dir}\")", "41": "", "42": "    def _get_cache_key(self, data: Any) -> str:", "43": "        \"\"\"Generate cache key from data.\"\"\"", "44": "        if isinstance(data, dict):", "45": "            data_str = json.dumps(data, sort_keys=True)", "46": "        else:", "47": "            data_str = str(data)", "48": "        return hashlib.sha256(data_str.encode()).hexdigest()[:16]", "49": "", "50": "    def get(self, key: str) -> Optional[Any]:", "51": "        \"\"\"", "52": "        Get cached result.", "53": "", "54": "        Args:", "55": "            key: Cache key", "56": "", "57": "        Returns:", "58": "            Cached data or None if not found", "59": "        \"\"\"", "60": "        if not self.enabled:", "61": "            return None", "62": "", "63": "        cache_file = self.cache_dir / f\"{key}.cache\"", "64": "        if cache_file.exists():", "65": "            try:", "66": "                with open(cache_file, \"rb\") as f:", "67": "                    logger.debug(f\"Cache hit: {key}\")", "68": "                    return pickle.load(f)", "69": "            except Exception as e:", "70": "                logger.warning(f\"Failed to load cache {key}: {e}\")", "71": "                return None", "72": "        return None", "73": "", "74": "    def set(self, key: str, value: Any) -> None:", "75": "        \"\"\"", "76": "        Store result in cache.", "77": "", "78": "        Args:", "79": "            key: Cache key", "80": "            value: Data to cache", "81": "        \"\"\"", "82": "        if not self.enabled:", "83": "            return", "84": "", "85": "        cache_file = self.cache_dir / f\"{key}.cache\"", "86": "        try:", "87": "            with open(cache_file, \"wb\") as f:", "88": "                pickle.dump(value, f)", "89": "            logger.debug(f\"Cached result: {key}\")", "90": "        except Exception as e:", "91": "            logger.warning(f\"Failed to save cache {key}: {e}\")", "92": "", "93": "    def clear(self) -> None:", "94": "        \"\"\"Clear all cached data.\"\"\"", "95": "        if not self.enabled:", "96": "            return", "97": "", "98": "        try:", "99": "            for cache_file in self.cache_dir.glob(\"*.cache\"):", "100": "                cache_file.unlink()", "101": "            logger.info(\"Cache cleared\")", "102": "        except Exception as e:", "103": "            logger.warning(f\"Failed to clear cache: {e}\")", "104": "", "105": "", "106": "class StreamingJSONParser:", "107": "    \"\"\"Parse large JSON files in streaming fashion.\"\"\"", "108": "", "109": "    @staticmethod", "110": "    def parse_coverage_file(file_path: Path, chunk_size: int = 1024) -> Iterator[tuple[str, dict]]:", "111": "        \"\"\"", "112": "        Parse coverage JSON file in chunks.", "113": "", "114": "        Args:", "115": "            file_path: Path to JSON file", "116": "            chunk_size: Number of tests to yield at once", "117": "", "118": "        Yields:", "119": "            (test_name, coverage_data) tuples", "120": "        \"\"\"", "121": "        try:", "122": "            with open(file_path) as f:", "123": "                data = json.load(f)", "124": "", "125": "            if not isinstance(data, dict):", "126": "                raise AnalysisError(\"Coverage file must contain a dictionary\")", "127": "", "128": "            items = list(data.items())", "129": "            for i in range(0, len(items), chunk_size):", "130": "                chunk = items[i : i + chunk_size]", "131": "                yield from chunk", "132": "", "133": "        except json.JSONDecodeError as e:", "134": "            raise AnalysisError(f\"Invalid JSON in coverage file: {e}\")", "135": "        except Exception as e:", "136": "            raise AnalysisError(f\"Error reading coverage file: {e}\")", "137": "", "138": "", "139": "class ParallelProcessor:", "140": "    \"\"\"Process tests in parallel for better performance.\"\"\"", "141": "", "142": "    def __init__(", "143": "        self, max_workers: int = 4, use_processes: bool = False, enabled: bool = True", "144": "    ) -> None:", "145": "        \"\"\"", "146": "        Initialize parallel processor.", "147": "", "148": "        Args:", "149": "            max_workers: Maximum number of parallel workers", "150": "            use_processes: Use ProcessPoolExecutor instead of ThreadPoolExecutor", "151": "            enabled: Whether parallel processing is enabled", "152": "        \"\"\"", "153": "        self.max_workers = max_workers", "154": "        self.use_processes = use_processes", "155": "        self.enabled = enabled", "156": "        logger.debug(", "157": "            f\"Parallel processing: enabled={enabled}, workers={max_workers}, \"", "158": "            f\"processes={use_processes}\"", "159": "        )", "160": "", "161": "    def map(self, func: Callable, items: list[Any], desc: str = \"Processing\") -> list[Any]:", "162": "        \"\"\"", "163": "        Map function over items in parallel.", "164": "", "165": "        Args:", "166": "            func: Function to apply to each item", "167": "            items: List of items to process", "168": "            desc: Description for logging", "169": "", "170": "        Returns:", "171": "            List of results", "172": "        \"\"\"", "173": "        if not self.enabled or len(items) < 2:", "174": "            logger.debug(f\"Sequential processing: {len(items)} items\")", "175": "            return [func(item) for item in items]", "176": "", "177": "        logger.info(f\"{desc}: {len(items)} items with {self.max_workers} workers\")", "178": "", "179": "        executor_class = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor", "180": "", "181": "        try:", "182": "            with executor_class(max_workers=self.max_workers) as executor:", "183": "                futures = {executor.submit(func, item): i for i, item in enumerate(items)}", "184": "                results = [None] * len(items)", "185": "", "186": "                for future in as_completed(futures):", "187": "                    idx = futures[future]", "188": "                    try:", "189": "                        results[idx] = future.result()", "190": "                    except Exception as e:", "191": "                        logger.error(f\"Error processing item {idx}: {e}\")", "192": "                        results[idx] = None", "193": "", "194": "                return results", "195": "", "196": "        except Exception as e:", "197": "            logger.error(f\"Parallel processing failed: {e}. Falling back to sequential.\")", "198": "            return [func(item) for item in items]", "199": "", "200": "", "201": "@lru_cache(maxsize=1024)", "202": "def compute_similarity(lines1_frozen: frozenset, lines2_frozen: frozenset) -> float:", "203": "    \"\"\"", "204": "    Compute Jaccard similarity between two sets of lines (cached).", "205": "", "206": "    Args:", "207": "        lines1_frozen: First set of lines (frozenset for hashability)", "208": "        lines2_frozen: Second set of lines", "209": "", "210": "    Returns:", "211": "        Similarity score (0.0 to 1.0)", "212": "    \"\"\"", "213": "    lines1 = set(lines1_frozen)", "214": "    lines2 = set(lines2_frozen)", "215": "", "216": "    intersection = lines1 & lines2", "217": "    union = lines1 | lines2", "218": "", "219": "    if len(union) == 0:", "220": "        return 0.0", "221": "", "222": "    return len(intersection) / len(union)", "223": "", "224": "", "225": "class ProgressTracker:", "226": "    \"\"\"Track progress of long-running operations.\"\"\"", "227": "", "228": "    def __init__(self, total: int, desc: str = \"Processing\") -> None:", "229": "        \"\"\"", "230": "        Initialize progress tracker.", "231": "", "232": "        Args:", "233": "            total: Total number of items", "234": "            desc: Description of operation", "235": "        \"\"\"", "236": "        self.total = total", "237": "        self.current = 0", "238": "        self.desc = desc", "239": "        self.last_logged_percent = -1", "240": "", "241": "    def update(self, n: int = 1) -> None:", "242": "        \"\"\"", "243": "        Update progress.", "244": "", "245": "        Args:", "246": "            n: Number of items processed", "247": "        \"\"\"", "248": "        self.current += n", "249": "        percent = int((self.current / self.total) * 100)", "250": "", "251": "        # Log at 0%, 25%, 50%, 75%, 100%", "252": "        if percent >= self.last_logged_percent + 25 or percent == 100:", "253": "            logger.info(f\"{self.desc}: {percent}% ({self.current}/{self.total})\")", "254": "            self.last_logged_percent = percent", "255": "", "256": "", "257": "def batch_iterator(items: list[Any], batch_size: int) -> Iterator[list[Any]]:", "258": "    \"\"\"", "259": "    Iterate over items in batches.", "260": "", "261": "    Args:", "262": "        items: List of items", "263": "        batch_size: Size of each batch", "264": "", "265": "    Yields:", "266": "        Batches of items", "267": "    \"\"\"", "268": "    for i in range(0, len(items), batch_size):", "269": "        yield items[i : i + batch_size]"}, "src/testiq/coverage_converter.py": {"1": "\"\"\"", "2": "Convert pytest coverage.json to TestIQ format.", "3": "", "4": "Pytest's coverage.json provides aggregated coverage across all tests.", "5": "This converter creates synthetic per-test coverage by analyzing pytest's", "6": "test output with the --cov flag.", "7": "", "8": "Usage:", "9": "    python -m testiq.coverage_converter coverage.json -o testiq_coverage.json", "10": "", "11": "Note: This provides aggregated coverage, not true per-test coverage.", "12": "For accurate per-test data, use the pytest plugin instead.", "13": "\"\"\"", "14": "", "15": "import json", "16": "import sys", "17": "from pathlib import Path", "18": "from typing import Any, Dict, List", "19": "", "20": "import click", "21": "", "22": "from testiq.logging_config import get_logger", "23": "", "24": "logger = get_logger(__name__)", "25": "", "26": "", "27": "def convert_pytest_coverage(coverage_data: Dict[str, Any]) -> Dict[str, Dict[str, List[int]]]:", "28": "    \"\"\"", "29": "    Convert pytest coverage.json format to TestIQ format.", "30": "", "31": "    Args:", "32": "        coverage_data: Pytest coverage.json data", "33": "", "34": "    Returns:", "35": "        TestIQ format: {test_name: {filename: [line_numbers]}}", "36": "", "37": "    Note: Since pytest coverage is aggregated, this creates a single", "38": "    synthetic \"all_tests\" entry with all covered lines.", "39": "    \"\"\"", "40": "    if \"files\" not in coverage_data:", "41": "        raise ValueError(\"Invalid pytest coverage format: missing 'files' key\")", "42": "", "43": "    testiq_format: Dict[str, Dict[str, List[int]]] = {}", "44": "    all_coverage: Dict[str, List[int]] = {}", "45": "", "46": "    # Extract coverage from pytest format", "47": "    for filepath, file_data in coverage_data[\"files\"].items():", "48": "        if \"executed_lines\" not in file_data:", "49": "            continue", "50": "", "51": "        executed_lines = file_data[\"executed_lines\"]", "52": "        if not isinstance(executed_lines, list):", "53": "            logger.warning(f\"Skipping {filepath}: executed_lines is not a list\")", "54": "            continue", "55": "", "56": "        # Make path relative if possible", "57": "        try:", "58": "            rel_path = str(Path(filepath).relative_to(Path.cwd()))", "59": "        except ValueError:", "60": "            rel_path = filepath", "61": "", "62": "        all_coverage[rel_path] = sorted(executed_lines)", "63": "", "64": "    # Create synthetic \"all_tests\" entry since pytest doesn't track per-test", "65": "    if all_coverage:", "66": "        testiq_format[\"all_tests_aggregated\"] = all_coverage", "67": "", "68": "    return testiq_format", "69": "", "70": "", "71": "def convert_pytest_contexts(coverage_data: Dict[str, Any]) -> Dict[str, Dict[str, List[int]]]:", "72": "    \"\"\"", "73": "    Convert pytest coverage with contexts (if available).", "74": "", "75": "    Pytest can track coverage per test if run with:", "76": "        pytest --cov --cov-context=test", "77": "", "78": "    Args:", "79": "        coverage_data: Pytest coverage.json with contexts", "80": "", "81": "    Returns:", "82": "        TestIQ format with per-test coverage (if contexts available)", "83": "    \"\"\"", "84": "    # Check if contexts are available", "85": "    meta = coverage_data.get(\"meta\", {})", "86": "    if not meta.get(\"show_contexts\", False):", "87": "        logger.warning(", "88": "            \"Coverage data doesn't include test contexts. \"", "89": "            \"Run with: pytest --cov --cov-context=test\"", "90": "        )", "91": "        return convert_pytest_coverage(coverage_data)", "92": "", "93": "    # Parse contexts if available", "94": "    testiq_format: Dict[str, Dict[str, List[int]]] = {}", "95": "", "96": "    for filepath, file_data in coverage_data.get(\"files\", {}).items():", "97": "        # Look for context-specific coverage", "98": "        if \"contexts\" in file_data:", "99": "            # Group by test context", "100": "            for context, lines in file_data[\"contexts\"].items():", "101": "                if context and context != \"\":", "102": "                    # Make path relative", "103": "                    try:", "104": "                        rel_path = str(Path(filepath).relative_to(Path.cwd()))", "105": "                    except ValueError:", "106": "                        rel_path = filepath", "107": "", "108": "                    if context not in testiq_format:", "109": "                        testiq_format[context] = {}", "110": "", "111": "                    testiq_format[context][rel_path] = sorted(lines)", "112": "", "113": "    if not testiq_format:", "114": "        # Fall back to aggregated format", "115": "        return convert_pytest_coverage(coverage_data)", "116": "", "117": "    return testiq_format", "118": "", "119": "", "120": "@click.command()", "121": "@click.argument(\"coverage_file\", type=click.Path(exists=True, path_type=Path))", "122": "@click.option(", "123": "    \"--output\",", "124": "    \"-o\",", "125": "    type=click.Path(path_type=Path),", "126": "    default=\"testiq_coverage.json\",", "127": "    help=\"Output file for TestIQ format\",", "128": ")", "129": "@click.option(", "130": "    \"--with-contexts\",", "131": "    is_flag=True,", "132": "    help=\"Try to extract per-test coverage from pytest contexts\",", "133": ")", "134": "def main(coverage_file: Path, output: Path, with_contexts: bool) -> None:", "135": "    \"\"\"", "136": "    Convert pytest coverage.json to TestIQ format.", "137": "", "138": "    COVERAGE_FILE: Path to pytest coverage.json file", "139": "", "140": "    Examples:", "141": "        # Basic conversion (aggregated coverage)", "142": "        python -m testiq.coverage_converter coverage.json", "143": "", "144": "        # With test contexts (if available)", "145": "        python -m testiq.coverage_converter coverage.json --with-contexts", "146": "", "147": "    Note: For accurate per-test coverage, use the pytest plugin:", "148": "        pytest --testiq-output=testiq_coverage.json", "149": "    \"\"\"", "150": "    try:", "151": "        # Load pytest coverage", "152": "        with open(coverage_file) as f:", "153": "            coverage_data = json.load(f)", "154": "", "155": "        # Convert format", "156": "        if with_contexts:", "157": "            testiq_data = convert_pytest_contexts(coverage_data)", "158": "        else:", "159": "            testiq_data = convert_pytest_coverage(coverage_data)", "160": "", "161": "        # Save TestIQ format", "162": "        output.parent.mkdir(parents=True, exist_ok=True)", "163": "        with open(output, \"w\") as f:", "164": "            json.dump(testiq_data, f, indent=2)", "165": "", "166": "        click.echo(f\"\u2713 Converted {len(testiq_data)} test(s)\")", "167": "        click.echo(f\"\u2713 Saved to: {output}\")", "168": "", "169": "        if not with_contexts and len(testiq_data) == 1:", "170": "            click.echo()", "171": "            click.echo(\"\u26a0\ufe0f  Note: This is aggregated coverage (all tests combined)\")", "172": "            click.echo(\"   For per-test analysis:\")", "173": "            click.echo(\"   1. Use: pytest --cov --cov-context=test\")", "174": "            click.echo(\"   2. Or use: pytest --testiq-output=testiq_coverage.json\")", "175": "", "176": "    except Exception as e:", "177": "        click.echo(f\"Error: {e}\", err=True)", "178": "        sys.exit(1)", "179": "", "180": "", "181": "if __name__ == \"__main__\":", "182": "    main()"}, "tests/test_reporting.py": {"1": "\"\"\"", "2": "Tests for reporting module (HTML and CSV generators).", "3": "\"\"\"", "4": "", "5": "import json", "6": "import tempfile", "7": "from pathlib import Path", "8": "", "9": "import pytest", "10": "", "11": "from testiq.analyzer import CoverageDuplicateFinder", "12": "from testiq.reporting import CSVReportGenerator, HTMLReportGenerator", "13": "", "14": "", "15": "@pytest.fixture", "16": "def sample_finder():", "17": "    \"\"\"Create a finder with sample test data.\"\"\"", "18": "    finder = CoverageDuplicateFinder()", "19": "", "20": "    # Exact duplicates", "21": "    finder.add_test_coverage(\"test_login_1\", {\"auth.py\": [1, 2, 3], \"user.py\": [10, 11]})", "22": "    finder.add_test_coverage(\"test_login_2\", {\"auth.py\": [1, 2, 3], \"user.py\": [10, 11]})", "23": "", "24": "    # Subset duplicates", "25": "    finder.add_test_coverage(\"test_short\", {\"utils.py\": [5, 6]})", "26": "    finder.add_test_coverage(\"test_long\", {\"utils.py\": [5, 6, 7, 8, 9]})", "27": "", "28": "    # Similar tests", "29": "    finder.add_test_coverage(\"test_similar_1\", {\"main.py\": [1, 2, 3, 4, 5]})", "30": "    finder.add_test_coverage(\"test_similar_2\", {\"main.py\": [1, 2, 3, 4, 10]})", "31": "", "32": "    # Unique test", "33": "    finder.add_test_coverage(\"test_unique\", {\"other.py\": [100, 101, 102]})", "34": "", "35": "    return finder", "36": "", "37": "", "38": "class TestHTMLReportGenerator:", "39": "    \"\"\"Tests for HTMLReportGenerator.\"\"\"", "40": "", "41": "    def test_generate_html_report(self, sample_finder):", "42": "        \"\"\"Test generating HTML report with statistics cards.\"\"\"", "43": "        generator = HTMLReportGenerator(sample_finder)", "44": "", "45": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".html\", delete=False) as f:", "46": "            output_path = Path(f.name)", "47": "", "48": "        try:", "49": "            generator.generate(output_path, threshold=0.8)", "50": "", "51": "            # Verify file exists and has content", "52": "            assert output_path.exists()", "53": "            content = output_path.read_text()", "54": "", "55": "            # Check for key HTML elements", "56": "            assert \"<!DOCTYPE html>\" in content", "57": "            assert \"<html\" in content", "58": "            assert \"TestIQ Analysis Report\" in content", "59": "            assert \"</html>\" in content", "60": "", "61": "            # Check for CSS styling", "62": "            assert \"<style>\" in content", "63": "            assert \"background:\" in content", "64": "            assert \"gradient\" in content.lower()", "65": "", "66": "            # Check for data sections", "67": "            assert \"Exact Duplicates\" in content", "68": "            assert \"Subset Duplicates\" in content", "69": "            assert \"Similar Tests\" in content", "70": "", "71": "            # Check for test names", "72": "            assert \"test_login_1\" in content", "73": "            assert \"test_login_2\" in content", "74": "", "75": "            # Check for stats cards structure", "76": "            assert \"class=\\\"stats\\\"\" in content", "77": "            assert \"class=\\\"stat-card\" in content", "78": "", "79": "        finally:", "80": "            if output_path.exists():", "81": "                output_path.unlink()", "82": "", "83": "    def test_html_report_empty_finder(self):", "84": "        \"\"\"Test HTML report generation with no tests.\"\"\"", "85": "        finder = CoverageDuplicateFinder()", "86": "        generator = HTMLReportGenerator(finder)", "87": "", "88": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".html\", delete=False) as f:", "89": "            output_path = Path(f.name)", "90": "", "91": "        try:", "92": "            generator.generate(output_path)", "93": "", "94": "            assert output_path.exists()", "95": "            content = output_path.read_text()", "96": "            # Check that it shows 0 tests in stats", "97": "            assert \"<div class=\\\"stat-value\\\">0</div>\" in content", "98": "            assert \"No exact duplicates found\" in content", "99": "", "100": "        finally:", "101": "            if output_path.exists():", "102": "                output_path.unlink()", "103": "", "104": "", "105": "class TestCSVReportGenerator:", "106": "    \"\"\"Tests for CSVReportGenerator.\"\"\"", "107": "", "108": "    def test_csv_generation_and_format(self, sample_finder):", "109": "        \"\"\"Test CSV generation and validate format is parseable.\"\"\"", "110": "        import csv", "111": "", "112": "        generator = CSVReportGenerator(sample_finder)", "113": "", "114": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as f:", "115": "            output_path = Path(f.name)", "116": "", "117": "        try:", "118": "            generator.generate_exact_duplicates(output_path)", "119": "", "120": "            assert output_path.exists()", "121": "            content = output_path.read_text()", "122": "", "123": "            # Test 1: Check CSV headers and content", "124": "            assert \"Group\" in content", "125": "            assert \"Test Name\" in content", "126": "            assert \"test_login_1\" in content", "127": "            assert \"test_login_2\" in content", "128": "", "129": "            # Test 2: Validate CSV is parseable", "130": "            with open(output_path, newline=\"\") as csvfile:", "131": "                reader = csv.DictReader(csvfile)", "132": "                rows = list(reader)", "133": "", "134": "                # Should have at least one row", "135": "                assert len(rows) > 0", "136": "", "137": "                # Should have expected columns", "138": "                assert \"Group\" in reader.fieldnames", "139": "                assert \"Test Name\" in reader.fieldnames", "140": "", "141": "        finally:", "142": "            if output_path.exists():", "143": "                output_path.unlink()", "144": "", "145": "    def test_generate_subset_duplicates_csv(self, sample_finder):", "146": "        \"\"\"Test generating CSV for subset duplicates.\"\"\"", "147": "        generator = CSVReportGenerator(sample_finder)", "148": "", "149": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as f:", "150": "            output_path = Path(f.name)", "151": "", "152": "        try:", "153": "            generator.generate_subset_duplicates(output_path)", "154": "", "155": "            assert output_path.exists()", "156": "            content = output_path.read_text()", "157": "", "158": "            # Check CSV headers", "159": "            assert \"Subset Test\" in content", "160": "            assert \"Superset Test\" in content", "161": "            assert \"Coverage Ratio\" in content", "162": "", "163": "            # Check data", "164": "            assert \"test_short\" in content", "165": "            assert \"test_long\" in content", "166": "", "167": "        finally:", "168": "            if output_path.exists():", "169": "                output_path.unlink()", "170": "", "171": "    def test_generate_similar_tests_csv(self, sample_finder):", "172": "        \"\"\"Test generating CSV for similar tests.\"\"\"", "173": "        generator = CSVReportGenerator(sample_finder)", "174": "", "175": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as f:", "176": "            output_path = Path(f.name)", "177": "", "178": "        try:", "179": "            generator.generate_similar_tests(output_path, threshold=0.5)", "180": "", "181": "            assert output_path.exists()", "182": "            content = output_path.read_text()", "183": "", "184": "            # Check CSV headers", "185": "            assert \"Test 1\" in content", "186": "            assert \"Test 2\" in content", "187": "            assert \"Similarity\" in content", "188": "", "189": "        finally:", "190": "            if output_path.exists():", "191": "                output_path.unlink()", "192": "", "193": "    def test_generate_summary_csv(self, sample_finder):", "194": "        \"\"\"Test generating summary CSV.\"\"\"", "195": "        generator = CSVReportGenerator(sample_finder)", "196": "", "197": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as f:", "198": "            output_path = Path(f.name)", "199": "", "200": "        try:", "201": "            generator.generate_summary(output_path, threshold=0.8)", "202": "", "203": "            assert output_path.exists()", "204": "            content = output_path.read_text()", "205": "", "206": "            # Check for summary sections", "207": "            assert \"Total Tests\" in content", "208": "            assert \"Exact Duplicates\" in content", "209": "            assert \"Subset Duplicates\" in content", "210": "", "211": "        finally:", "212": "            if output_path.exists():", "213": "                output_path.unlink()", "214": "", "215": "    def test_csv_report_empty_finder(self):", "216": "        \"\"\"Test CSV report generation with no tests.\"\"\"", "217": "        finder = CoverageDuplicateFinder()", "218": "        generator = CSVReportGenerator(finder)", "219": "", "220": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as f:", "221": "            output_path = Path(f.name)", "222": "", "223": "        try:", "224": "            generator.generate_summary(output_path)", "225": "", "226": "            assert output_path.exists()", "227": "            content = output_path.read_text()", "228": "            assert \"0\" in content  # Should show 0 tests", "229": "", "230": "        finally:", "231": "            if output_path.exists():", "232": "                output_path.unlink()", "233": "", "234": ""}, "src/testiq/reporting.py": {"1": "\"\"\"", "2": "Advanced reporting formats for TestIQ.", "3": "Generates HTML, CSV, and enhanced reports.", "4": "\"\"\"", "5": "", "6": "import csv", "7": "import json", "8": "from datetime import datetime", "9": "from pathlib import Path", "10": "from typing import Any, Optional", "11": "", "12": "from testiq.analyzer import CoverageDuplicateFinder", "13": "from testiq.logging_config import get_logger", "14": "from testiq.source_reader import SourceCodeReader", "15": "", "16": "logger = get_logger(__name__)", "17": "", "18": "", "19": "class HTMLReportGenerator:", "20": "    \"\"\"Generate beautiful HTML reports with charts and styling.\"\"\"", "21": "", "22": "    def __init__(self, finder: CoverageDuplicateFinder) -> None:", "23": "        \"\"\"Initialize HTML report generator.\"\"\"", "24": "        self.finder = finder", "25": "", "26": "    def _build_coverage_data_js(self) -> str:", "27": "        \"\"\"Build JavaScript code to populate coverage data.\"\"\"", "28": "        # Group lines by file", "29": "        file_coverage = {}", "30": "        for test in self.finder.tests:", "31": "            for filename, line in test.covered_lines:", "32": "                if filename not in file_coverage:", "33": "                    file_coverage[filename] = {'lines': set(), 'tests': set()}", "34": "                file_coverage[filename]['lines'].add(line)", "35": "                file_coverage[filename]['tests'].add(test.test_name)", "36": "", "37": "        # Build JS code", "38": "        js_lines = []", "39": "        for filename, data in sorted(file_coverage.items()):", "40": "            lines_count = len(data['lines'])", "41": "            tests_count = len(data['tests'])", "42": "            js_lines.append(f\"coverageByFile[{json.dumps(filename)}] = {{lines: {lines_count}, tests: {tests_count}}};\")", "43": "", "44": "        return '\\n        '.join(js_lines)", "45": "", "46": "    def generate(", "47": "        self,", "48": "        output_path: Path,", "49": "        title: str = \"TestIQ Analysis Report\",", "50": "        threshold: float = 0.3,", "51": "    ) -> None:", "52": "        \"\"\"", "53": "        Generate HTML report.", "54": "", "55": "        Args:", "56": "            output_path: Path to save HTML file", "57": "            title: Report title", "58": "            threshold: Similarity threshold for analysis (default: 0.3 = 30%)", "59": "        \"\"\"", "60": "        from testiq import __version__", "61": "", "62": "        logger.info(f\"Generating HTML report: {output_path}\")", "63": "        logger.info(f\"  Threshold: {threshold:.1%}\")", "64": "        logger.info(f\"  Total tests: {len(self.finder.tests)}\")", "65": "", "66": "        exact_dups = self.finder.find_exact_duplicates()", "67": "        subset_dups = self.finder.get_sorted_subset_duplicates()  # Use sorted version", "68": "        similar = self.finder.find_similar_coverage(threshold)", "69": "        duplicate_count = self.finder.get_duplicate_count()", "70": "", "71": "        logger.info(f\"  Exact duplicate groups: {len(exact_dups)} ({duplicate_count} tests)\")", "72": "        logger.info(f\"  Subset duplicates: {len(subset_dups)}\")", "73": "        logger.info(f\"  Similar pairs: {len(similar)}\")", "74": "", "75": "        html = self._generate_html(title, exact_dups, subset_dups, similar, threshold, __version__)", "76": "", "77": "        output_path.write_text(html)", "78": "        logger.info(f\"HTML report saved: {output_path}\")", "79": "", "80": "    def _generate_html(", "81": "        self,", "82": "        title: str,", "83": "        exact_dups: list[list[str]],", "84": "        subset_dups: list[tuple[str, str, float]],", "85": "        similar: list[tuple[str, str, float]],", "86": "        threshold: float,", "87": "        version: str,", "88": "    ) -> str:", "89": "        \"\"\"Generate HTML content.\"\"\"", "90": "        total_tests = len(self.finder.tests)", "91": "        duplicate_count = self.finder.get_duplicate_count()", "92": "", "93": "        # Collect and read source files for the split-screen view", "94": "        source_reader = SourceCodeReader()", "95": "        all_files = set()", "96": "        unique_lines_covered = set()", "97": "        for test in self.finder.tests:", "98": "            for filename, line in test.covered_lines:", "99": "                all_files.add(filename)", "100": "                unique_lines_covered.add((filename, line))", "101": "", "102": "        source_code_map = source_reader.read_multiple(list(all_files))", "103": "", "104": "        # Calculate total lines in all files", "105": "        total_lines_in_files = sum(len(lines) for lines in source_code_map.values())", "106": "        lines_covered = len(unique_lines_covered)", "107": "", "108": "        coverage_percentage = (lines_covered / total_lines_in_files * 100) if total_lines_in_files > 0 else 0", "109": "", "110": "        html = f\"\"\"<!DOCTYPE html>", "111": "<html lang=\"en\">", "112": "<head>", "113": "    <meta charset=\"UTF-8\">", "114": "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "115": "    <title>{title}</title>", "116": "    <style>", "117": "        * {{", "118": "            margin: 0;", "119": "            padding: 0;", "120": "            box-sizing: border-box;", "121": "        }}", "122": "        body {{", "123": "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;", "124": "            line-height: 1.6;", "125": "            color: #333;", "126": "            background: #f5f7fa;", "127": "            padding: 20px;", "128": "        }}", "129": "        .container {{", "130": "            max-width: 1600px;", "131": "            margin: 0 auto;", "132": "            background: white;", "133": "            border-radius: 8px;", "134": "            box-shadow: 0 2px 8px rgba(0,0,0,0.1);", "135": "            padding: 40px;", "136": "        }}", "137": "        h1 {{", "138": "            color: #2c3e50;", "139": "            margin-bottom: 10px;", "140": "            font-size: 2.5em;", "141": "        }}", "142": "        .timestamp {{", "143": "            color: #7f8c8d;", "144": "            font-size: 0.9em;", "145": "            margin-bottom: 30px;", "146": "        }}", "147": "        .stats {{", "148": "            display: grid;", "149": "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));", "150": "            gap: 20px;", "151": "            margin-bottom: 40px;", "152": "        }}", "153": "        .stat-card {{", "154": "            background: linear-gradient(135deg, #4a5568 0%, #2d3748 100%);", "155": "            color: white;", "156": "            padding: 20px;", "157": "            border-radius: 8px;", "158": "            box-shadow: 0 4px 6px rgba(0,0,0,0.1);", "159": "            cursor: pointer;", "160": "            transition: transform 0.2s ease, box-shadow 0.2s ease;", "161": "        }}", "162": "        .stat-card:hover {{", "163": "            transform: translateY(-3px);", "164": "            box-shadow: 0 6px 12px rgba(0,0,0,0.15);", "165": "        }}", "166": "        .stat-card.danger {{", "167": "            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);", "168": "        }}", "169": "        .stat-card.success {{", "170": "            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);", "171": "        }}", "172": "        .stat-card.warning {{", "173": "            background: linear-gradient(135deg, #f7971e 0%, #ffd200 100%);", "174": "        }}", "175": "        .stat-card.info {{", "176": "            background: linear-gradient(135deg, #00c6ff 0%, #0072ff 100%);", "177": "        }}", "178": "        .stat-value {{", "179": "            font-size: 2.5em;", "180": "            font-weight: bold;", "181": "            margin-bottom: 5px;", "182": "        }}", "183": "        .stat-label {{", "184": "            font-size: 0.9em;", "185": "            opacity: 0.9;", "186": "        }}", "187": "        h2 {{", "188": "            color: #2c3e50;", "189": "            margin-top: 40px;", "190": "            margin-bottom: 20px;", "191": "            padding-bottom: 10px;", "192": "            border-bottom: 3px solid #00c6ff;", "193": "        }}", "194": "        table {{", "195": "            width: 100%;", "196": "            border-collapse: collapse;", "197": "            margin-bottom: 30px;", "198": "            background: white;", "199": "        }}", "200": "        th {{", "201": "            background: #00c6ff;", "202": "            color: white;", "203": "            padding: 12px;", "204": "            text-align: left;", "205": "            font-weight: 600;", "206": "        }}", "207": "        td {{", "208": "            padding: 12px;", "209": "            border-bottom: 1px solid #ecf0f1;", "210": "            word-break: break-word;", "211": "            overflow-wrap: break-word;", "212": "            max-width: 400px;", "213": "        }}", "214": "        tr:hover {{", "215": "            background: #f8f9fa;", "216": "        }}", "217": "        .badge {{", "218": "            display: inline-block;", "219": "            padding: 4px 12px;", "220": "            border-radius: 12px;", "221": "            font-size: 0.85em;", "222": "            font-weight: 600;", "223": "        }}", "224": "        .badge-danger {{", "225": "            background: #fee;", "226": "            color: #c33;", "227": "        }}", "228": "        .badge-warning {{", "229": "            background: #ffeaa7;", "230": "            color: #d63031;", "231": "        }}", "232": "        .badge-info {{", "233": "            background: #dfe6e9;", "234": "            color: #2d3436;", "235": "        }}", "236": "        .test-group {{", "237": "            background: #f8f9fa;", "238": "            padding: 15px;", "239": "            border-radius: 6px;", "240": "            margin-bottom: 15px;", "241": "            border-left: 4px solid #00c6ff;", "242": "        }}", "243": "        .test-name {{", "244": "            font-family: 'Courier New', monospace;", "245": "            font-size: 0.9em;", "246": "            background: #ecf0f1;", "247": "            padding: 2px 6px;", "248": "            border-radius: 3px;", "249": "            display: inline-block;", "250": "            max-width: 100%;", "251": "            line-height: 1.6;", "252": "        }}", "253": "        .test-name .test-part {{", "254": "            display: inline;", "255": "        }}", "256": "        .test-name .test-separator {{", "257": "            color: #3498db;", "258": "            font-weight: bold;", "259": "            margin: 0 2px;", "260": "        }}", "261": "        .action {{", "262": "            color: #27ae60;", "263": "            font-weight: 600;", "264": "        }}", "265": "        .footer {{", "266": "            margin-top: 60px;", "267": "            padding-top: 20px;", "268": "            border-top: 1px solid #ecf0f1;", "269": "            text-align: center;", "270": "            color: #7f8c8d;", "271": "            font-size: 0.9em;", "272": "        }}", "273": "        .clickable-row {{", "274": "            cursor: pointer;", "275": "            transition: all 0.2s ease;", "276": "        }}", "277": "        .clickable-row:hover {{", "278": "            background: #e8f4f8 !important;", "279": "            transform: translateX(3px);", "280": "        }}", "281": "        .modal {{", "282": "            display: none;", "283": "            position: fixed;", "284": "            z-index: 1000;", "285": "            left: 0;", "286": "            top: 0;", "287": "            width: 100%;", "288": "            height: 100%;", "289": "            background-color: rgba(0,0,0,0.7);", "290": "            animation: fadeIn 0.3s;", "291": "        }}", "292": "        .modal-content {{", "293": "            background-color: white;", "294": "            margin: 10px auto;", "295": "            padding: 0;", "296": "            width: calc(100% - 20px);", "297": "            height: calc(100vh - 20px);", "298": "            border-radius: 8px;", "299": "            box-shadow: 0 4px 20px rgba(0,0,0,0.3);", "300": "            display: flex;", "301": "            flex-direction: column;", "302": "            overflow: hidden;", "303": "        }}", "304": "        .modal-header {{", "305": "            padding: 20px;", "306": "            background: linear-gradient(135deg, #00c6ff 0%, #764ba2 100%);", "307": "            color: white;", "308": "            border-radius: 8px 8px 0 0;", "309": "            display: flex;", "310": "            justify-content: space-between;", "311": "            align-items: center;", "312": "            flex-shrink: 0;", "313": "        }}", "314": "        .close {{", "315": "            color: white;", "316": "            font-size: 32px;", "317": "            font-weight: bold;", "318": "            cursor: pointer;", "319": "            line-height: 1;", "320": "            transition: transform 0.2s;", "321": "        }}", "322": "        .close:hover {{", "323": "            transform: scale(1.2);", "324": "        }}", "325": "        .split-view {{", "326": "            display: flex;", "327": "            flex: 1;", "328": "            overflow-y: auto;", "329": "            overflow-x: hidden;", "330": "            min-height: 0;", "331": "        }}", "332": "        .split-view.independent {{", "333": "            overflow: hidden;", "334": "        }}", "335": "        .file-panel {{", "336": "            flex: 1;", "337": "            display: flex;", "338": "            flex-direction: column;", "339": "            border-right: 2px solid #ecf0f1;", "340": "            min-width: 0;", "341": "        }}", "342": "        .file-panel.independent {{", "343": "            overflow-y: auto;", "344": "        }}", "345": "        .file-panel:last-child {{", "346": "            border-right: none;", "347": "        }}", "348": "        .panel-header {{", "349": "            padding: 15px;", "350": "            background: #f8f9fa;", "351": "            border-bottom: 2px solid #ecf0f1;", "352": "            font-weight: 600;", "353": "            color: #2c3e50;", "354": "            position: sticky;", "355": "            top: 0;", "356": "            z-index: 10;", "357": "            flex-shrink: 0;", "358": "        }}", "359": "        .file-content {{", "360": "            padding: 20px;", "361": "            font-family: 'Courier New', monospace;", "362": "            font-size: 0.9em;", "363": "            line-height: 1.8;", "364": "            background: #fafafa;", "365": "        }}", "366": "        .code-line {{", "367": "            padding: 2px 8px;", "368": "            border-radius: 3px;", "369": "            margin: 1px 0;", "370": "            white-space: pre;", "371": "        }}", "372": "        .covered-both {{", "373": "            background: #c8e6c9;", "374": "            border-left: 3px solid #4caf50;", "375": "            font-weight: 600;", "376": "        }}", "377": "        .covered-single {{", "378": "            background: #fff9c4;", "379": "            border-left: 3px solid #fbc02d;", "380": "            font-weight: 500;", "381": "        }}", "382": "        .covered {{", "383": "            background: #d4edda;", "384": "            border-left: 3px solid #28a745;", "385": "            font-weight: 600;", "386": "        }}", "387": "        .not-covered {{", "388": "            opacity: 0.6;", "389": "        }}", "390": "        .file-path {{", "391": "            font-family: 'Courier New', monospace;", "392": "            font-size: 0.85em;", "393": "            color: #7f8c8d;", "394": "            font-weight: bold;", "395": "            margin-bottom: 10px;", "396": "        }}", "397": "        .coverage-info {{", "398": "            background: #e8f4f8;", "399": "            padding: 15px;", "400": "            margin: 10px 20px;", "401": "            border-left: 4px solid #00c6ff;", "402": "            border-radius: 4px;", "403": "            display: flex;", "404": "            align-items: center;", "405": "            gap: 20px;", "406": "            flex-wrap: wrap;", "407": "            flex-shrink: 0;", "408": "        }}", "409": "        .filter-section {{", "410": "            display: flex;", "411": "            align-items: center;", "412": "            gap: 5px;", "413": "        }}", "414": "        .filter-select {{", "415": "            padding: 6px 10px;", "416": "            border: 1px solid rgba(255,255,255,0.5);", "417": "            border-radius: 4px;", "418": "            font-size: 0.85em;", "419": "            background: rgba(255,255,255,0.95);", "420": "            cursor: pointer;", "421": "            min-width: 180px;", "422": "            color: #2c3e50;", "423": "        }}", "424": "        .filter-select:hover {{", "425": "            background: white;", "426": "            border-color: white;", "427": "        }}", "428": "        .sync-toggle {{", "429": "            display: flex;", "430": "            align-items: center;", "431": "            gap: 6px;", "432": "            padding: 6px 12px;", "433": "            background: rgba(255,255,255,0.95);", "434": "            border: 1px solid rgba(255,255,255,0.5);", "435": "            border-radius: 4px;", "436": "            cursor: pointer;", "437": "            transition: all 0.2s;", "438": "            font-size: 0.85em;", "439": "            color: #2c3e50;", "440": "        }}", "441": "        .sync-toggle:hover {{", "442": "            background: white;", "443": "            border-color: white;", "444": "            background: #f0f0ff;", "445": "        }}", "446": "        .sync-toggle.active {{", "447": "            background: #00c6ff;", "448": "            color: white;", "449": "        }}", "450": "        .sync-checkbox {{", "451": "            width: 18px;", "452": "            height: 18px;", "453": "            cursor: pointer;", "454": "        }}", "455": "            border-left: 4px solid #00c6ff;", "456": "            border-radius: 4px;", "457": "        }}", "458": "        @keyframes fadeIn {{", "459": "            from {{ opacity: 0; }}", "460": "            to {{ opacity: 1; }}", "461": "        }}", "462": "        .progress-bar {{", "463": "            height: 30px;", "464": "            background: #ecf0f1;", "465": "            border-radius: 15px;", "466": "            overflow: hidden;", "467": "            margin: 20px 0;", "468": "        }}", "469": "        .progress-fill {{", "470": "            height: 100%;", "471": "            background: linear-gradient(90deg, #00c6ff 0%, #0072ff 100%);", "472": "            display: flex;", "473": "            align-items: center;", "474": "            justify-content: center;", "475": "            color: white;", "476": "            font-weight: 600;", "477": "            transition: width 0.3s ease;", "478": "        }}", "479": "        .tabs {{", "480": "            display: flex;", "481": "            gap: 10px;", "482": "            margin: 30px 0 20px 0;", "483": "            border-bottom: 2px solid #ecf0f1;", "484": "        }}", "485": "        .tab {{", "486": "            padding: 12px 24px;", "487": "            background: #f8f9fa;", "488": "            border: none;", "489": "            border-radius: 8px 8px 0 0;", "490": "            cursor: pointer;", "491": "            font-weight: 600;", "492": "            font-size: 16px;", "493": "            color: #7f8c8d;", "494": "            transition: all 0.3s ease;", "495": "            position: relative;", "496": "            bottom: -2px;", "497": "        }}", "498": "        .tab:hover {{", "499": "            background: #e9ecef;", "500": "            color: #495057;", "501": "        }}", "502": "        .tab.active {{", "503": "            background: white;", "504": "            color: #00c6ff;", "505": "            border-bottom: 2px solid #00c6ff;", "506": "        }}", "507": "        .tab-content {{", "508": "            display: none;", "509": "            animation: fadeIn 0.3s;", "510": "        }}", "511": "        .tab-content.active {{", "512": "            display: block;", "513": "        }}", "514": "        .pagination {{", "515": "            display: flex;", "516": "            justify-content: center;", "517": "            align-items: center;", "518": "            gap: 10px;", "519": "            margin: 20px 0;", "520": "        }}", "521": "        .pagination-controls {{", "522": "            display: flex;", "523": "            align-items: center;", "524": "            gap: 10px;", "525": "            margin-bottom: 20px;", "526": "            justify-content: space-between;", "527": "            flex-wrap: wrap;", "528": "        }}", "529": "        .page-size-selector {{", "530": "            display: flex;", "531": "            align-items: center;", "532": "            gap: 8px;", "533": "            font-size: 0.9em;", "534": "            color: #2c3e50;", "535": "        }}", "536": "        .page-size-selector label {{", "537": "            font-weight: 600;", "538": "        }}", "539": "        .page-size-selector select {{", "540": "            padding: 6px 12px;", "541": "            border: 2px solid #3498db;", "542": "            border-radius: 6px;", "543": "            background: white;", "544": "            color: #2c3e50;", "545": "            font-size: 0.9em;", "546": "            cursor: pointer;", "547": "            transition: all 0.2s ease;", "548": "        }}", "549": "        .page-size-selector select:hover {{", "550": "            border-color: #00c6ff;", "551": "            box-shadow: 0 2px 8px rgba(0, 198, 255, 0.2);", "552": "        }}", "553": "        .page-size-selector select:focus {{", "554": "            outline: none;", "555": "            border-color: #00c6ff;", "556": "            box-shadow: 0 0 0 3px rgba(0, 198, 255, 0.1);", "557": "        }}", "558": "        .page-btn {{", "559": "            padding: 8px 12px;", "560": "            background: #00c6ff;", "561": "            color: white;", "562": "            border: none;", "563": "            border-radius: 4px;", "564": "            cursor: pointer;", "565": "            font-weight: 600;", "566": "            transition: all 0.2s ease;", "567": "        }}", "568": "        .page-btn:hover:not(:disabled) {{", "569": "            background: #0088cc;", "570": "            transform: translateY(-2px);", "571": "        }}", "572": "        .page-btn:disabled {{", "573": "            background: #bdc3c7;", "574": "            cursor: not-allowed;", "575": "            opacity: 0.6;", "576": "        }}", "577": "        .page-info {{", "578": "            color: #7f8c8d;", "579": "            font-weight: 600;", "580": "        }}", "581": "        .loading {{", "582": "            text-align: center;", "583": "            padding: 20px;", "584": "            color: #00c6ff;", "585": "        }}", "586": "        .spinner {{", "587": "            border: 3px solid #f3f3f3;", "588": "            border-top: 3px solid #00c6ff;", "589": "            border-radius: 50%;", "590": "            width: 40px;", "591": "            height: 40px;", "592": "            animation: spin 1s linear infinite;", "593": "            margin: 20px auto;", "594": "        }}", "595": "        @keyframes spin {{", "596": "            0% {{ transform: rotate(0deg); }}", "597": "            100% {{ transform: rotate(360deg); }}", "598": "        }}", "599": "", "600": "        /* Responsive: Hide text on small screens, keep only icon */", "601": "        @media (max-width: 768px) {{", "602": "            .view-coverage-text {{", "603": "                display: none;", "604": "            }}", "605": "        }}", "606": "    </style>", "607": "</head>", "608": "<body>", "609": "    <div class=\"container\">", "610": "        <h1>\ud83e\uddea {title}</h1>", "611": "        <div class=\"timestamp\">Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>", "612": "", "613": "        <div class=\"stats\">", "614": "            <div class=\"stat-card\" onclick=\"switchTab('exact')\">", "615": "                <div class=\"stat-value\">{total_tests}</div>", "616": "                <div class=\"stat-label\">Total Test Methods</div>", "617": "            </div>", "618": "            <div class=\"stat-card danger\" onclick=\"switchTab('exact')\">", "619": "                <div class=\"stat-value\">{duplicate_count}</div>", "620": "                <div class=\"stat-label\">Duplicates</div>", "621": "            </div>", "622": "            <div class=\"stat-card info\" onclick=\"switchTab('similar')\">", "623": "                <div class=\"stat-value\">{len(similar)}</div>", "624": "                <div class=\"stat-label\">Similar Test Pairs</div>", "625": "            </div>", "626": "            <div class=\"stat-card warning\" onclick=\"switchTab('subset')\">", "627": "                <div class=\"stat-value\">{len(subset_dups)}</div>", "628": "                <div class=\"stat-label\">Subset Duplicates</div>", "629": "            </div>", "630": "        </div>", "631": "", "632": "", "633": "", "634": "        <div class=\"tabs\">", "635": "            <button class=\"tab active\" onclick=\"switchTab('exact')\">\ud83c\udfaf Exact Duplicates ({len(exact_dups)})</button>", "636": "            <button class=\"tab\" onclick=\"switchTab('similar')\">\ud83d\udd0d Similar Tests ({len(similar)})</button>", "637": "            <button class=\"tab\" onclick=\"switchTab('subset')\">\ud83d\udcca Subset Duplicates ({len(subset_dups)})</button>", "638": "        </div>", "639": "", "640": "        <div id=\"exact-content\" class=\"tab-content active\">", "641": "            <div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;\">", "642": "                <h2 style=\"margin: 0;\">\ud83c\udfaf Exact Duplicates</h2>", "643": "                <div class=\"page-size-selector\">", "644": "                    <label for=\"exact-page-size\">Items per page:</label>", "645": "                    <select id=\"exact-page-size\" onchange=\"changePageSize('exact', parseInt(this.value))\">", "646": "                        <option value=\"10\">10</option>", "647": "                        <option value=\"20\" selected>20</option>", "648": "                        <option value=\"50\">50</option>", "649": "                        <option value=\"100\">100</option>", "650": "                        <option value=\"999999\">All</option>", "651": "                    </select>", "652": "                </div>", "653": "            </div>", "654": "            <p>Tests with identical code coverage that can be safely removed.</p>", "655": "            <div class=\"pagination-controls\">", "656": "                <div id=\"exact-pagination\" class=\"pagination\"></div>", "657": "            </div>", "658": "            <div id=\"exact-table\"></div>", "659": "        </div>", "660": "", "661": "        <div id=\"similar-content\" class=\"tab-content\">", "662": "            <div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;\">", "663": "                <h2 style=\"margin: 0;\">\ud83d\udd0d Similar Tests (\u2265{threshold:.0%} overlap)</h2>", "664": "                <div class=\"page-size-selector\">", "665": "                    <label for=\"similar-page-size\">Items per page:</label>", "666": "                    <select id=\"similar-page-size\" onchange=\"changePageSize('similar', parseInt(this.value))\">", "667": "                        <option value=\"10\">10</option>", "668": "                        <option value=\"20\" selected>20</option>", "669": "                        <option value=\"50\">50</option>", "670": "                        <option value=\"100\">100</option>", "671": "                        <option value=\"999999\">All</option>", "672": "                    </select>", "673": "                </div>", "674": "            </div>", "675": "            <p>Test pairs with significant code coverage overlap that may indicate redundancy.</p>", "676": "            <div class=\"pagination-controls\">", "677": "                <div id=\"similar-pagination\" class=\"pagination\"></div>", "678": "            </div>", "679": "            <div id=\"similar-table\"></div>", "680": "        </div>", "681": "", "682": "        <div id=\"subset-content\" class=\"tab-content\">", "683": "            <div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;\">", "684": "                <h2 style=\"margin: 0;\">\ud83d\udcca Subset Duplicates</h2>", "685": "                <div class=\"page-size-selector\">", "686": "                    <label for=\"subset-page-size\">Items per page:</label>", "687": "                    <select id=\"subset-page-size\" onchange=\"changePageSize('subset', parseInt(this.value))\">", "688": "                        <option value=\"10\">10</option>", "689": "                        <option value=\"20\" selected>20</option>", "690": "                        <option value=\"50\">50</option>", "691": "                        <option value=\"100\">100</option>", "692": "                        <option value=\"999999\">All</option>", "693": "                    </select>", "694": "                </div>", "695": "            </div>", "696": "            <p>Tests that are subsets of other tests and may be redundant.</p>", "697": "            <div class=\"pagination-controls\">", "698": "                <div id=\"subset-pagination\" class=\"pagination\"></div>", "699": "            </div>", "700": "            <div id=\"subset-table\"></div>", "701": "        </div>", "702": "", "703": "        <\script>", "704": "        // Utility function for escaping HTML to prevent XSS", "705": "        function escapeHtml(text) {{", "706": "            const div = document.createElement('div');", "707": "            div.textContent = text;", "708": "            return div.innerHTML;", "709": "        }}", "710": "", "711": "        // Utility function to format test names with separators", "712": "        function formatTestName(testName) {{", "713": "            // Split test name at :: for better readability", "714": "            const parts = testName.split('::');", "715": "            if (parts.length === 1) return testName;", "716": "", "717": "            return parts.map((part, idx) => {{", "718": "                if (idx === parts.length - 1) {{", "719": "                    return '<span class=\"test-part\">' + part + '</span>';", "720": "                }}", "721": "                return '<span class=\"test-part\">' + part + '</span><span class=\"test-separator\">::</span><wbr>';", "722": "            }}).join('');", "723": "        }}", "724": "", "725": "        // Data for pagination", "726": "        // Note: Coverage data is limited to first 20 items for similar and subset", "727": "        const maxSimilar = 20;", "728": "        const maxSubset = 20;", "729": "        const exactDupsData = {json.dumps([[list(group), i-1] for i, group in enumerate(exact_dups, 1)])};", "730": "        const similarData = {json.dumps([[test1, test2, similarity, len(exact_dups) + idx] for idx, (test1, test2, similarity) in enumerate(similar[:20])])};", "731": "        const subsetData = {json.dumps([[subset_test, superset_test, ratio, len(exact_dups) + min(len(similar), 20) + i] for i, (subset_test, superset_test, ratio) in enumerate(subset_dups[:20])])};", "732": "", "733": "        // Build coverage data per file", "734": "        const coverageByFile = {{}};", "735": "        {self._build_coverage_data_js()}", "736": "", "737": "        let itemsPerPage = {{ exact: 20, similar: 20, subset: 20 }};", "738": "        let currentPages = {{ exact: 1, similar: 1, subset: 1, coverage: 1 }};", "739": "", "740": "        function changePageSize(type, newSize) {{", "741": "            itemsPerPage[type] = newSize;", "742": "            currentPages[type] = 1; // Reset to first page", "743": "", "744": "            // Re-render the appropriate section", "745": "            if (type === 'exact') {{", "746": "                renderExactDuplicates(1);", "747": "            }} else if (type === 'similar') {{", "748": "                renderSimilarTests(1);", "749": "            }} else if (type === 'subset') {{", "750": "                renderSubsetDuplicates(1);", "751": "            }}", "752": "        }}", "753": "", "754": "        function truncateTestName(testName) {{", "755": "            if (!testName || typeof testName !== 'string') {{", "756": "                return '';", "757": "            }}", "758": "", "759": "            // Extract just the meaningful parts of the test name", "760": "            const parts = testName.split('::');", "761": "            if (parts.length <= 2) {{", "762": "                return testName;", "763": "            }}", "764": "", "765": "            try {{", "766": "                // Get the file name (without path)", "767": "                const filePart = parts[0].split('/').pop() || parts[0];", "768": "                // Get the class name (if exists) and test name", "769": "                const classPart = parts.length > 2 ? parts[parts.length - 2] : '';", "770": "                const testPart = parts[parts.length - 1];", "771": "", "772": "                // Format: FileName::Class::test_name", "773": "                if (classPart) {{", "774": "                    return filePart + '::' + classPart + '::' + testPart;", "775": "                }} else {{", "776": "                    return filePart + '::' + testPart;", "777": "                }}", "778": "            }} catch (e) {{", "779": "                console.error('Error truncating test name:', e);", "780": "                return testName;", "781": "            }}", "782": "        }}", "783": "", "784": "        function switchTab(tabName) {{", "785": "            // Hide all tabs", "786": "            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));", "787": "            document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));", "788": "", "789": "            // Show selected tab", "790": "            event.target.classList.add('active');", "791": "            document.getElementById(tabName + '-content').classList.add('active');", "792": "        }}", "793": "", "794": "        function renderExactDuplicates(page) {{", "795": "            const pageSize = itemsPerPage['exact'];", "796": "            const start = (page - 1) * pageSize;", "797": "            const end = start + pageSize;", "798": "            const pageData = exactDupsData.slice(start, end);", "799": "", "800": "            let html = '';", "801": "            if (pageData.length === 0) {{", "802": "                html = '<p style=\"color: #27ae60; text-align: center; padding: 20px;\">\u2713 No exact duplicates found!</p>';", "803": "            }} else {{", "804": "                html = `", "805": "                <table>", "806": "                    <thead>", "807": "                        <tr>", "808": "                            <th>Group</th>", "809": "                            <th>Tests</th>", "810": "                            <th>Count</th>", "811": "                            <th>Action</th>", "812": "                        </tr>", "813": "                    </thead>", "814": "                    <tbody>`;", "815": "", "816": "                pageData.forEach(([group, coverageIdx], idx) => {{", "817": "                    const groupNum = start + idx + 1;", "818": "                    const testList = group.map(test => {{", "819": "                        if (!test) return '';", "820": "                        const truncated = truncateTestName(test);", "821": "                        return `<span class=\"test-name\" title=\"${{escapeHtml(test)}}\" style=\"cursor: help;\">${{escapeHtml(truncated)}}</span>`;", "822": "                    }}).filter(t => t).join('<br>');", "823": "                    html += `", "824": "                        <tr class=\"clickable-row\" onclick=\"showComparison(${{coverageIdx}})\">", "825": "                            <td><strong>Group ${{groupNum}}</strong></td>", "826": "                            <td>${{testList}}</td>", "827": "                            <td><span class=\"badge badge-danger\">${{group.length}} tests</span></td>", "828": "                            <td><span style=\"color: #00c6ff; font-weight: 600;\">\ud83d\udd0d <span class=\"view-coverage-text\">View Coverage</span></span></td>", "829": "                        </tr>`;", "830": "                }});", "831": "", "832": "                html += '</tbody></table>';", "833": "            }}", "834": "", "835": "            document.getElementById('exact-table').innerHTML = html;", "836": "            renderPagination('exact', exactDupsData.length, page, pageSize);", "837": "            formatTestNames();", "838": "        }}", "839": "", "840": "        function renderSimilarTests(page) {{", "841": "            const pageSize = itemsPerPage['similar'];", "842": "            const start = (page - 1) * pageSize;", "843": "            const end = start + pageSize;", "844": "            const pageData = similarData.slice(start, end);", "845": "", "846": "            let html = '';", "847": "            if (pageData.length === 0) {{", "848": "                html = '<p style=\"color: #27ae60; text-align: center; padding: 20px;\">\u2713 No similar tests found!</p>';", "849": "            }} else {{", "850": "                html = `", "851": "                <table>", "852": "                    <thead>", "853": "                        <tr>", "854": "                            <th>Test 1</th>", "855": "                            <th>Test 2</th>", "856": "                            <th>Similarity</th>", "857": "                            <th>Action</th>", "858": "                        </tr>", "859": "                    </thead>", "860": "                    <tbody>`;", "861": "", "862": "                pageData.forEach(([test1, test2, similarity, coverageIdx]) => {{", "863": "                    if (!test1 || !test2) return;", "864": "                    const t1 = escapeHtml(truncateTestName(test1));", "865": "                    const t2 = escapeHtml(truncateTestName(test2));", "866": "                    const simPercent = (similarity * 100).toFixed(1);", "867": "                    html += `", "868": "                        <tr class=\"clickable-row\" onclick=\"showComparison(${{coverageIdx}})\">", "869": "                            <td><span class=\"test-name\" title=\"${{escapeHtml(test1)}}\" style=\"cursor: help;\">${{t1}}</span></td>", "870": "                            <td><span class=\"test-name\" title=\"${{escapeHtml(test2)}}\" style=\"cursor: help;\">${{t2}}</span></td>", "871": "                            <td><span class=\"badge badge-info\">${{simPercent}}%</span></td>", "872": "                            <td><span style=\"color: #00c6ff; font-weight: 600;\">\ud83d\udd0d <span class=\"view-coverage-text\">View Coverage</span></span></td>", "873": "                        </tr>`;", "874": "                }});", "875": "", "876": "                html += '</tbody></table>';", "877": "            }}", "878": "", "879": "            document.getElementById('similar-table').innerHTML = html;", "880": "            renderPagination('similar', similarData.length, page, pageSize);", "881": "            formatTestNames();", "882": "        }}", "883": "", "884": "        function renderSubsetDuplicates(page) {{", "885": "            const pageSize = itemsPerPage['subset'];", "886": "            const start = (page - 1) * pageSize;", "887": "            const end = start + pageSize;", "888": "            const pageData = subsetData.slice(start, end);", "889": "", "890": "            let html = '';", "891": "            if (pageData.length === 0) {{", "892": "                html = '<p style=\"color: #27ae60; text-align: center; padding: 20px;\">\u2713 No subset duplicates found!</p>';", "893": "            }} else {{", "894": "                html = `", "895": "                <table>", "896": "                    <thead>", "897": "                        <tr>", "898": "                            <th>Subset Test</th>", "899": "                            <th>Superset Test</th>", "900": "                            <th>Coverage Ratio</th>", "901": "                            <th>Action</th>", "902": "                        </tr>", "903": "                    </thead>", "904": "                    <tbody>`;", "905": "", "906": "                pageData.forEach(([subsetTest, supersetTest, ratio, coverageIdx]) => {{", "907": "                    if (!subsetTest || !supersetTest) return;", "908": "                    const sub = escapeHtml(truncateTestName(subsetTest));", "909": "                    const sup = escapeHtml(truncateTestName(supersetTest));", "910": "                    const ratioPercent = (ratio * 100).toFixed(1);", "911": "                    html += `", "912": "                        <tr class=\"clickable-row\" onclick=\"showComparison(${{coverageIdx}})\">", "913": "                            <td><span class=\"test-name\" title=\"${{escapeHtml(subsetTest)}}\" style=\"cursor: help;\">${{sub}}</span></td>", "914": "                            <td><span class=\"test-name\" title=\"${{escapeHtml(supersetTest)}}\" style=\"cursor: help;\">${{sup}}</span></td>", "915": "                            <td><span class=\"badge badge-warning\">${{ratioPercent}}%</span></td>", "916": "                            <td><span style=\"color: #00c6ff; font-weight: 600;\">\ud83d\udd0d <span class=\"view-coverage-text\">View Coverage</span></span></td>", "917": "                        </tr>`;", "918": "                }});", "919": "", "920": "                html += '</tbody></table>';", "921": "            }}", "922": "", "923": "            document.getElementById('subset-table').innerHTML = html;", "924": "            renderPagination('subset', subsetData.length, page, pageSize);", "925": "            formatTestNames();", "926": "        }}", "927": "", "928": "        function renderPagination(type, totalItems, currentPage, pageSize) {{", "929": "            const totalPages = Math.ceil(totalItems / pageSize);", "930": "", "931": "            if (totalPages <= 1) {{", "932": "                document.getElementById(type + '-pagination').innerHTML = '';", "933": "                return;", "934": "            }}", "935": "", "936": "            const start = (currentPage - 1) * pageSize + 1;", "937": "            const end = Math.min(currentPage * pageSize, totalItems);", "938": "", "939": "            let html = '<button class=\"page-btn\" onclick=\"changePage(\\\\'' + type + '\\\\', ' + (currentPage - 1) + ')\" ' +", "940": "                (currentPage === 1 ? 'disabled' : '') + '>\u2190 Previous</button>' +", "941": "                '<span class=\"page-info\">' + start + '-' + end + ' of ' + totalItems + ' | Page ' + currentPage + '/' + totalPages + '</span>' +", "942": "                '<button class=\"page-btn\" onclick=\"changePage(\\\\'' + type + '\\\\', ' + (currentPage + 1) + ')\" ' +", "943": "                (currentPage === totalPages ? 'disabled' : '') + '>Next \u2192</button>';", "944": "", "945": "            document.getElementById(type + '-pagination').innerHTML = html;", "946": "        }}", "947": "", "948": "        function changePage(type, newPage) {{", "949": "            currentPages[type] = newPage;", "950": "", "951": "            if (type === 'exact') {{", "952": "                renderExactDuplicates(newPage);", "953": "            }} else if (type === 'similar') {{", "954": "                renderSimilarTests(newPage);", "955": "            }} else if (type === 'subset') {{", "956": "                renderSubsetDuplicates(newPage);", "957": "            }}", "958": "", "959": "            // Scroll to top of table", "960": "            document.getElementById(type + '-content').scrollIntoView({{ behavior: 'smooth', block: 'start' }});", "961": "        }}", "962": "", "963": "        function formatTestNames() {{", "964": "            const testNames = document.querySelectorAll('.test-name');", "965": "            testNames.forEach(el => {{", "966": "                const originalText = el.textContent;", "967": "                if (originalText.includes('::')) {{", "968": "                    el.innerHTML = formatTestName(originalText);", "969": "                }}", "970": "            }});", "971": "        }}", "972": "", "973": "        // Initialize on page load", "974": "        document.addEventListener('DOMContentLoaded', function() {{", "975": "            renderExactDuplicates(1);", "976": "            renderSimilarTests(1);", "977": "            renderSubsetDuplicates(1);", "978": "        }});", "979": "        <\/script>", "980": "\"\"\"", "981": "", "982": "        # Add modal and JavaScript for split-screen view", "983": "        test_coverage_map = {test.test_name: test.covered_lines for test in self.finder.tests}", "984": "", "985": "        coverage_data = []", "986": "", "987": "        # Add exact duplicates data (first in order)", "988": "        for group in exact_dups:", "989": "            if len(group) < 2:", "990": "                continue", "991": "            # Compare first test with second test in group", "992": "            test1 = group[0]", "993": "            test2 = group[1]", "994": "            test1_cov = test_coverage_map.get(test1, set())", "995": "            test2_cov = test_coverage_map.get(test2, set())", "996": "", "997": "            # Convert to dict format", "998": "            test1_dict = {}", "999": "            for filename, line in test1_cov:", "1000": "                if filename not in test1_dict:", "1001": "                    test1_dict[filename] = []", "1002": "                test1_dict[filename].append(line)", "1003": "", "1004": "            test2_dict = {}", "1005": "            for filename, line in test2_cov:", "1006": "                if filename not in test2_dict:", "1007": "                    test2_dict[filename] = []", "1008": "                test2_dict[filename].append(line)", "1009": "", "1010": "            # Sort line numbers", "1011": "            for lines in test1_dict.values():", "1012": "                lines.sort()", "1013": "            for lines in test2_dict.values():", "1014": "                lines.sort()", "1015": "", "1016": "            coverage_data.append({", "1017": "                \"subset\": test1_dict,", "1018": "                \"superset\": test2_dict,", "1019": "                \"ratio\": 1.0,", "1020": "                \"subsetName\": test1,", "1021": "                \"supersetName\": test2", "1022": "            })", "1023": "", "1024": "        # Add similar tests data (second in order)", "1025": "        for test1, test2, similarity in similar[:20]:", "1026": "            test1_cov = test_coverage_map.get(test1, set())", "1027": "            test2_cov = test_coverage_map.get(test2, set())", "1028": "", "1029": "            # Convert to dict format", "1030": "            test1_dict = {}", "1031": "            for filename, line in test1_cov:", "1032": "                if filename not in test1_dict:", "1033": "                    test1_dict[filename] = []", "1034": "                test1_dict[filename].append(line)", "1035": "", "1036": "            test2_dict = {}", "1037": "            for filename, line in test2_cov:", "1038": "                if filename not in test2_dict:", "1039": "                    test2_dict[filename] = []", "1040": "                test2_dict[filename].append(line)", "1041": "", "1042": "            # Sort line numbers", "1043": "            for lines in test1_dict.values():", "1044": "                lines.sort()", "1045": "            for lines in test2_dict.values():", "1046": "                lines.sort()", "1047": "", "1048": "            coverage_data.append({", "1049": "                \"subset\": test1_dict,", "1050": "                \"superset\": test2_dict,", "1051": "                \"ratio\": similarity,", "1052": "                \"subsetName\": test1,", "1053": "                \"supersetName\": test2", "1054": "            })", "1055": "", "1056": "        # Add subset duplicates data (third in order)", "1057": "        for subset_test, superset_test, ratio in subset_dups[:20]:", "1058": "            subset_cov = test_coverage_map.get(subset_test, set())", "1059": "            superset_cov = test_coverage_map.get(superset_test, set())", "1060": "", "1061": "            # Convert to dict format", "1062": "            subset_dict = {}", "1063": "            for filename, line in subset_cov:", "1064": "                if filename not in subset_dict:", "1065": "                    subset_dict[filename] = []", "1066": "                subset_dict[filename].append(line)", "1067": "", "1068": "            superset_dict = {}", "1069": "            for filename, line in superset_cov:", "1070": "                if filename not in superset_dict:", "1071": "                    superset_dict[filename] = []", "1072": "                superset_dict[filename].append(line)", "1073": "", "1074": "            # Sort line numbers", "1075": "            for lines in subset_dict.values():", "1076": "                lines.sort()", "1077": "            for lines in superset_dict.values():", "1078": "                lines.sort()", "1079": "", "1080": "            coverage_data.append({", "1081": "                \"subset\": subset_dict,", "1082": "                \"superset\": superset_dict,", "1083": "                \"ratio\": ratio,", "1084": "                \"subsetName\": subset_test,", "1085": "                \"supersetName\": superset_test", "1086": "            })", "1087": "", "1088": "        # Serialize JSON data before embedding", "1089": "        coverage_data_json = json.dumps(coverage_data, ensure_ascii=True)", "1090": "        source_code_map_json = json.dumps(source_code_map, ensure_ascii=True)", "1091": "", "1092": "        # Escape HTML-breaking tags in JSON strings", "1093": "        # Even though it's in JSON, the browser's HTML parser will see <\/script> and break", "1094": "        coverage_data_json = coverage_data_json.replace('<\/script>', '<\\\\/script>').replace('<\script', '<\\\\script')", "1095": "        source_code_map_json = source_code_map_json.replace('<\/script>', '<\\\\/script>').replace('<\script', '<\\\\script')", "1096": "", "1097": "        html += \"\"\"", "1098": "        <!-- Modal for split-screen coverage view -->", "1099": "        <div id=\"comparisonModal\" class=\"modal\">", "1100": "            <div class=\"modal-content\">", "1101": "                <div class=\"modal-header\">", "1102": "                    <h2 style=\"margin: 0;\">\ud83d\udcca Coverage Comparison: Execution Paths</h2>", "1103": "                    <div style=\"display: flex; align-items: center; gap: 15px;\">", "1104": "                        <div class=\"filter-section\">", "1105": "                            <label for=\"fileFilter\" style=\"font-weight: 600; margin-right: 8px;\">\ud83d\udcc1</label>", "1106": "                            <select id=\"fileFilter\" class=\"filter-select\" onchange=\"applyFileFilter()\">", "1107": "                                <option value=\"\">All Files</option>", "1108": "                            </select>", "1109": "                        </div>", "1110": "                        <div class=\"sync-toggle\" id=\"syncToggle\" onclick=\"toggleSync()\">", "1111": "                            <input type=\"checkbox\" id=\"syncCheckbox\" class=\"sync-checkbox\" checked>", "1112": "                            <label for=\"syncCheckbox\" style=\"cursor: pointer; user-select: none;\">\ud83d\udd17 Sync Scroll</label>", "1113": "                        </div>", "1114": "                        <span class=\"close\" onclick=\"closeModal()\">&times;</span>", "1115": "                    </div>", "1116": "                </div>", "1117": "                <div style=\"background: #e3f2fd; border-left: 4px solid #2196F3; padding: 12px 16px; margin: 0 20px 16px; border-radius: 4px; font-size: 14px;\">", "1118": "                    <strong>\u2139\ufe0f Note:</strong> Source code is identical. Highlighting shows <strong>which lines each test executed</strong>.", "1119": "                    Different execution paths are normal due to conditional branches (if/elif/else), early returns, and functions called with different parameters.", "1120": "                </div>", "1121": "                <div class=\"coverage-info\">", "1122": "                    <div>", "1123": "                        <strong>Subset Test:</strong> <span id=\"subsetName\" class=\"test-name\"></span>", "1124": "                        &nbsp;&nbsp;|&nbsp;&nbsp;", "1125": "                        <strong>Superset Test:</strong> <span id=\"supersetName\" class=\"test-name\"></span>", "1126": "                        &nbsp;&nbsp;|&nbsp;&nbsp;", "1127": "                        <strong>Coverage Ratio:</strong> <span id=\"coverageRatio\" class=\"badge badge-warning\"></span>", "1128": "                    </div>", "1129": "                    <div style=\"margin-top: 10px; padding: 8px; background: #f5f5f5; border-radius: 4px; display: inline-flex; gap: 20px; font-size: 13px;\">", "1130": "                        <span><span style=\"display: inline-block; width: 16px; height: 16px; background: #c8e6c9; border-radius: 3px; vertical-align: middle;\"></span> Both tests executed</span>", "1131": "                        <span><span style=\"display: inline-block; width: 16px; height: 16px; background: #fff9c4; border-radius: 3px; vertical-align: middle;\"></span> Only one test executed</span>", "1132": "                        <span><span style=\"display: inline-block; width: 16px; height: 16px; background: #ffffff; border: 1px solid #ddd; border-radius: 3px; vertical-align: middle;\"></span> Neither test executed</span>", "1133": "                    </div>", "1134": "                </div>", "1135": "                <div class=\"split-view\">", "1136": "                    <div class=\"file-panel\">", "1137": "                        <div class=\"panel-header\">\ud83d\udcc4 Subset Test Coverage</div>", "1138": "                        <div id=\"subsetContent\" class=\"file-content\"></div>", "1139": "                    </div>", "1140": "                    <div class=\"file-panel\">", "1141": "                        <div class=\"panel-header\">\ud83d\udcc4 Superset Test Coverage</div>", "1142": "                        <div id=\"supersetContent\" class=\"file-content\"></div>", "1143": "                    </div>", "1144": "                </div>", "1145": "            </div>", "1146": "        </div>", "1147": "", "1148": "        <\script>", "1149": "        const coverageData = \"\"\" + coverage_data_json + \"\"\";\\n        const sourceCode = \"\"\" + source_code_map_json + \"\"\";\\n        let currentData = null;", "1150": "        let syncEnabled = true;", "1151": "        let isScrolling = false;", "1152": "", "1153": "        function showComparison(index) {{", "1154": "            const data = coverageData[index];", "1155": "            if (!data) return;", "1156": "", "1157": "            currentData = data;", "1158": "", "1159": "            document.getElementById('subsetName').innerHTML = formatTestName(data.subsetName);", "1160": "            document.getElementById('supersetName').innerHTML = formatTestName(data.supersetName);", "1161": "            document.getElementById('coverageRatio').textContent = (data.ratio * 100).toFixed(1) + '%';", "1162": "", "1163": "            // Populate file filter", "1164": "            const allFiles = new Set([...Object.keys(data.subset), ...Object.keys(data.superset)]);", "1165": "            const fileFilter = document.getElementById('fileFilter');", "1166": "            fileFilter.innerHTML = '<option value=\"\">All Files</option>';", "1167": "            Array.from(allFiles).sort().forEach(file => {{", "1168": "                const option = document.createElement('option');", "1169": "                option.value = file;", "1170": "                option.textContent = file;", "1171": "                fileFilter.appendChild(option);", "1172": "            }});", "1173": "", "1174": "            renderBothPanels();", "1175": "", "1176": "            document.getElementById('comparisonModal').style.display = 'block';", "1177": "", "1178": "            // Scroll to top of the modal", "1179": "            const splitView = document.querySelector('.split-view');", "1180": "            if (splitView) {{", "1181": "                splitView.scrollTop = 0;", "1182": "            }}", "1183": "        }}", "1184": "", "1185": "        function renderBothPanels() {{", "1186": "            const selectedFile = document.getElementById('fileFilter').value;", "1187": "", "1188": "            // Get all unique files from both sides", "1189": "            const subsetFiles = Object.keys(currentData.subset).sort();", "1190": "            const supersetFiles = Object.keys(currentData.superset).sort();", "1191": "            const allFiles = [...new Set([...subsetFiles, ...supersetFiles])].sort();", "1192": "", "1193": "            // Apply file filter", "1194": "            const filesToRender = selectedFile ? [selectedFile] : allFiles;", "1195": "", "1196": "            let subsetHtml = '';", "1197": "            let supersetHtml = '';", "1198": "", "1199": "            for (const file of filesToRender) {{", "1200": "                const subsetLines = currentData.subset[file] || [];", "1201": "                const supersetLines = currentData.superset[file] || [];", "1202": "", "1203": "                // Get all unique line numbers from both sides", "1204": "                let allLineNums = [...new Set([...subsetLines, ...supersetLines])].sort((a, b) => a - b);", "1205": "", "1206": "                if (allLineNums.length === 0) continue;", "1207": "", "1208": "                const subsetLineSet = new Set(subsetLines);", "1209": "                const supersetLineSet = new Set(supersetLines);", "1210": "                const fileSource = sourceCode[file] || {};", "1211": "", "1212": "                // Find and include method/class definitions for context", "1213": "                const minLine = Math.min(...allLineNums);", "1214": "                const maxLine = Math.max(...allLineNums);", "1215": "                const contextLines = new Set(allLineNums);", "1216": "", "1217": "                // Scan backwards from each covered line to find def/class", "1218": "                for (const lineNum of allLineNums) {{", "1219": "                    for (let i = lineNum - 1; i >= Math.max(1, minLine - 20); i--) {{", "1220": "                        const line = fileSource[i] || '';", "1221": "                        const trimmed = line.trim();", "1222": "                        if (trimmed.startsWith('def ') || trimmed.startsWith('class ') || trimmed.startsWith('async def ')) {{", "1223": "                            contextLines.add(i);", "1224": "                            break;", "1225": "                        }}", "1226": "                        // Stop if we hit another definition or empty line followed by def", "1227": "                        if (trimmed === '' && i < lineNum - 5) break;", "1228": "                    }}", "1229": "                }}", "1230": "", "1231": "                // Convert back to sorted array", "1232": "                allLineNums = Array.from(contextLines).sort((a, b) => a - b);", "1233": "", "1234": "                // Add file headers", "1235": "                subsetHtml += '<div class=\\\"file-section\\\" style=\\\"margin-bottom: 30px;\\\">';", "1236": "                subsetHtml += '<div class=\\\"file-path\\\">\ud83d\udcc4 ' + escapeHtml(file) + '</div>';", "1237": "", "1238": "                supersetHtml += '<div class=\\\"file-section\\\" style=\\\"margin-bottom: 30px;\\\">';", "1239": "                supersetHtml += '<div class=\\\"file-path\\\">\ud83d\udcc4 ' + escapeHtml(file) + '</div>';", "1240": "", "1241": "                // Render each line with gap detection", "1242": "                let prevLineNum = null;", "1243": "                let inDocstring = false;", "1244": "                let docstringDelimiter = '';", "1245": "", "1246": "                for (let idx = 0; idx < allLineNums.length; idx++) {{", "1247": "                    const lineNum = allLineNums[idx];", "1248": "                    const sourceLine = fileSource[lineNum] || '';", "1249": "                    const trimmed = sourceLine.trim();", "1250": "", "1251": "                    // Track docstring state", "1252": "                    if (trimmed.startsWith('\\\"\\\"\\\"') || trimmed.startsWith(\"'''\")) {{", "1253": "                        const delimiter = trimmed.startsWith('\\\"\\\"\\\"') ? '\\\"\\\"\\\"' : \"'''\";", "1254": "                        if (!inDocstring) {{", "1255": "                            // Starting a docstring", "1256": "                            inDocstring = true;", "1257": "                            docstringDelimiter = delimiter;", "1258": "                            // Check if it's a single-line docstring", "1259": "                            const afterDelimiter = trimmed.substring(3);", "1260": "                            if (afterDelimiter.includes(delimiter)) {{", "1261": "                                inDocstring = false; // Single-line docstring", "1262": "                            }}", "1263": "                            continue; // Skip docstring opening line", "1264": "                        }} else if (inDocstring && delimiter === docstringDelimiter) {{", "1265": "                            // Ending a docstring", "1266": "                            inDocstring = false;", "1267": "                            docstringDelimiter = '';", "1268": "                            continue; // Skip docstring closing line", "1269": "                        }}", "1270": "                    }} else if (inDocstring) {{", "1271": "                        // Skip content inside docstring", "1272": "                        continue;", "1273": "                    }}", "1274": "", "1275": "                    // Handle gap between lines", "1276": "                    if (prevLineNum !== null && lineNum - prevLineNum > 1) {{", "1277": "                        const gap = lineNum - prevLineNum - 1;", "1278": "                        const gapStart = prevLineNum + 1;", "1279": "                        const gapEnd = lineNum - 1;", "1280": "", "1281": "                        if (gap > 3) {{", "1282": "                            // Show collapsible gap for >3 lines", "1283": "                            const gapId = 'gap_' + file.replace(/[^a-zA-Z0-9]/g, '_') + '_' + gapStart + '_' + gapEnd;", "1284": "                            const gapText = '... (' + gap + ' line' + (gap > 1 ? 's' : '') + ')';", "1285": "", "1286": "                            subsetHtml += '<div class=\\\"code-line gap-line\\\" style=\\\"color: #000000; text-align: center; font-style: normal; background: transparent; cursor: pointer; padding: 2px 8px;\\\" ';", "1287": "                            subsetHtml += 'data-gap-id=\\\"' + gapId + '\\\" data-gap-start=\\\"' + gapStart + '\\\" data-gap-end=\\\"' + gapEnd + '\\\" data-file=\\\"' + escapeHtml(file) + '\\\" ';", "1288": "                            subsetHtml += 'onclick=\\\"toggleGap(this, \\\\'subset\\\\')\\\" title=\\\"Click to expand\\\">';", "1289": "                            subsetHtml += '<strong>' + gapText + '</strong>';", "1290": "                            subsetHtml += '</div>';", "1291": "", "1292": "                            supersetHtml += '<div class=\\\"code-line gap-line\\\" style=\\\"color: #000000; text-align: center; font-style: normal; background: transparent; cursor: pointer; padding: 2px 8px;\\\" ';", "1293": "                            supersetHtml += 'data-gap-id=\\\"' + gapId + '\\\" data-gap-start=\\\"' + gapStart + '\\\" data-gap-end=\\\"' + gapEnd + '\\\" data-file=\\\"' + escapeHtml(file) + '\\\" ';", "1294": "                            supersetHtml += 'onclick=\\\"toggleGap(this, \\\\'superset\\\\')\\\" title=\\\"Click to expand\\\">';", "1295": "                            supersetHtml += '<strong>' + gapText + '</strong>';", "1296": "                            supersetHtml += '</div>';", "1297": "                        }} else {{", "1298": "                            // Show lines if gap is 3 or less", "1299": "                            for (let gapLine = gapStart; gapLine <= gapEnd; gapLine++) {{", "1300": "                                const gapSource = fileSource[gapLine] || '';", "1301": "                                const gapLineNumStr = String(gapLine).padStart(4, ' ');", "1302": "", "1303": "                                subsetHtml += '<div class=\\\"code-line\\\" style=\\\"opacity: 0.4; background: #fafafa;\\\">';", "1304": "                                subsetHtml += '<span style=\\\"color: #bbb; margin-right: 10px;\\\">' + gapLineNumStr + '</span>';", "1305": "                                subsetHtml += '<span style=\\\"color: #aaa;\\\">' + escapeHtml(gapSource) + '</span>';", "1306": "                                subsetHtml += '</div>';", "1307": "", "1308": "                                supersetHtml += '<div class=\\\"code-line\\\" style=\\\"opacity: 0.4; background: #fafafa;\\\">';", "1309": "                                supersetHtml += '<span style=\\\"color: #bbb; margin-right: 10px;\\\">' + gapLineNumStr + '</span>';", "1310": "                                supersetHtml += '<span style=\\\"color: #aaa;\\\">' + escapeHtml(gapSource) + '</span>';", "1311": "                                supersetHtml += '</div>';", "1312": "                            }}", "1313": "                        }}", "1314": "                    }}", "1315": "", "1316": "                    prevLineNum = lineNum;", "1317": "                    const lineNumStr = String(lineNum).padStart(4, ' ');", "1318": "                    const isDefLine = trimmed.startsWith('def ') || trimmed.startsWith('class ') || trimmed.startsWith('async def ');", "1319": "", "1320": "                    // Determine coverage status", "1321": "                    const inSubset = subsetLineSet.has(lineNum);", "1322": "                    const inSuperset = supersetLineSet.has(lineNum);", "1323": "                    const inBoth = inSubset && inSuperset;", "1324": "", "1325": "                    // Render left side (subset)", "1326": "                    if (inBoth) {{", "1327": "                        // Both tests executed this line - GREEN", "1328": "                        subsetHtml += '<div class=\\\"code-line covered-both\\\">';", "1329": "                        subsetHtml += '<span style=\\\"color: #999; margin-right: 10px;\\\">' + lineNumStr + '</span>';", "1330": "                        subsetHtml += escapeHtml(sourceLine) || '&nbsp;';", "1331": "                        subsetHtml += '</div>';", "1332": "                    }} else if (inSubset) {{", "1333": "                        // Only subset executed this line - YELLOW", "1334": "                        subsetHtml += '<div class=\\\"code-line covered-single\\\">';", "1335": "                        subsetHtml += '<span style=\\\"color: #999; margin-right: 10px;\\\">' + lineNumStr + '</span>';", "1336": "                        subsetHtml += escapeHtml(sourceLine) || '&nbsp;';", "1337": "                        subsetHtml += '</div>';", "1338": "                    }} else if (isDefLine) {{", "1339": "                        // Show def/class lines as context", "1340": "                        subsetHtml += '<div class=\\\"code-line\\\" style=\\\"background: #e8f4f8; font-weight: 600;\\\">';", "1341": "                        subsetHtml += '<span style=\\\"color: #999; margin-right: 10px;\\\">' + lineNumStr + '</span>';", "1342": "                        subsetHtml += escapeHtml(sourceLine);", "1343": "                        subsetHtml += '</div>';", "1344": "                    }} else {{", "1345": "                        // Show actual code dimmed for non-executed lines", "1346": "                        subsetHtml += '<div class=\\\"code-line\\\" style=\\\"opacity: 0.4; background: #fafafa;\\\">';", "1347": "                        subsetHtml += '<span style=\\\"color: #bbb; margin-right: 10px;\\\">' + lineNumStr + '</span>';", "1348": "                        subsetHtml += '<span style=\\\"color: #999;\\\">' + escapeHtml(sourceLine) + '</span>';", "1349": "                        subsetHtml += '</div>';", "1350": "                    }}", "1351": "", "1352": "                    // Render right side (superset)", "1353": "                    if (inBoth) {{", "1354": "                        // Both tests executed this line - GREEN", "1355": "                        supersetHtml += '<div class=\\\"code-line covered-both\\\">';", "1356": "                        supersetHtml += '<span style=\\\"color: #999; margin-right: 10px;\\\">' + lineNumStr + '</span>';", "1357": "                        supersetHtml += escapeHtml(sourceLine) || '&nbsp;';", "1358": "                        supersetHtml += '</div>';", "1359": "                    }} else if (inSuperset) {{", "1360": "                        // Only superset executed this line - YELLOW", "1361": "                        supersetHtml += '<div class=\\\"code-line covered-single\\\">';", "1362": "                        supersetHtml += '<span style=\\\"color: #999; margin-right: 10px;\\\">' + lineNumStr + '</span>';", "1363": "                        supersetHtml += escapeHtml(sourceLine) || '&nbsp;';", "1364": "                        supersetHtml += '</div>';", "1365": "                    }} else if (isDefLine) {{", "1366": "                        // Show def/class lines as context", "1367": "                        supersetHtml += '<div class=\\\"code-line\\\" style=\\\"background: #e8f4f8; font-weight: 600;\\\">';", "1368": "                        supersetHtml += '<span style=\\\"color: #999; margin-right: 10px;\\\">' + lineNumStr + '</span>';", "1369": "                        supersetHtml += escapeHtml(sourceLine);", "1370": "                        supersetHtml += '</div>';", "1371": "                    }} else {{", "1372": "                        // Show actual code dimmed for non-executed lines", "1373": "                        supersetHtml += '<div class=\\\"code-line\\\" style=\\\"opacity: 0.4; background: #fafafa;\\\">';", "1374": "                        supersetHtml += '<span style=\\\"color: #bbb; margin-right: 10px;\\\">' + lineNumStr + '</span>';", "1375": "                        supersetHtml += '<span style=\\\"color: #999;\\\">' + escapeHtml(sourceLine) + '</span>';", "1376": "                        supersetHtml += '</div>';", "1377": "                    }}", "1378": "                }}", "1379": "", "1380": "                subsetHtml += '</div>';", "1381": "                supersetHtml += '</div>';", "1382": "            }}", "1383": "", "1384": "            document.getElementById('subsetContent').innerHTML = subsetHtml || '<p style=\\\"padding: 20px; color: #7f8c8d;\\\">No coverage data</p>';", "1385": "            document.getElementById('supersetContent').innerHTML = supersetHtml || '<p style=\\\"padding: 20px; color: #7f8c8d;\\\">No coverage data</p>';", "1386": "        }}", "1387": "", "1388": "        function applyFileFilter() {{", "1389": "            renderBothPanels();", "1390": "        }}", "1391": "", "1392": "        function toggleSync() {{", "1393": "            syncEnabled = !syncEnabled;", "1394": "            const checkbox = document.getElementById('syncCheckbox');", "1395": "            const toggle = document.getElementById('syncToggle');", "1396": "            const splitView = document.querySelector('.split-view');", "1397": "            const filePanels = document.querySelectorAll('.file-panel');", "1398": "", "1399": "            checkbox.checked = syncEnabled;", "1400": "            if (syncEnabled) {{", "1401": "                toggle.classList.add('active');", "1402": "                // Use single scroll - both panels scroll together", "1403": "                splitView.classList.remove('independent');", "1404": "                filePanels.forEach(panel => panel.classList.remove('independent'));", "1405": "            }} else {{", "1406": "                toggle.classList.remove('active');", "1407": "                // Enable independent scrolling for each panel", "1408": "                splitView.classList.add('independent');", "1409": "                filePanels.forEach(panel => panel.classList.add('independent'));", "1410": "            }}", "1411": "        }}", "1412": "", "1413": "        function toggleGap(element, side) {{", "1414": "            const gapId = element.getAttribute('data-gap-id');", "1415": "            const gapStart = parseInt(element.getAttribute('data-gap-start'));", "1416": "            const gapEnd = parseInt(element.getAttribute('data-gap-end'));", "1417": "            const file = element.getAttribute('data-file');", "1418": "", "1419": "            // Find the corresponding gap in the other panel", "1420": "            const otherSide = side === 'subset' ? 'superset' : 'subset';", "1421": "            const otherContent = document.getElementById(otherSide + 'Content');", "1422": "            const otherGap = otherContent.querySelector('.gap-line[data-gap-id=\"' + gapId + '\"]');", "1423": "", "1424": "            // Check if already expanded", "1425": "            const isExpanded = element.classList.contains('expanded');", "1426": "", "1427": "            if (isExpanded) {{", "1428": "                // Collapse both sides - show gap element again", "1429": "                const expandedLines = element.parentElement.querySelectorAll('.expanded-line[data-gap-id=\"' + gapId + '\"]');", "1430": "                expandedLines.forEach(line => line.remove());", "1431": "                element.classList.remove('expanded');", "1432": "                element.style.display = 'block';", "1433": "                const gap = gapEnd - gapStart + 1;", "1434": "                element.innerHTML = '<strong>... (' + gap + ' line' + (gap > 1 ? 's' : '') + ')</strong>';", "1435": "", "1436": "                // Collapse other side", "1437": "                if (otherGap) {{", "1438": "                    const otherExpandedLines = otherGap.parentElement.querySelectorAll('.expanded-line[data-gap-id=\"' + gapId + '\"]');", "1439": "                    otherExpandedLines.forEach(line => line.remove());", "1440": "                    otherGap.classList.remove('expanded');", "1441": "                    otherGap.style.display = 'block';", "1442": "                    otherGap.innerHTML = '<strong>... (' + gap + ' line' + (gap > 1 ? 's' : '') + ')</strong>';", "1443": "                }}", "1444": "            }} else {{", "1445": "                // Expand both sides - hide gap element completely", "1446": "                element.classList.add('expanded');", "1447": "                element.style.display = 'none';", "1448": "", "1449": "                const fileSource = sourceCode[file] || {};", "1450": "                const subsetLineSet = new Set(currentData.subset[file] || []);", "1451": "                const supersetLineSet = new Set(currentData.superset[file] || []);", "1452": "", "1453": "                let subsetInsertHtml = '';", "1454": "                let supersetInsertHtml = '';", "1455": "", "1456": "                // Build in ASCENDING order from gapStart to gapEnd", "1457": "                // Show ALL lines when gap is expanded (including docstrings, comments, etc.)", "1458": "                for (let lineNum = gapStart; lineNum <= gapEnd; lineNum++) {{", "1459": "                    const sourceLine = fileSource[lineNum] || '';", "1460": "                    const lineNumStr = String(lineNum).padStart(4, ' ');", "1461": "", "1462": "                    const inSubset = subsetLineSet.has(lineNum);", "1463": "                    const inSuperset = supersetLineSet.has(lineNum);", "1464": "                    const inBoth = inSubset && inSuperset;", "1465": "", "1466": "                    // Make all expanded lines clickable to collapse", "1467": "                    const clickHandler = 'onclick=\"toggleGap(document.querySelector(\\\\'.gap-line[data-gap-id=\\\\\\\\\\\\'' + gapId + '\\\\\\\\\\\\']\\\\'), \\\\'subset\\\\')\" style=\"cursor: pointer;\" title=\"Click to collapse\"';", "1468": "", "1469": "                    // Always show actual source code, with color coding for execution status", "1470": "                    // Build subset side HTML", "1471": "                    if (inBoth) {{", "1472": "                        subsetInsertHtml += '<div class=\"code-line expanded-line covered-both\" data-gap-id=\"' + gapId + '\" ' + clickHandler + '>';", "1473": "                        subsetInsertHtml += '<span style=\"color: #999; margin-right: 10px;\">' + lineNumStr + '</span>';", "1474": "                        subsetInsertHtml += escapeHtml(sourceLine) || '&nbsp;';", "1475": "                        subsetInsertHtml += '</div>';", "1476": "                    }} else if (inSubset) {{", "1477": "                        subsetInsertHtml += '<div class=\"code-line expanded-line covered-single\" data-gap-id=\"' + gapId + '\" ' + clickHandler + '>';", "1478": "                        subsetInsertHtml += '<span style=\"color: #999; margin-right: 10px;\">' + lineNumStr + '</span>';", "1479": "                        subsetInsertHtml += escapeHtml(sourceLine) || '&nbsp;';", "1480": "                        subsetInsertHtml += '</div>';", "1481": "                    }} else {{", "1482": "                        // Show actual code even when not executed, just dimmed", "1483": "                        subsetInsertHtml += '<div class=\"code-line expanded-line\" data-gap-id=\"' + gapId + '\" style=\"opacity: 0.4; background: #fafafa; cursor: pointer;\" onclick=\"toggleGap(document.querySelector(\\\\'.gap-line[data-gap-id=\\\\\\\\\\\\'' + gapId + '\\\\\\\\\\\\']\\\\'), \\\\'subset\\\\')\" title=\"Click to collapse\">';", "1484": "                        subsetInsertHtml += '<span style=\"color: #bbb; margin-right: 10px;\">' + lineNumStr + '</span>';", "1485": "                        subsetInsertHtml += '<span style=\"color: #999;\">' + escapeHtml(sourceLine) + '</span>';", "1486": "                        subsetInsertHtml += '</div>';", "1487": "                    }}", "1488": "", "1489": "                    // Build superset side HTML", "1490": "                    if (inBoth) {{", "1491": "                        supersetInsertHtml += '<div class=\"code-line expanded-line covered-both\" data-gap-id=\"' + gapId + '\" ' + clickHandler + '>';", "1492": "                        supersetInsertHtml += '<span style=\"color: #999; margin-right: 10px;\">' + lineNumStr + '</span>';", "1493": "                        supersetInsertHtml += escapeHtml(sourceLine) || '&nbsp;';", "1494": "                        supersetInsertHtml += '</div>';", "1495": "                    }} else if (inSuperset) {{", "1496": "                        supersetInsertHtml += '<div class=\"code-line expanded-line covered-single\" data-gap-id=\"' + gapId + '\" ' + clickHandler + '>';", "1497": "                        supersetInsertHtml += '<span style=\"color: #999; margin-right: 10px;\">' + lineNumStr + '</span>';", "1498": "                        supersetInsertHtml += escapeHtml(sourceLine) || '&nbsp;';", "1499": "                        supersetInsertHtml += '</div>';", "1500": "                    }} else {{", "1501": "                        // Show actual code even when not executed, just dimmed", "1502": "                        supersetInsertHtml += '<div class=\"code-line expanded-line\" data-gap-id=\"' + gapId + '\" style=\"opacity: 0.4; background: #fafafa; cursor: pointer;\" onclick=\"toggleGap(document.querySelector(\\\\'.gap-line[data-gap-id=\\\\\\\\\\\\'' + gapId + '\\\\\\\\\\\\']\\\\'), \\\\'superset\\\\')\" title=\"Click to collapse\">';", "1503": "                        supersetInsertHtml += '<span style=\"color: #bbb; margin-right: 10px;\">' + lineNumStr + '</span>';", "1504": "                        supersetInsertHtml += '<span style=\"color: #999;\">' + escapeHtml(sourceLine) + '</span>';", "1505": "                        supersetInsertHtml += '</div>';", "1506": "                    }}", "1507": "                }}", "1508": "", "1509": "                // Insert in correct order: create elements array first, then insert in REVERSE", "1510": "                if (side === 'subset') {{", "1511": "                    const tempDiv = document.createElement('div');", "1512": "                    tempDiv.innerHTML = subsetInsertHtml;", "1513": "                    const elementsArray = Array.from(tempDiv.children);", "1514": "                    // Insert in REVERSE order to maintain ascending line numbers", "1515": "                    for (let i = elementsArray.length - 1; i >= 0; i--) {{", "1516": "                        element.parentNode.insertBefore(elementsArray[i], element.nextSibling);", "1517": "                    }}", "1518": "                }} else {{", "1519": "                    const tempDiv = document.createElement('div');", "1520": "                    tempDiv.innerHTML = supersetInsertHtml;", "1521": "                    const elementsArray = Array.from(tempDiv.children);", "1522": "                    // Insert in REVERSE order to maintain ascending line numbers", "1523": "                    for (let i = elementsArray.length - 1; i >= 0; i--) {{", "1524": "                        element.parentNode.insertBefore(elementsArray[i], element.nextSibling);", "1525": "                    }}", "1526": "                }}", "1527": "", "1528": "                // Expand other side - hide it too", "1529": "                if (otherGap) {{", "1530": "                    otherGap.classList.add('expanded');", "1531": "                    otherGap.style.display = 'none';", "1532": "", "1533": "                    const tempDiv2 = document.createElement('div');", "1534": "                    if (otherSide === 'subset') {{", "1535": "                        tempDiv2.innerHTML = subsetInsertHtml;", "1536": "                    }} else {{", "1537": "                        tempDiv2.innerHTML = supersetInsertHtml;", "1538": "                    }}", "1539": "                    const elementsArray2 = Array.from(tempDiv2.children);", "1540": "                    // Insert in REVERSE order to maintain ascending line numbers", "1541": "                    for (let i = elementsArray2.length - 1; i >= 0; i--) {{", "1542": "                        otherGap.parentNode.insertBefore(elementsArray2[i], otherGap.nextSibling);", "1543": "                    }}", "1544": "                }}", "1545": "            }}", "1546": "        }}", "1547": "", "1548": "        function closeModal() {{", "1549": "            document.getElementById('comparisonModal').style.display = 'none';", "1550": "            currentData = null;", "1551": "        }}", "1552": "", "1553": "        window.onclick = function(event) {{", "1554": "            const modal = document.getElementById('comparisonModal');", "1555": "            if (event.target == modal) {{", "1556": "                closeModal();", "1557": "            }}", "1558": "        }}", "1559": "", "1560": "        document.addEventListener('keydown', function(event) {{", "1561": "            if (event.key === 'Escape') {{", "1562": "                closeModal();", "1563": "            }}", "1564": "        }});", "1565": "", "1566": "        // Format all test names on page load", "1567": "        document.addEventListener('DOMContentLoaded', function() {{", "1568": "            const testNames = document.querySelectorAll('.test-name');", "1569": "            testNames.forEach(el => {{", "1570": "                const originalText = el.textContent;", "1571": "                if (originalText.includes('::')) {{", "1572": "                    el.innerHTML = formatTestName(originalText);", "1573": "                }}", "1574": "            }});", "1575": "", "1576": "            // Initial render after DOM is loaded and all functions are defined", "1577": "            renderExactDuplicates(1);", "1578": "            renderSimilarTests(1);", "1579": "            renderSubsetDuplicates(1);", "1580": "        }});", "1581": "        <\/script>", "1582": "", "1583": "        <div class=\"footer\">", "1584": "            <p>Generated by <strong>TestIQ v{version}</strong> on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>", "1585": "            <p>Analysis threshold: {threshold:.1%} | \ud83d\udd17 <a href=\"https://github.com/pydevtools/TestIQ\" style=\"color: #00c6ff;\">github.com/pydevtools/TestIQ</a></p>", "1586": "        </div>", "1587": "    </div>", "1588": "</body>", "1589": "</html>", "1590": "\"\"\"", "1591": "        return html", "1592": "", "1593": "", "1594": "class CSVReportGenerator:", "1595": "    \"\"\"Generate CSV reports for data analysis and spreadsheets.\"\"\"", "1596": "", "1597": "    def __init__(self, finder: CoverageDuplicateFinder) -> None:", "1598": "        \"\"\"Initialize CSV report generator.\"\"\"", "1599": "        self.finder = finder", "1600": "", "1601": "    def generate_exact_duplicates(self, output_path: Path) -> None:", "1602": "        \"\"\"Generate CSV of exact duplicates.\"\"\"", "1603": "        logger.info(f\"Generating exact duplicates CSV: {output_path}\")", "1604": "", "1605": "        exact_dups = self.finder.find_exact_duplicates()", "1606": "        duplicate_count = self.finder.get_duplicate_count()", "1607": "", "1608": "        logger.info(f\"  Found {len(exact_dups)} groups with {duplicate_count} duplicates\")", "1609": "", "1610": "        with open(output_path, \"w\", newline=\"\") as f:", "1611": "            writer = csv.writer(f)", "1612": "            writer.writerow([\"Group\", \"Test Name\", \"Action\"])", "1613": "", "1614": "            for i, group in enumerate(exact_dups, 1):", "1615": "                for j, test in enumerate(group):", "1616": "                    action = \"Keep\" if j == 0 else \"Remove\"", "1617": "                    writer.writerow([f\"Group {i}\", test, action])", "1618": "", "1619": "        logger.info(f\"CSV report saved: {output_path}\")", "1620": "", "1621": "    def generate_subset_duplicates(self, output_path: Path) -> None:", "1622": "        \"\"\"Generate CSV of subset duplicates (sorted by coverage ratio).\"\"\"", "1623": "        logger.info(f\"Generating subset duplicates CSV: {output_path}\")", "1624": "", "1625": "        subsets = self.finder.get_sorted_subset_duplicates()  # Use sorted version", "1626": "", "1627": "        logger.info(f\"  Found {len(subsets)} subset duplicates\")", "1628": "", "1629": "        with open(output_path, \"w\", newline=\"\") as f:", "1630": "            writer = csv.writer(f)", "1631": "            writer.writerow([\"Subset Test\", \"Superset Test\", \"Coverage Ratio\", \"Action\"])", "1632": "", "1633": "            for subset_test, superset_test, ratio in subsets:", "1634": "                writer.writerow(", "1635": "                    [", "1636": "                        subset_test,", "1637": "                        superset_test,", "1638": "                        f\"{ratio:.1%}\",  # Consistent 1 decimal place", "1639": "                        \"Review for removal\",", "1640": "                    ]", "1641": "                )", "1642": "", "1643": "        logger.info(f\"CSV report saved: {output_path}\")", "1644": "", "1645": "    def generate_similar_tests(self, output_path: Path, threshold: float = 0.3) -> None:", "1646": "        \"\"\"", "1647": "        Generate CSV of similar tests.", "1648": "", "1649": "        Args:", "1650": "            output_path: Path to save CSV", "1651": "            threshold: Similarity threshold (default: 0.3 = 30%)", "1652": "        \"\"\"", "1653": "        logger.info(f\"Generating similar tests CSV: {output_path}\")", "1654": "        logger.info(f\"  Threshold: {threshold:.1%}\")", "1655": "", "1656": "        similar = self.finder.find_similar_coverage(threshold)", "1657": "", "1658": "        logger.info(f\"  Found {len(similar)} similar test pairs\")", "1659": "", "1660": "        with open(output_path, \"w\", newline=\"\") as f:", "1661": "            writer = csv.writer(f)", "1662": "            writer.writerow([\"Test 1\", \"Test 2\", \"Similarity\", \"Action\"])", "1663": "", "1664": "            for test1, test2, similarity in similar:", "1665": "                writer.writerow(", "1666": "                    [test1, test2, f\"{similarity:.1%}\", \"Review for merge\"]  # Consistent 1 decimal", "1667": "                )", "1668": "", "1669": "        logger.info(f\"CSV report saved: {output_path}\")", "1670": "", "1671": "    def generate_summary(self, output_path: Path, threshold: float = 0.3) -> None:", "1672": "        \"\"\"", "1673": "        Generate summary CSV with all data and metadata.", "1674": "", "1675": "        Args:", "1676": "            output_path: Path to save CSV", "1677": "            threshold: Similarity threshold (default: 0.3 = 30%)", "1678": "        \"\"\"", "1679": "        from testiq import __version__", "1680": "        from datetime import datetime", "1681": "", "1682": "        logger.info(f\"Generating summary CSV: {output_path}\")", "1683": "        logger.info(f\"  Threshold: {threshold:.1%}\")", "1684": "", "1685": "        exact_dups = self.finder.find_exact_duplicates()", "1686": "        subsets = self.finder.get_sorted_subset_duplicates()  # Use sorted", "1687": "        similar = self.finder.find_similar_coverage(threshold)", "1688": "        duplicate_count = self.finder.get_duplicate_count()", "1689": "", "1690": "        logger.info(f\"  Exact duplicates: {duplicate_count}\")", "1691": "        logger.info(f\"  Subset duplicates: {len(subsets)}\")", "1692": "        logger.info(f\"  Similar pairs: {len(similar)}\")", "1693": "", "1694": "        with open(output_path, \"w\", newline=\"\") as f:", "1695": "            writer = csv.writer(f)", "1696": "", "1697": "            # Metadata section", "1698": "            writer.writerow([\"METADATA\"])", "1699": "            writer.writerow([\"Generated\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")])", "1700": "            writer.writerow([\"TestIQ Version\", __version__])", "1701": "            writer.writerow([\"Similarity Threshold\", f\"{threshold:.1%}\"])", "1702": "            writer.writerow([])", "1703": "", "1704": "            # Summary statistics", "1705": "            writer.writerow([\"SUMMARY STATISTICS\"])", "1706": "            writer.writerow([\"Metric\", \"Value\"])", "1707": "            writer.writerow([\"Total Tests\", len(self.finder.tests)])", "1708": "            writer.writerow([\"Exact Duplicates (can remove)\", duplicate_count])", "1709": "            writer.writerow([\"Exact Duplicate Groups\", len(exact_dups)])", "1710": "            writer.writerow([\"Subset Duplicates\", len(subsets)])", "1711": "            writer.writerow([\"Similar Test Pairs\", len(similar)])", "1712": "            writer.writerow([])", "1713": "", "1714": "            # Exact duplicates section", "1715": "            writer.writerow([\"EXACT DUPLICATES\"])", "1716": "            writer.writerow([\"Group\", \"Test Name\", \"Action\"])", "1717": "            for i, group in enumerate(exact_dups, 1):", "1718": "                for j, test in enumerate(group):", "1719": "                    action = \"Keep\" if j == 0 else \"Remove\"", "1720": "                    writer.writerow([f\"Group {i}\", test, action])", "1721": "            writer.writerow([])", "1722": "", "1723": "            # Subset duplicates section (all, sorted by ratio)", "1724": "            writer.writerow([\"SUBSET DUPLICATES (sorted by coverage ratio)\"])", "1725": "            writer.writerow([\"Subset Test\", \"Superset Test\", \"Coverage Ratio\"])", "1726": "            for subset_test, superset_test, ratio in subsets:", "1727": "                writer.writerow([subset_test, superset_test, f\"{ratio:.1%}\"])", "1728": "            writer.writerow([])", "1729": "", "1730": "            # Similar tests section (all)", "1731": "            writer.writerow([\"SIMILAR TESTS\"])", "1732": "            writer.writerow([\"Test 1\", \"Test 2\", \"Similarity\"])", "1733": "            for test1, test2, similarity in similar:", "1734": "                writer.writerow([test1, test2, f\"{similarity:.1%}\"])", "1735": "", "1736": "        logger.info(f\"CSV report saved: {output_path}\")"}, "/Users/kkotari/github/TestIQ/tests/test_coverage_converter.py": {"1": "\"\"\"Tests for coverage converter module.\"\"\"", "2": "", "3": "import json", "4": "import tempfile", "5": "from pathlib import Path", "6": "", "7": "import pytest", "8": "", "9": "from testiq.coverage_converter import (", "10": "    convert_pytest_contexts,", "11": "    convert_pytest_coverage,", "12": ")", "13": "", "14": "", "15": "class TestConvertPytestCoverage:", "16": "    \"\"\"Tests for convert_pytest_coverage function.\"\"\"", "17": "", "18": "    def test_basic_conversion(self):", "19": "        \"\"\"Test basic conversion of pytest coverage format.\"\"\"", "20": "        coverage_data = {", "21": "            \"files\": {", "22": "                \"src/module.py\": {", "23": "                    \"executed_lines\": [1, 2, 3, 5, 10]", "24": "                },", "25": "                \"src/other.py\": {", "26": "                    \"executed_lines\": [1, 4, 7]", "27": "                }", "28": "            }", "29": "        }", "30": "", "31": "        result = convert_pytest_coverage(coverage_data)", "32": "", "33": "        assert \"all_tests_aggregated\" in result", "34": "        assert len(result[\"all_tests_aggregated\"]) == 2", "35": "        assert sorted(result[\"all_tests_aggregated\"][\"src/module.py\"]) == [1, 2, 3, 5, 10]", "36": "        assert sorted(result[\"all_tests_aggregated\"][\"src/other.py\"]) == [1, 4, 7]", "37": "", "38": "    def test_missing_files_key(self):", "39": "        \"\"\"Test error handling for missing 'files' key.\"\"\"", "40": "        coverage_data = {}", "41": "", "42": "        with pytest.raises(ValueError, match=\"missing 'files' key\"):", "43": "            convert_pytest_coverage(coverage_data)", "44": "", "45": "    def test_missing_executed_lines(self):", "46": "        \"\"\"Test handling of files without executed_lines.\"\"\"", "47": "        coverage_data = {", "48": "            \"files\": {", "49": "                \"src/module.py\": {", "50": "                    \"something_else\": [1, 2, 3]", "51": "                }", "52": "            }", "53": "        }", "54": "", "55": "        result = convert_pytest_coverage(coverage_data)", "56": "        assert result == {}", "57": "", "58": "    def test_invalid_executed_lines_type(self):", "59": "        \"\"\"Test handling of non-list executed_lines.\"\"\"", "60": "        coverage_data = {", "61": "            \"files\": {", "62": "                \"src/module.py\": {", "63": "                    \"executed_lines\": \"not a list\"", "64": "                }", "65": "            }", "66": "        }", "67": "", "68": "        result = convert_pytest_coverage(coverage_data)", "69": "        assert result == {}", "70": "", "71": "    def test_empty_coverage(self):", "72": "        \"\"\"Test handling of empty coverage data.\"\"\"", "73": "        coverage_data = {\"files\": {}}", "74": "", "75": "        result = convert_pytest_coverage(coverage_data)", "76": "        assert result == {}", "77": "", "78": "    def test_line_sorting(self):", "79": "        \"\"\"Test that lines are sorted in output.\"\"\"", "80": "        coverage_data = {", "81": "            \"files\": {", "82": "                \"src/module.py\": {", "83": "                    \"executed_lines\": [10, 5, 1, 3, 2]", "84": "                }", "85": "            }", "86": "        }", "87": "", "88": "        result = convert_pytest_coverage(coverage_data)", "89": "        assert result[\"all_tests_aggregated\"][\"src/module.py\"] == [1, 2, 3, 5, 10]", "90": "", "91": "", "92": "class TestConvertPytestContexts:", "93": "    \"\"\"Tests for convert_pytest_contexts function.\"\"\"", "94": "", "95": "    def test_with_contexts(self):", "96": "        \"\"\"Test conversion with test contexts available.\"\"\"", "97": "        coverage_data = {", "98": "            \"meta\": {", "99": "                \"show_contexts\": True", "100": "            },", "101": "            \"files\": {", "102": "                \"src/module.py\": {", "103": "                    \"contexts\": {", "104": "                        \"test_foo\": [1, 2, 3],", "105": "                        \"test_bar\": [4, 5, 6]", "106": "                    }", "107": "                }", "108": "            }", "109": "        }", "110": "", "111": "        result = convert_pytest_contexts(coverage_data)", "112": "", "113": "        assert \"test_foo\" in result", "114": "        assert \"test_bar\" in result", "115": "        assert result[\"test_foo\"][\"src/module.py\"] == [1, 2, 3]", "116": "        assert result[\"test_bar\"][\"src/module.py\"] == [4, 5, 6]", "117": "", "118": "    def test_without_contexts(self):", "119": "        \"\"\"Test fallback to aggregated format when no contexts.\"\"\"", "120": "        coverage_data = {", "121": "            \"meta\": {", "122": "                \"show_contexts\": False", "123": "            },", "124": "            \"files\": {", "125": "                \"src/module.py\": {", "126": "                    \"executed_lines\": [1, 2, 3]", "127": "                }", "128": "            }", "129": "        }", "130": "", "131": "        result = convert_pytest_contexts(coverage_data)", "132": "", "133": "        # Should fall back to aggregated format", "134": "        assert \"all_tests_aggregated\" in result", "135": "", "136": "    def test_empty_context_name(self):", "137": "        \"\"\"Test handling of empty context names.\"\"\"", "138": "        coverage_data = {", "139": "            \"meta\": {", "140": "                \"show_contexts\": True", "141": "            },", "142": "            \"files\": {", "143": "                \"src/module.py\": {", "144": "                    \"contexts\": {", "145": "                        \"\": [1, 2, 3],", "146": "                        \"test_foo\": [4, 5]", "147": "                    }", "148": "                }", "149": "            }", "150": "        }", "151": "", "152": "        result = convert_pytest_contexts(coverage_data)", "153": "", "154": "        # Empty context should be skipped", "155": "        assert \"\" not in result", "156": "        assert \"test_foo\" in result", "157": "", "158": "    def test_no_contexts_field(self):", "159": "        \"\"\"Test fallback when contexts field is missing.\"\"\"", "160": "        coverage_data = {", "161": "            \"meta\": {", "162": "                \"show_contexts\": True", "163": "            },", "164": "            \"files\": {", "165": "                \"src/module.py\": {", "166": "                    \"executed_lines\": [1, 2, 3]", "167": "                }", "168": "            }", "169": "        }", "170": "", "171": "        result = convert_pytest_contexts(coverage_data)", "172": "", "173": "        # Should fall back to aggregated format", "174": "        assert \"all_tests_aggregated\" in result", "175": "", "176": "", "177": "class TestCLI:", "178": "    \"\"\"Tests for coverage_converter CLI.\"\"\"", "179": "", "180": "    def test_basic_conversion_cli(self, tmp_path):", "181": "        \"\"\"Test CLI basic conversion.\"\"\"", "182": "        # Create test coverage file", "183": "        coverage_file = tmp_path / \"coverage.json\"", "184": "        coverage_data = {", "185": "            \"files\": {", "186": "                \"src/module.py\": {", "187": "                    \"executed_lines\": [1, 2, 3]", "188": "                }", "189": "            }", "190": "        }", "191": "        coverage_file.write_text(json.dumps(coverage_data))", "192": "", "193": "        # Import and run CLI", "194": "        from testiq.coverage_converter import main", "195": "        from click.testing import CliRunner", "196": "", "197": "        runner = CliRunner()", "198": "        output_file = tmp_path / \"output.json\"", "199": "        result = runner.invoke(main, [str(coverage_file), \"-o\", str(output_file)])", "200": "", "201": "        assert result.exit_code == 0", "202": "        assert output_file.exists()", "203": "", "204": "        # Verify output", "205": "        output_data = json.loads(output_file.read_text())", "206": "        assert \"all_tests_aggregated\" in output_data", "207": "", "208": "    def test_with_contexts_flag(self, tmp_path):", "209": "        \"\"\"Test CLI with --with-contexts flag.\"\"\"", "210": "        # Create test coverage file with contexts", "211": "        coverage_file = tmp_path / \"coverage.json\"", "212": "        coverage_data = {", "213": "            \"meta\": {\"show_contexts\": True},", "214": "            \"files\": {", "215": "                \"src/module.py\": {", "216": "                    \"contexts\": {", "217": "                        \"test_foo\": [1, 2, 3]", "218": "                    }", "219": "                }", "220": "            }", "221": "        }", "222": "        coverage_file.write_text(json.dumps(coverage_data))", "223": "", "224": "        from testiq.coverage_converter import main", "225": "        from click.testing import CliRunner", "226": "", "227": "        runner = CliRunner()", "228": "        output_file = tmp_path / \"output.json\"", "229": "        result = runner.invoke(", "230": "            main,", "231": "            [str(coverage_file), \"-o\", str(output_file), \"--with-contexts\"]", "232": "        )", "233": "", "234": "        assert result.exit_code == 0", "235": "        assert output_file.exists()", "236": "", "237": "        # Verify output has contexts", "238": "        output_data = json.loads(output_file.read_text())", "239": "        assert \"test_foo\" in output_data", "240": "", "241": "    def test_default_output_filename(self, tmp_path, monkeypatch):", "242": "        \"\"\"Test CLI uses default output filename.\"\"\"", "243": "        # Create test coverage file", "244": "        coverage_file = tmp_path / \"coverage.json\"", "245": "        coverage_data = {", "246": "            \"files\": {", "247": "                \"src/module.py\": {", "248": "                    \"executed_lines\": [1, 2, 3]", "249": "                }", "250": "            }", "251": "        }", "252": "        coverage_file.write_text(json.dumps(coverage_data))", "253": "", "254": "        # Change to tmp directory", "255": "        monkeypatch.chdir(tmp_path)", "256": "", "257": "        from testiq.coverage_converter import main", "258": "        from click.testing import CliRunner", "259": "", "260": "        runner = CliRunner()", "261": "        result = runner.invoke(main, [str(coverage_file)])", "262": "", "263": "        assert result.exit_code == 0", "264": "        assert (tmp_path / \"testiq_coverage.json\").exists()", "265": "", "266": "    def test_invalid_json(self, tmp_path):", "267": "        \"\"\"Test CLI handles invalid JSON.\"\"\"", "268": "        coverage_file = tmp_path / \"invalid.json\"", "269": "        coverage_file.write_text(\"not valid json\")", "270": "", "271": "        from testiq.coverage_converter import main", "272": "        from click.testing import CliRunner", "273": "", "274": "        runner = CliRunner()", "275": "        result = runner.invoke(main, [str(coverage_file)])", "276": "", "277": "        assert result.exit_code == 1", "278": "        assert \"Error\" in result.output"}, "src/testiq/cicd.py": {"1": "\"\"\"", "2": "CI/CD integration features for TestIQ.", "3": "Provides quality gates, baseline comparison, and trend tracking.", "4": "\"\"\"", "5": "", "6": "import json", "7": "from dataclasses import dataclass, field", "8": "from datetime import datetime", "9": "from pathlib import Path", "10": "from typing import Any, Optional", "11": "", "12": "from testiq.analyzer import CoverageDuplicateFinder", "13": "from testiq.exceptions import ValidationError", "14": "from testiq.logging_config import get_logger", "15": "", "16": "logger = get_logger(__name__)", "17": "", "18": "", "19": "@dataclass", "20": "class QualityGate:", "21": "    \"\"\"Define quality gate thresholds for CI/CD.\"\"\"", "22": "", "23": "    max_duplicates: Optional[int] = None", "24": "    max_duplicate_percentage: Optional[float] = None", "25": "    max_subset_duplicates: Optional[int] = None", "26": "    max_similar_pairs: Optional[int] = None", "27": "    fail_on_increase: bool = True", "28": "", "29": "    def __post_init__(self) -> None:", "30": "        \"\"\"Validate quality gate configuration.\"\"\"", "31": "        if self.max_duplicate_percentage is not None:", "32": "            if not 0.0 <= self.max_duplicate_percentage <= 100.0:", "33": "                raise ValidationError(", "34": "                    f\"max_duplicate_percentage must be 0-100, got {self.max_duplicate_percentage}\"", "35": "                )", "36": "", "37": "", "38": "@dataclass", "39": "class AnalysisResult:", "40": "    \"\"\"Results from TestIQ analysis.\"\"\"", "41": "", "42": "    timestamp: str", "43": "    total_tests: int", "44": "    exact_duplicates: int", "45": "    duplicate_groups: int", "46": "    subset_duplicates: int", "47": "    similar_pairs: int", "48": "    duplicate_percentage: float", "49": "    threshold: float", "50": "    metadata: dict[str, Any] = field(default_factory=dict)", "51": "", "52": "    def to_dict(self) -> dict[str, Any]:", "53": "        \"\"\"Convert to dictionary.\"\"\"", "54": "        return {", "55": "            \"timestamp\": self.timestamp,", "56": "            \"total_tests\": self.total_tests,", "57": "            \"exact_duplicates\": self.exact_duplicates,", "58": "            \"duplicate_groups\": self.duplicate_groups,", "59": "            \"subset_duplicates\": self.subset_duplicates,", "60": "            \"similar_pairs\": self.similar_pairs,", "61": "            \"duplicate_percentage\": self.duplicate_percentage,", "62": "            \"threshold\": self.threshold,", "63": "            \"metadata\": self.metadata,", "64": "        }", "65": "", "66": "    @classmethod", "67": "    def from_dict(cls, data: dict[str, Any]) -> \"AnalysisResult\":", "68": "        \"\"\"Create from dictionary.\"\"\"", "69": "        return cls(", "70": "            timestamp=data[\"timestamp\"],", "71": "            total_tests=data[\"total_tests\"],", "72": "            exact_duplicates=data[\"exact_duplicates\"],", "73": "            duplicate_groups=data[\"duplicate_groups\"],", "74": "            subset_duplicates=data[\"subset_duplicates\"],", "75": "            similar_pairs=data[\"similar_pairs\"],", "76": "            duplicate_percentage=data[\"duplicate_percentage\"],", "77": "            threshold=data[\"threshold\"],", "78": "            metadata=data.get(\"metadata\", {}),", "79": "        )", "80": "", "81": "", "82": "class QualityGateChecker:", "83": "    \"\"\"Check if analysis results pass quality gates.\"\"\"", "84": "", "85": "    def __init__(self, gate: QualityGate) -> None:", "86": "        \"\"\"Initialize quality gate checker.\"\"\"", "87": "        self.gate = gate", "88": "        logger.info(f\"Initialized quality gate: {gate}\")", "89": "", "90": "    def check(", "91": "        self,", "92": "        finder: CoverageDuplicateFinder,", "93": "        threshold: float = 0.3,", "94": "        baseline: Optional[AnalysisResult] = None,", "95": "    ) -> tuple[bool, dict[str, Any]]:", "96": "        \"\"\"", "97": "        Check if analysis passes quality gates.", "98": "", "99": "        Args:", "100": "            finder: CoverageDuplicateFinder instance", "101": "            threshold: Similarity threshold", "102": "            baseline: Optional baseline results for comparison", "103": "", "104": "        Returns:", "105": "            (passed, details) tuple where passed is bool and details contains information", "106": "        \"\"\"", "107": "        logger.info(\"Checking quality gates\")", "108": "", "109": "        exact_dups = finder.find_exact_duplicates()", "110": "        subset_dups = finder.find_subset_duplicates()", "111": "        similar = finder.find_similar_coverage(threshold)", "112": "", "113": "        total_tests = len(finder.tests)", "114": "        duplicate_count = sum(len(g) - 1 for g in exact_dups)", "115": "        duplicate_percentage = (", "116": "            (duplicate_count / total_tests * 100) if total_tests > 0 else 0", "117": "        )", "118": "", "119": "        current = AnalysisResult(", "120": "            timestamp=datetime.now().isoformat(),", "121": "            total_tests=total_tests,", "122": "            exact_duplicates=duplicate_count,", "123": "            duplicate_groups=len(exact_dups),", "124": "            subset_duplicates=len(subset_dups),", "125": "            similar_pairs=len(similar),", "126": "            duplicate_percentage=duplicate_percentage,", "127": "            threshold=threshold,", "128": "        )", "129": "", "130": "        failures = []", "131": "        passed = True", "132": "", "133": "        # Check absolute thresholds", "134": "        if (", "135": "            self.gate.max_duplicates is not None", "136": "            and duplicate_count > self.gate.max_duplicates", "137": "        ):", "138": "            failures.append(", "139": "                f\"Exact duplicates ({duplicate_count}) exceeds limit ({self.gate.max_duplicates})\"", "140": "            )", "141": "            passed = False", "142": "", "143": "        if (", "144": "            self.gate.max_duplicate_percentage is not None", "145": "            and duplicate_percentage > self.gate.max_duplicate_percentage", "146": "        ):", "147": "            failures.append(", "148": "                f\"Duplicate percentage ({duplicate_percentage:.1f}%) exceeds limit ({self.gate.max_duplicate_percentage:.1f}%)\"", "149": "            )", "150": "            passed = False", "151": "", "152": "        if (", "153": "            self.gate.max_subset_duplicates is not None", "154": "            and len(subset_dups) > self.gate.max_subset_duplicates", "155": "        ):", "156": "            failures.append(", "157": "                f\"Subset duplicates ({len(subset_dups)}) exceeds limit ({self.gate.max_subset_duplicates})\"", "158": "            )", "159": "            passed = False", "160": "", "161": "        if (", "162": "            self.gate.max_similar_pairs is not None", "163": "            and len(similar) > self.gate.max_similar_pairs", "164": "        ):", "165": "            failures.append(", "166": "                f\"Similar pairs ({len(similar)}) exceeds limit ({self.gate.max_similar_pairs})\"", "167": "            )", "168": "            passed = False", "169": "", "170": "        # Check against baseline", "171": "        if baseline and self.gate.fail_on_increase:", "172": "            if current.exact_duplicates > baseline.exact_duplicates:", "173": "                failures.append(", "174": "                    f\"Exact duplicates increased from {baseline.exact_duplicates} to {current.exact_duplicates}\"", "175": "                )", "176": "                passed = False", "177": "", "178": "            if current.subset_duplicates > baseline.subset_duplicates:", "179": "                failures.append(", "180": "                    f\"Subset duplicates increased from {baseline.subset_duplicates} to {current.subset_duplicates}\"", "181": "                )", "182": "                passed = False", "183": "", "184": "        details = {", "185": "            \"passed\": passed,", "186": "            \"current\": current.to_dict(),", "187": "            \"baseline\": baseline.to_dict() if baseline else None,", "188": "            \"failures\": failures,", "189": "            \"gate_config\": {", "190": "                \"max_duplicates\": self.gate.max_duplicates,", "191": "                \"max_duplicate_percentage\": self.gate.max_duplicate_percentage,", "192": "                \"max_subset_duplicates\": self.gate.max_subset_duplicates,", "193": "                \"max_similar_pairs\": self.gate.max_similar_pairs,", "194": "                \"fail_on_increase\": self.gate.fail_on_increase,", "195": "            },", "196": "        }", "197": "", "198": "        if passed:", "199": "            logger.info(\"\u2713 Quality gates passed\")", "200": "        else:", "201": "            logger.warning(f\"\u2717 Quality gates failed: {failures}\")", "202": "", "203": "        return passed, details", "204": "", "205": "", "206": "class BaselineManager:", "207": "    \"\"\"Manage baseline results for comparison.\"\"\"", "208": "", "209": "    def __init__(self, baseline_dir: Path) -> None:", "210": "        \"\"\"Initialize baseline manager.\"\"\"", "211": "        self.baseline_dir = baseline_dir", "212": "        self.baseline_dir.mkdir(parents=True, exist_ok=True)", "213": "        logger.debug(f\"Baseline directory: {baseline_dir}\")", "214": "", "215": "    def save(self, result: AnalysisResult, name: str = \"baseline\") -> Path:", "216": "        \"\"\"Save analysis result as baseline.\"\"\"", "217": "        baseline_file = self.baseline_dir / f\"{name}.json\"", "218": "", "219": "        with open(baseline_file, \"w\") as f:", "220": "            json.dump(result.to_dict(), f, indent=2)", "221": "", "222": "        logger.info(f\"Baseline saved: {baseline_file}\")", "223": "        return baseline_file", "224": "", "225": "    def load(self, name: str = \"baseline\") -> Optional[AnalysisResult]:", "226": "        \"\"\"Load baseline result.\"\"\"", "227": "        baseline_file = self.baseline_dir / f\"{name}.json\"", "228": "", "229": "        if not baseline_file.exists():", "230": "            logger.warning(f\"Baseline not found: {baseline_file}\")", "231": "            return None", "232": "", "233": "        with open(baseline_file) as f:", "234": "            data = json.load(f)", "235": "", "236": "        logger.info(f\"Baseline loaded: {baseline_file}\")", "237": "        return AnalysisResult.from_dict(data)", "238": "", "239": "    def list_baselines(self) -> list[dict[str, Any]]:", "240": "        \"\"\"List available baselines with their details.\"\"\"", "241": "        baselines = []", "242": "        for baseline_file in self.baseline_dir.glob(\"*.json\"):", "243": "            try:", "244": "                result = self.load(baseline_file.stem)", "245": "                if result:", "246": "                    baselines.append({", "247": "                        \"name\": baseline_file.stem,", "248": "                        \"result\": result,", "249": "                    })", "250": "            except Exception as e:", "251": "                logger.warning(f\"Failed to load baseline {baseline_file.stem}: {e}\")", "252": "", "253": "        logger.debug(f\"Available baselines: {[b['name'] for b in baselines]}\")", "254": "        return baselines", "255": "", "256": "", "257": "class TrendTracker:", "258": "    \"\"\"Track test duplicate trends over time.\"\"\"", "259": "", "260": "    def __init__(self, history_file: Path) -> None:", "261": "        \"\"\"Initialize trend tracker.\"\"\"", "262": "        self.history_file = history_file", "263": "        self.history_file.parent.mkdir(parents=True, exist_ok=True)", "264": "", "265": "    def add_result(self, result: AnalysisResult) -> None:", "266": "        \"\"\"Add analysis result to history.\"\"\"", "267": "        history = self.load_history()", "268": "        history.append(result.to_dict())", "269": "", "270": "        with open(self.history_file, \"w\") as f:", "271": "            json.dump(history, f, indent=2)", "272": "", "273": "        logger.info(f\"Result added to trend history: {self.history_file}\")", "274": "", "275": "    def load_history(self) -> list[dict[str, Any]]:", "276": "        \"\"\"Load historical results.\"\"\"", "277": "        if not self.history_file.exists():", "278": "            return []", "279": "", "280": "        with open(self.history_file) as f:", "281": "            return json.load(f)", "282": "", "283": "    def get_trend(self, metric: str, limit: int = 10) -> list[float]:", "284": "        \"\"\"", "285": "        Get trend for specific metric.", "286": "", "287": "        Args:", "288": "            metric: Metric name (e.g., 'exact_duplicates', 'duplicate_percentage')", "289": "            limit: Number of recent results to return", "290": "", "291": "        Returns:", "292": "            List of metric values over time", "293": "        \"\"\"", "294": "        history = self.load_history()", "295": "        recent = history[-limit:] if len(history) > limit else history", "296": "        return [r.get(metric, 0) for r in recent]", "297": "", "298": "    def is_improving(self, metric: str = \"exact_duplicates\") -> bool:", "299": "        \"\"\"Check if trend is improving (decreasing for duplicates).\"\"\"", "300": "        trend = self.get_trend(metric, limit=5)", "301": "        if len(trend) < 2:", "302": "            return True  # Not enough data", "303": "", "304": "        # Check if generally decreasing", "305": "        improvements = sum(", "306": "            1 for i in range(1, len(trend)) if trend[i] <= trend[i - 1]", "307": "        )", "308": "        return improvements >= len(trend) // 2", "309": "", "310": "", "311": "def get_exit_code(", "312": "    passed: bool, duplicate_count: int, _total_tests: int", "313": ") -> int:", "314": "    \"\"\"", "315": "    Get appropriate exit code for CI/CD.", "316": "", "317": "    Args:", "318": "        passed: Whether quality gates passed", "319": "        duplicate_count: Number of duplicates found", "320": "        _total_tests: Total number of tests (reserved for future use)", "321": "", "322": "    Returns:", "323": "        Exit code (0=success, 1=duplicates found, 2=quality gate failed)", "324": "    \"\"\"", "325": "    if not passed:", "326": "        return 2  # Quality gate failed", "327": "", "328": "    if duplicate_count > 0:", "329": "        return 1  # Duplicates found but within limits", "330": "", "331": "    return 0  # All good"}, "tests/test_plugins.py": {"1": "\"\"\"", "2": "Tests for plugin/hook system.", "3": "\"\"\"", "4": "", "5": "import pytest", "6": "", "7": "from testiq.plugins import (", "8": "    HookContext,", "9": "    HookType,", "10": "    PluginManager,", "11": "    clear_hooks,", "12": "    get_global_manager,", "13": "    register_hook,", "14": "    trigger_hook,", "15": "    unregister_hook,", "16": ")", "17": "", "18": "", "19": "@pytest.fixture(autouse=True)", "20": "def reset_global_manager():", "21": "    \"\"\"Reset global plugin manager before each test.\"\"\"", "22": "    manager = get_global_manager()", "23": "    manager.clear_hooks()", "24": "    yield", "25": "    manager.clear_hooks()", "26": "", "27": "", "28": "class TestHookType:", "29": "    \"\"\"Tests for HookType enum.\"\"\"", "30": "", "31": "    def test_hook_type_validation_scenarios(self):", "32": "        \"\"\"Test hook type definitions and nonexistent hook behavior.\"\"\"", "33": "        # Test 1: All expected hook types exist", "34": "        expected_hooks = [", "35": "            \"BEFORE_ANALYSIS\",", "36": "            \"AFTER_ANALYSIS\",", "37": "            \"ON_DUPLICATE_FOUND\",", "38": "            \"ON_SUBSET_FOUND\",", "39": "            \"ON_SIMILAR_FOUND\",", "40": "            \"ON_ERROR\",", "41": "            \"ON_QUALITY_GATE_FAIL\",", "42": "        ]", "43": "", "44": "        for hook in expected_hooks:", "45": "            assert hasattr(HookType, hook)", "46": "", "47": "        # Test 2: Triggering nonexistent hook doesn't raise error", "48": "        manager = PluginManager()", "49": "        # Should not raise an error", "50": "        manager.trigger(HookType.BEFORE_ANALYSIS, {})", "51": "", "52": "    def test_hook_type_validation_extended(self):", "53": "        \"\"\"Test hook type values and unregister nonexistent hook behavior.\"\"\"", "54": "        # Test 1: Hook types have string values", "55": "        assert isinstance(HookType.BEFORE_ANALYSIS.value, str)", "56": "        assert HookType.BEFORE_ANALYSIS.value == \"before_analysis\"", "57": "", "58": "        # Test 2: Unregistering nonexistent hook doesn't raise error", "59": "        manager = PluginManager()", "60": "", "61": "        def my_hook(ctx: HookContext):", "62": "            pass", "63": "", "64": "        # Should not raise an error", "65": "        manager.unregister_hook(HookType.BEFORE_ANALYSIS, my_hook)", "66": "", "67": "", "68": "class TestPluginManager:", "69": "    \"\"\"Tests for PluginManager.\"\"\"", "70": "", "71": "    def test_hook_registration_scenarios(self):", "72": "        \"\"\"Test hook registration including single and multiple hooks.\"\"\"", "73": "        manager = PluginManager()", "74": "", "75": "        # Test 1: Register single hook", "76": "        called = []", "77": "", "78": "        def my_hook(ctx: HookContext):", "79": "            called.append(ctx.data)", "80": "", "81": "        manager.register_hook(HookType.BEFORE_ANALYSIS, my_hook)", "82": "", "83": "        # Verify hook is registered", "84": "        assert HookType.BEFORE_ANALYSIS in manager.hooks", "85": "        assert len(manager.hooks[HookType.BEFORE_ANALYSIS]) == 1", "86": "", "87": "        # Test 2: Register multiple hooks for same event", "88": "        manager2 = PluginManager()", "89": "", "90": "        def hook1(ctx: HookContext):", "91": "            pass", "92": "", "93": "        def hook2(ctx: HookContext):", "94": "            pass", "95": "", "96": "        manager2.register_hook(HookType.BEFORE_ANALYSIS, hook1)", "97": "        manager2.register_hook(HookType.BEFORE_ANALYSIS, hook2)", "98": "", "99": "        assert len(manager2.hooks[HookType.BEFORE_ANALYSIS]) == 2", "100": "", "101": "    def test_trigger_hook(self):", "102": "        \"\"\"Test triggering a hook.\"\"\"", "103": "        manager = PluginManager()", "104": "        results = []", "105": "", "106": "        def my_hook(ctx: HookContext):", "107": "            results.append(ctx.data)", "108": "", "109": "        manager.register_hook(HookType.AFTER_ANALYSIS, my_hook)", "110": "        manager.trigger(HookType.AFTER_ANALYSIS, {\"test\": \"data\", \"count\": 42})", "111": "", "112": "        assert len(results) == 1", "113": "        assert results[0][\"test\"] == \"data\"", "114": "        assert results[0][\"count\"] == 42", "115": "", "116": "    def test_trigger_multiple_hooks(self):", "117": "        \"\"\"Test triggering multiple hooks in order.\"\"\"", "118": "        manager = PluginManager()", "119": "        results = []", "120": "", "121": "        def hook1(ctx: HookContext):", "122": "            results.append(\"hook1\")", "123": "", "124": "        def hook2(ctx: HookContext):", "125": "            results.append(\"hook2\")", "126": "", "127": "        manager.register_hook(HookType.ON_ERROR, hook1)", "128": "        manager.register_hook(HookType.ON_ERROR, hook2)", "129": "        manager.trigger(HookType.ON_ERROR, {})", "130": "", "131": "        assert results == [\"hook1\", \"hook2\"]", "132": "", "133": "", "134": "", "135": "    def test_unregister_hook(self):", "136": "        \"\"\"Test unregistering a hook.\"\"\"", "137": "        manager = PluginManager()", "138": "", "139": "        def my_hook(ctx: HookContext):", "140": "            pass", "141": "", "142": "        manager.register_hook(HookType.AFTER_ANALYSIS, my_hook)", "143": "        assert len(manager.hooks[HookType.AFTER_ANALYSIS]) == 1", "144": "", "145": "        manager.unregister_hook(HookType.AFTER_ANALYSIS, my_hook)", "146": "        assert len(manager.hooks.get(HookType.AFTER_ANALYSIS, [])) == 0", "147": "", "148": "", "149": "", "150": "    def test_clear_hooks(self):", "151": "        \"\"\"Test clearing all hooks.\"\"\"", "152": "        manager = PluginManager()", "153": "", "154": "        def hook1(ctx: HookContext):", "155": "            pass", "156": "", "157": "        def hook2(ctx: HookContext):", "158": "            pass", "159": "", "160": "        manager.register_hook(HookType.BEFORE_ANALYSIS, hook1)", "161": "        manager.register_hook(HookType.AFTER_ANALYSIS, hook2)", "162": "", "163": "        manager.clear_hooks()", "164": "        # All hooks should be empty lists", "165": "        for hooks_list in manager.hooks.values():", "166": "            assert len(hooks_list) == 0", "167": "", "168": "    def test_hook_error_handling(self):", "169": "        \"\"\"Test that hook errors don't break the system.\"\"\"", "170": "        manager = PluginManager()", "171": "        results = []", "172": "", "173": "        def failing_hook(ctx: HookContext):", "174": "            raise ValueError(\"Hook error!\")", "175": "", "176": "        def working_hook(ctx: HookContext):", "177": "            results.append(\"worked\")", "178": "", "179": "        manager.register_hook(HookType.ON_ERROR, failing_hook)", "180": "        manager.register_hook(HookType.ON_ERROR, working_hook)", "181": "", "182": "        # Should not raise, and working_hook should still execute", "183": "        manager.trigger(HookType.ON_ERROR, {})", "184": "", "185": "        # Working hook should have been called despite first hook failing", "186": "        assert \"worked\" in results", "187": "", "188": "", "189": "class TestGlobalFunctions:", "190": "    \"\"\"Tests for global convenience functions.\"\"\"", "191": "", "192": "    def test_global_hook_registration_scenarios(self):", "193": "        \"\"\"Test global hook registration including multiple registrations.\"\"\"", "194": "        # Test 1: Simple registration", "195": "        called = []", "196": "", "197": "        def my_hook(ctx: HookContext):", "198": "            called.append(True)", "199": "", "200": "        register_hook(HookType.BEFORE_ANALYSIS, my_hook)", "201": "        trigger_hook(HookType.BEFORE_ANALYSIS)", "202": "", "203": "        assert len(called) == 1", "204": "", "205": "        # Test 2: Multiple registrations of same function", "206": "        called2 = []", "207": "", "208": "        def my_hook2(ctx: HookContext):", "209": "            called2.append(True)", "210": "", "211": "        register_hook(HookType.ON_DUPLICATE_FOUND, my_hook2)", "212": "        register_hook(HookType.ON_DUPLICATE_FOUND, my_hook2)  # Register again", "213": "", "214": "        trigger_hook(HookType.ON_DUPLICATE_FOUND)", "215": "", "216": "        # Function should be called twice", "217": "        assert len(called2) == 2", "218": "", "219": "    def test_unregister_global_hook(self):", "220": "        \"\"\"Test unregistering a global hook.\"\"\"", "221": "        called = []", "222": "", "223": "        def my_hook(ctx: HookContext):", "224": "            called.append(True)", "225": "", "226": "        register_hook(HookType.AFTER_ANALYSIS, my_hook)", "227": "        unregister_hook(HookType.AFTER_ANALYSIS, my_hook)", "228": "        trigger_hook(HookType.AFTER_ANALYSIS)", "229": "", "230": "        assert len(called) == 0", "231": "", "232": "    def test_clear_global_hooks(self):", "233": "        \"\"\"Test clearing all global hooks.\"\"\"", "234": "", "235": "        def hook1(ctx: HookContext):", "236": "            pass", "237": "", "238": "        def hook2(ctx: HookContext):", "239": "            pass", "240": "", "241": "        register_hook(HookType.BEFORE_ANALYSIS, hook1)", "242": "        register_hook(HookType.AFTER_ANALYSIS, hook2)", "243": "", "244": "        clear_hooks()", "245": "", "246": "        manager = get_global_manager()", "247": "        for hooks_list in manager.hooks.values():", "248": "            assert len(hooks_list) == 0", "249": "", "250": "", "251": "", "252": "    def test_hook_with_data(self):", "253": "        \"\"\"Test hooks receive correct data.\"\"\"", "254": "        received_data = []", "255": "", "256": "        def my_hook(ctx: HookContext):", "257": "            received_data.append(ctx.data)", "258": "", "259": "        register_hook(HookType.ON_DUPLICATE_FOUND, my_hook)", "260": "        trigger_hook(", "261": "            HookType.ON_DUPLICATE_FOUND,", "262": "            test1=\"test_login_1\",", "263": "            test2=\"test_login_2\",", "264": "            similarity=1.0,", "265": "        )", "266": "", "267": "        assert len(received_data) == 1", "268": "        assert received_data[0][\"test1\"] == \"test_login_1\"", "269": "        assert received_data[0][\"test2\"] == \"test_login_2\"", "270": "        assert received_data[0][\"similarity\"] == 1.0", "271": "", "272": "", "273": "class TestPluginIntegration:", "274": "    \"\"\"Integration tests for plugin system.\"\"\"", "275": "", "276": "    def test_complete_workflow(self):", "277": "        \"\"\"Test a complete workflow with multiple hooks.\"\"\"", "278": "        events = []", "279": "", "280": "        def before_hook(ctx: HookContext):", "281": "            events.append(\"before\")", "282": "", "283": "        def after_hook(ctx: HookContext):", "284": "            events.append(\"after\")", "285": "", "286": "        def duplicate_hook(ctx: HookContext):", "287": "            events.append(f\"duplicate: {ctx.data.get('count', 0)}\")", "288": "", "289": "        # Register hooks", "290": "        register_hook(HookType.BEFORE_ANALYSIS, before_hook)", "291": "        register_hook(HookType.AFTER_ANALYSIS, after_hook)", "292": "        register_hook(HookType.ON_DUPLICATE_FOUND, duplicate_hook)", "293": "", "294": "        # Simulate analysis workflow", "295": "        trigger_hook(HookType.BEFORE_ANALYSIS)", "296": "        trigger_hook(HookType.ON_DUPLICATE_FOUND, count=5)", "297": "        trigger_hook(HookType.ON_DUPLICATE_FOUND, count=3)", "298": "        trigger_hook(HookType.AFTER_ANALYSIS)", "299": "", "300": "        assert events == [", "301": "            \"before\",", "302": "            \"duplicate: 5\",", "303": "            \"duplicate: 3\",", "304": "            \"after\",", "305": "        ]", "306": "", "307": "    def test_conditional_hook_execution(self):", "308": "        \"\"\"Test that hooks can conditionally execute logic.\"\"\"", "309": "        high_priority_alerts = []", "310": "", "311": "        def alert_hook(ctx: HookContext):", "312": "            count = ctx.data.get(\"count\", 0)", "313": "            if count > 10:", "314": "                high_priority_alerts.append(count)", "315": "", "316": "        register_hook(HookType.ON_DUPLICATE_FOUND, alert_hook)", "317": "", "318": "        # Low count - should not alert", "319": "        trigger_hook(HookType.ON_DUPLICATE_FOUND, count=5)", "320": "        assert len(high_priority_alerts) == 0", "321": "", "322": "        # High count - should alert", "323": "        trigger_hook(HookType.ON_DUPLICATE_FOUND, count=15)", "324": "        assert len(high_priority_alerts) == 1", "325": "        assert high_priority_alerts[0] == 15"}, "src/testiq/plugins.py": {"1": "\"\"\"", "2": "Plugin and hook system for TestIQ.", "3": "Allows users to extend functionality with custom callbacks.", "4": "\"\"\"", "5": "", "6": "from dataclasses import dataclass", "7": "from enum import Enum", "8": "from typing import Any, Callable, Optional", "9": "", "10": "from testiq.logging_config import get_logger", "11": "", "12": "logger = get_logger(__name__)", "13": "", "14": "", "15": "class HookType(Enum):", "16": "    \"\"\"Types of hooks available in TestIQ.\"\"\"", "17": "", "18": "    BEFORE_ANALYSIS = \"before_analysis\"", "19": "    AFTER_ANALYSIS = \"after_analysis\"", "20": "    ON_DUPLICATE_FOUND = \"on_duplicate_found\"", "21": "    ON_SUBSET_FOUND = \"on_subset_found\"", "22": "    ON_SIMILAR_FOUND = \"on_similar_found\"", "23": "    ON_ERROR = \"on_error\"", "24": "    ON_QUALITY_GATE_FAIL = \"on_quality_gate_fail\"", "25": "", "26": "", "27": "@dataclass", "28": "class HookContext:", "29": "    \"\"\"Context passed to hook callbacks.\"\"\"", "30": "", "31": "    hook_type: HookType", "32": "    data: dict[str, Any]", "33": "    metadata: dict[str, Any]", "34": "", "35": "", "36": "class PluginManager:", "37": "    \"\"\"Manage plugins and hooks for TestIQ.\"\"\"", "38": "", "39": "    def __init__(self) -> None:", "40": "        \"\"\"Initialize plugin manager.\"\"\"", "41": "        self._hooks: dict[HookType, list[Callable]] = {hook: [] for hook in HookType}", "42": "        logger.debug(\"Plugin manager initialized\")", "43": "", "44": "    def register_hook(", "45": "        self, hook_type: HookType, callback: Callable[[HookContext], None]", "46": "    ) -> None:", "47": "        \"\"\"", "48": "        Register a hook callback.", "49": "", "50": "        Args:", "51": "            hook_type: Type of hook to register", "52": "            callback: Callback function that receives HookContext", "53": "", "54": "        Example:", "55": "            >>> def on_duplicate(ctx: HookContext):", "56": "            ...     print(f\"Found duplicates: {ctx.data['group']}\")", "57": "            >>> plugin_manager.register_hook(HookType.ON_DUPLICATE_FOUND, on_duplicate)", "58": "        \"\"\"", "59": "        self._hooks[hook_type].append(callback)", "60": "        logger.info(f\"Registered hook: {hook_type.value} -> {callback.__name__}\")", "61": "", "62": "    def unregister_hook(", "63": "        self, hook_type: HookType, callback: Callable[[HookContext], None]", "64": "    ) -> bool:", "65": "        \"\"\"", "66": "        Unregister a hook callback.", "67": "", "68": "        Args:", "69": "            hook_type: Type of hook", "70": "            callback: Callback to remove", "71": "", "72": "        Returns:", "73": "            True if callback was found and removed", "74": "        \"\"\"", "75": "        if callback in self._hooks[hook_type]:", "76": "            self._hooks[hook_type].remove(callback)", "77": "            logger.info(f\"Unregistered hook: {hook_type.value} -> {callback.__name__}\")", "78": "            return True", "79": "        return False", "80": "", "81": "    def trigger(", "82": "        self, hook_type: HookType, data: dict[str, Any], metadata: Optional[dict[str, Any]] = None", "83": "    ) -> None:", "84": "        \"\"\"", "85": "        Trigger all callbacks for a hook type.", "86": "", "87": "        Args:", "88": "            hook_type: Type of hook to trigger", "89": "            data: Data to pass to callbacks", "90": "            metadata: Optional metadata", "91": "        \"\"\"", "92": "        if not self._hooks[hook_type]:", "93": "            return", "94": "", "95": "        context = HookContext(", "96": "            hook_type=hook_type, data=data, metadata=metadata or {}", "97": "        )", "98": "", "99": "        logger.debug(f\"Triggering hook: {hook_type.value} ({len(self._hooks[hook_type])} callbacks)\")", "100": "", "101": "        for callback in self._hooks[hook_type]:", "102": "            try:", "103": "                callback(context)", "104": "            except Exception as e:", "105": "                logger.error(", "106": "                    f\"Error in hook callback {callback.__name__}: {e}\",", "107": "                    exc_info=True,", "108": "                )", "109": "", "110": "    def get_hooks(self, hook_type: HookType) -> list[Callable]:", "111": "        \"\"\"Get all registered hooks for a type.\"\"\"", "112": "        return self._hooks[hook_type].copy()", "113": "", "114": "    def clear_hooks(self, hook_type: Optional[HookType] = None) -> None:", "115": "        \"\"\"", "116": "        Clear hooks.", "117": "", "118": "        Args:", "119": "            hook_type: Specific hook type to clear, or None to clear all", "120": "        \"\"\"", "121": "        if hook_type:", "122": "            self._hooks[hook_type].clear()", "123": "            logger.info(f\"Cleared hooks: {hook_type.value}\")", "124": "        else:", "125": "            for hook in HookType:", "126": "                self._hooks[hook].clear()", "127": "            logger.info(\"Cleared all hooks\")", "128": "", "129": "    @property", "130": "    def hooks(self) -> dict[HookType, list[Callable]]:", "131": "        \"\"\"Get all hooks.\"\"\"", "132": "        return self._hooks", "133": "", "134": "", "135": "# Global plugin manager instance (singleton pattern with lazy initialization)", "136": "_plugin_manager: Optional[PluginManager] = None", "137": "", "138": "", "139": "def get_plugin_manager() -> PluginManager:", "140": "    \"\"\"Get the global plugin manager instance.\"\"\"", "141": "    global _plugin_manager", "142": "    if _plugin_manager is None:", "143": "        _plugin_manager = PluginManager()", "144": "    return _plugin_manager", "145": "", "146": "", "147": "# Convenience functions", "148": "def register_hook(hook_type: HookType, callback: Callable[[HookContext], None]) -> None:", "149": "    \"\"\"Register a hook callback (convenience function).\"\"\"", "150": "    get_plugin_manager().register_hook(hook_type, callback)", "151": "", "152": "", "153": "def unregister_hook(hook_type: HookType, callback: Callable[[HookContext], None]) -> bool:", "154": "    \"\"\"Unregister a hook callback (convenience function).\"\"\"", "155": "    return get_plugin_manager().unregister_hook(hook_type, callback)", "156": "", "157": "", "158": "def trigger_hook(", "159": "    hook_type: HookType, data: Optional[dict[str, Any]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs", "160": ") -> None:", "161": "    \"\"\"Trigger hook callbacks (convenience function).\"\"\"", "162": "    # If kwargs provided, use them as data", "163": "    if kwargs and data is None:", "164": "        data = kwargs", "165": "    elif data is None:", "166": "        data = {}", "167": "    get_plugin_manager().trigger(hook_type, data, metadata)", "168": "", "169": "", "170": "def clear_hooks(hook_type: Optional[HookType] = None) -> None:", "171": "    \"\"\"Clear hooks (convenience function).\"\"\"", "172": "    get_plugin_manager().clear_hooks(hook_type)", "173": "", "174": "", "175": "# Alias for backward compatibility", "176": "def get_global_manager() -> PluginManager:", "177": "    \"\"\"Get the global plugin manager instance (alias for get_plugin_manager).\"\"\"", "178": "    return get_plugin_manager()"}, "src/testiq/security.py": {"1": "\"\"\"", "2": "Security utilities for TestIQ.", "3": "Provides input validation, sanitization, and security checks.", "4": "\"\"\"", "5": "", "6": "import hashlib", "7": "from pathlib import Path", "8": "from typing import Any", "9": "", "10": "from testiq.exceptions import SecurityError, ValidationError", "11": "", "12": "# Default security constants (can be overridden by config)", "13": "# These match the defaults in config.SecurityConfig", "14": "MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB", "15": "MAX_TESTS = 50000", "16": "MAX_LINES_PER_FILE = 100000", "17": "ALLOWED_EXTENSIONS = {\".json\", \".yaml\", \".yml\"}", "18": "", "19": "# Dangerous path patterns for security validation", "20": "DANGEROUS_PATTERNS = {\"../\", \"..\\\\\", \"~\"}", "21": "", "22": "", "23": "def validate_file_path(file_path: Path, check_exists: bool = True) -> Path:", "24": "    \"\"\"", "25": "    Validate and sanitize file path.", "26": "", "27": "    Args:", "28": "        file_path: Path to validate", "29": "        check_exists: Whether to check if file exists", "30": "", "31": "    Returns:", "32": "        Resolved absolute path", "33": "", "34": "    Raises:", "35": "        SecurityError: If path is dangerous", "36": "        ValidationError: If path is invalid", "37": "    \"\"\"", "38": "    try:", "39": "        # Resolve to absolute path", "40": "        resolved = file_path.resolve()", "41": "", "42": "        # Check for path traversal attempts", "43": "        path_str = str(file_path)", "44": "        for pattern in DANGEROUS_PATTERNS:", "45": "            if pattern in path_str:", "46": "                raise SecurityError(f\"Dangerous path pattern detected: {pattern}\")", "47": "", "48": "        # Check if path escapes intended directory", "49": "        # (This is a basic check, adjust based on your security requirements)", "50": "        if check_exists and not resolved.exists():", "51": "            raise ValidationError(f\"File does not exist: {file_path}\")", "52": "", "53": "        # Check file extension", "54": "        if resolved.suffix.lower() not in ALLOWED_EXTENSIONS:", "55": "            raise SecurityError(", "56": "                f\"File extension not allowed: {resolved.suffix}. \"", "57": "                f\"Allowed: {', '.join(ALLOWED_EXTENSIONS)}\"", "58": "            )", "59": "", "60": "        return resolved", "61": "", "62": "    except (OSError, RuntimeError) as e:", "63": "        raise ValidationError(f\"Invalid file path: {file_path} - {e}\")", "64": "", "65": "", "66": "def check_file_size(file_path: Path, max_size: int = MAX_FILE_SIZE) -> None:", "67": "    \"\"\"", "68": "    Check if file size is within limits.", "69": "", "70": "    Args:", "71": "        file_path: Path to check", "72": "        max_size: Maximum allowed size in bytes", "73": "", "74": "    Raises:", "75": "        SecurityError: If file is too large", "76": "    \"\"\"", "77": "    try:", "78": "        file_size = file_path.stat().st_size", "79": "        if file_size > max_size:", "80": "            size_mb = file_size / (1024 * 1024)", "81": "            max_mb = max_size / (1024 * 1024)", "82": "            raise SecurityError(f\"File too large: {size_mb:.2f}MB exceeds limit of {max_mb:.2f}MB\")", "83": "    except OSError as e:", "84": "        raise ValidationError(f\"Cannot check file size: {e}\")", "85": "", "86": "", "87": "def validate_coverage_data(data: dict[str, Any], max_tests: int = MAX_TESTS) -> None:", "88": "    \"\"\"", "89": "    Validate coverage data structure and limits.", "90": "", "91": "    Args:", "92": "        data: Coverage data dictionary", "93": "        max_tests: Maximum number of tests allowed", "94": "", "95": "    Raises:", "96": "        ValidationError: If data is invalid", "97": "        SecurityError: If limits are exceeded", "98": "    \"\"\"", "99": "    if not isinstance(data, dict):", "100": "        raise ValidationError(\"Coverage data must be a dictionary\")", "101": "", "102": "    if len(data) == 0:", "103": "        raise ValidationError(\"Coverage data is empty\")", "104": "", "105": "    if len(data) > max_tests:", "106": "        raise SecurityError(f\"Too many tests: {len(data)} exceeds limit of {max_tests}\")", "107": "", "108": "    # Validate structure", "109": "    for test_name, coverage in data.items():", "110": "        if not isinstance(test_name, str):", "111": "            raise ValidationError(f\"Test name must be string, got: {type(test_name)}\")", "112": "", "113": "        if not test_name.strip():", "114": "            raise ValidationError(\"Test name cannot be empty\")", "115": "", "116": "        if not isinstance(coverage, dict):", "117": "            raise ValidationError(", "118": "                f\"Coverage for '{test_name}' must be a dictionary, got: {type(coverage)}\"", "119": "            )", "120": "", "121": "        # Validate each file's coverage", "122": "        total_lines = 0", "123": "        for file_name, lines in coverage.items():", "124": "            if not isinstance(file_name, str):", "125": "                raise ValidationError(f\"File name must be string, got: {type(file_name)}\")", "126": "", "127": "            if not isinstance(lines, list):", "128": "                raise ValidationError(", "129": "                    f\"Coverage lines for '{file_name}' must be a list, got: {type(lines)}\"", "130": "                )", "131": "", "132": "            total_lines += len(lines)", "133": "", "134": "            # Validate line numbers", "135": "            for line_num in lines:", "136": "                if not isinstance(line_num, int):", "137": "                    raise ValidationError(f\"Line number must be integer, got: {type(line_num)}\")", "138": "                if line_num < 1:", "139": "                    raise ValidationError(f\"Invalid line number: {line_num} (must be >= 1)\")", "140": "", "141": "        # Check total lines limit", "142": "        if total_lines > MAX_LINES_PER_FILE:", "143": "            raise SecurityError(", "144": "                f\"Test '{test_name}' covers too many lines: {total_lines} \"", "145": "                f\"exceeds limit of {MAX_LINES_PER_FILE}\"", "146": "            )", "147": "", "148": "", "149": "def sanitize_output_path(output_path: Path, allowed_dirs: list[Path] = None) -> Path:", "150": "    \"\"\"", "151": "    Sanitize output file path.", "152": "", "153": "    Args:", "154": "        output_path: Path to sanitize", "155": "        allowed_dirs: List of allowed directories (if None, any directory is allowed)", "156": "", "157": "    Returns:", "158": "        Sanitized absolute path", "159": "", "160": "    Raises:", "161": "        SecurityError: If path is not allowed", "162": "    \"\"\"", "163": "    try:", "164": "        resolved = output_path.resolve()", "165": "", "166": "        # Check for dangerous patterns", "167": "        path_str = str(output_path)", "168": "        for pattern in DANGEROUS_PATTERNS:", "169": "            if pattern in path_str:", "170": "                raise SecurityError(f\"Dangerous path pattern detected: {pattern}\")", "171": "", "172": "        # Check allowed directories", "173": "        if allowed_dirs:", "174": "            allowed = False", "175": "            for allowed_dir in allowed_dirs:", "176": "                try:", "177": "                    resolved.relative_to(allowed_dir.resolve())", "178": "                    allowed = True", "179": "                    break", "180": "                except ValueError:", "181": "                    continue", "182": "", "183": "            if not allowed:", "184": "                raise SecurityError(f\"Output path not in allowed directories: {output_path}\")", "185": "", "186": "        return resolved", "187": "", "188": "    except (OSError, RuntimeError) as e:", "189": "        raise ValidationError(f\"Invalid output path: {output_path} - {e}\")", "190": "", "191": "", "192": "def compute_file_hash(file_path: Path) -> str:", "193": "    \"\"\"", "194": "    Compute SHA-256 hash of file for integrity verification.", "195": "", "196": "    This function is primarily used for file integrity checks and cache validation.", "197": "    Can be used to verify that coverage files haven't been tampered with.", "198": "", "199": "    Args:", "200": "        file_path: Path to file", "201": "", "202": "    Returns:", "203": "        Hexadecimal hash string", "204": "    \"\"\"", "205": "    sha256_hash = hashlib.sha256()", "206": "    with open(file_path, \"rb\") as f:", "207": "        for byte_block in iter(lambda: f.read(4096), b\"\"):", "208": "            sha256_hash.update(byte_block)", "209": "    return sha256_hash.hexdigest()"}, "src/testiq/analyzer.py": {"1": "\"\"\"", "2": "Coverage-based test duplicate detector.", "3": "Analyzes test coverage to find redundant tests.", "4": "\"\"\"", "5": "", "6": "import time", "7": "from collections import defaultdict", "8": "from dataclasses import dataclass", "9": "from typing import Optional", "10": "", "11": "from testiq.exceptions import AnalysisError, ValidationError", "12": "from testiq.logging_config import get_logger", "13": "from testiq.performance import (", "14": "    CacheManager,", "15": "    ParallelProcessor,", "16": "    ProgressTracker,", "17": "    compute_similarity,", "18": ")", "19": "", "20": "logger = get_logger(__name__)", "21": "", "22": "# Constants", "23": "NO_TESTS_WARNING = \"No tests to analyze\"", "24": "", "25": "", "26": "@dataclass", "27": "class CoverageData:", "28": "    \"\"\"Represents coverage data for a single test.\"\"\"", "29": "", "30": "    test_name: str", "31": "    covered_lines: set[tuple[str, int]]  # (filename, line_number)", "32": "", "33": "    def __hash__(self) -> int:", "34": "        return hash(self.test_name)", "35": "", "36": "", "37": "class CoverageDuplicateFinder:", "38": "    \"\"\"Finds duplicate tests based on coverage analysis.\"\"\"", "39": "", "40": "    def __init__(", "41": "        self,", "42": "        enable_parallel: bool = True,", "43": "        max_workers: int = 4,", "44": "        enable_caching: bool = True,", "45": "        cache_dir: Optional[str] = None,", "46": "    ) -> None:", "47": "        \"\"\"", "48": "        Initialize the duplicate finder.", "49": "", "50": "        Args:", "51": "            enable_parallel: Enable parallel processing", "52": "            max_workers: Maximum number of parallel workers", "53": "            enable_caching: Enable result caching", "54": "            cache_dir: Directory for cache files", "55": "        \"\"\"", "56": "        self.tests: list[CoverageData] = []", "57": "        self.parallel_processor = ParallelProcessor(", "58": "            max_workers=max_workers, enabled=enable_parallel", "59": "        )", "60": "        self.cache_manager = CacheManager(cache_dir=cache_dir, enabled=enable_caching)", "61": "        logger.info(", "62": "            f\"Initialized CoverageDuplicateFinder (parallel={enable_parallel}, \"", "63": "            f\"caching={enable_caching})\"", "64": "        )", "65": "", "66": "    def add_test_coverage(self, test_name: str, coverage: dict[str, list[int]]) -> None:", "67": "        \"\"\"", "68": "        Add coverage data for a test.", "69": "", "70": "        Args:", "71": "            test_name: Name of the test", "72": "            coverage: Dict mapping filename -> list of covered line numbers", "73": "", "74": "        Raises:", "75": "            ValidationError: If test_name is empty or coverage is invalid", "76": "        \"\"\"", "77": "        if not test_name or not test_name.strip():", "78": "            raise ValidationError(\"Test name cannot be empty\")", "79": "", "80": "        if not isinstance(coverage, dict):", "81": "            raise ValidationError(f\"Coverage must be a dict, got {type(coverage)}\")", "82": "", "83": "        try:", "84": "            covered_lines = set()", "85": "            for filename, lines in coverage.items():", "86": "                if not isinstance(lines, list):", "87": "                    raise ValidationError(", "88": "                        f\"Coverage lines for '{filename}' must be a list, got {type(lines)}\"", "89": "                    )", "90": "                for line in lines:", "91": "                    if not isinstance(line, int) or line < 1:", "92": "                        raise ValidationError(f\"Invalid line number for '{filename}': {line}\")", "93": "                    covered_lines.add((filename, line))", "94": "", "95": "            self.tests.append(CoverageData(test_name, covered_lines))", "96": "            logger.debug(f\"Added test '{test_name}' with {len(covered_lines)} covered lines\")", "97": "", "98": "        except Exception as e:", "99": "            logger.error(f\"Error adding test coverage for '{test_name}': {e}\")", "100": "            raise", "101": "", "102": "    def find_exact_duplicates(self) -> list[list[str]]:", "103": "        \"\"\"", "104": "        Find tests with identical coverage.", "105": "", "106": "        Returns:", "107": "            List of test groups where each group has identical coverage", "108": "", "109": "        Raises:", "110": "            AnalysisError: If analysis fails", "111": "        \"\"\"", "112": "        if not self.tests:", "113": "            logger.warning(NO_TESTS_WARNING)", "114": "            return []", "115": "", "116": "        logger.info(f\"Finding exact duplicates among {len(self.tests)} tests\")", "117": "        start_time = time.time()", "118": "", "119": "        try:", "120": "            coverage_map: dict[frozenset, list[str]] = defaultdict(list)", "121": "", "122": "            for test in self.tests:", "123": "                coverage_key = frozenset(test.covered_lines)", "124": "                coverage_map[coverage_key].append(test.test_name)", "125": "", "126": "            # Only return groups with more than one test (duplicates)", "127": "            duplicates = [tests for tests in coverage_map.values() if len(tests) > 1]", "128": "", "129": "            elapsed = time.time() - start_time", "130": "            logger.info(f\"Found {len(duplicates)} duplicate groups in {elapsed:.2f}s\")", "131": "            return duplicates", "132": "", "133": "        except Exception as e:", "134": "            logger.error(f\"Error finding exact duplicates: {e}\")", "135": "            raise AnalysisError(f\"Failed to find exact duplicates: {e}\")", "136": "", "137": "    def find_subset_duplicates(self) -> list[tuple[str, str, float]]:", "138": "        \"\"\"", "139": "        Find tests where one is a subset of another.", "140": "", "141": "        Returns:", "142": "            List of (subset_test, superset_test, coverage_ratio) tuples", "143": "", "144": "        Raises:", "145": "            AnalysisError: If analysis fails", "146": "        \"\"\"", "147": "        if not self.tests:", "148": "            logger.warning(NO_TESTS_WARNING)", "149": "            return []", "150": "", "151": "        logger.info(f\"Finding subset duplicates among {len(self.tests)} tests\")", "152": "        start_time = time.time()", "153": "", "154": "        try:", "155": "            subsets = []", "156": "            progress = ProgressTracker(len(self.tests), \"Subset analysis\")", "157": "", "158": "            for i, test1 in enumerate(self.tests):", "159": "                for test2 in self.tests[i + 1 :]:", "160": "                    if test1.covered_lines == test2.covered_lines:", "161": "                        continue  # Skip exact duplicates (handled separately)", "162": "", "163": "                    if test1.covered_lines.issubset(test2.covered_lines):", "164": "                        ratio = len(test1.covered_lines) / len(test2.covered_lines)", "165": "                        subsets.append((test1.test_name, test2.test_name, ratio))", "166": "                    elif test2.covered_lines.issubset(test1.covered_lines):", "167": "                        ratio = len(test2.covered_lines) / len(test1.covered_lines)", "168": "                        subsets.append((test2.test_name, test1.test_name, ratio))", "169": "", "170": "                if i % 10 == 0:", "171": "                    progress.update(10)", "172": "", "173": "            elapsed = time.time() - start_time", "174": "            logger.info(f\"Found {len(subsets)} subset duplicates in {elapsed:.2f}s\")", "175": "            return subsets", "176": "", "177": "        except Exception as e:", "178": "            logger.error(f\"Error finding subset duplicates: {e}\")", "179": "            raise AnalysisError(f\"Failed to find subset duplicates: {e}\")", "180": "", "181": "    def get_sorted_subset_duplicates(self) -> list[tuple[str, str, float]]:", "182": "        \"\"\"", "183": "        Get subset duplicates sorted by coverage ratio (highest first).", "184": "", "185": "        Returns:", "186": "            List of (subset_test, superset_test, coverage_ratio) tuples sorted by ratio", "187": "        \"\"\"", "188": "        subsets = self.find_subset_duplicates()", "189": "        return sorted(subsets, key=lambda x: x[2], reverse=True)", "190": "", "191": "    def get_duplicate_count(self) -> int:", "192": "        \"\"\"", "193": "        Get the total number of duplicate tests that can be removed.", "194": "", "195": "        Returns:", "196": "            Number of tests that are exact duplicates (excluding one to keep per group)", "197": "        \"\"\"", "198": "        exact_dups = self.find_exact_duplicates()", "199": "        return sum(len(g) - 1 for g in exact_dups)", "200": "", "201": "    def get_statistics(self, threshold: float = 0.3) -> dict:", "202": "        \"\"\"", "203": "        Get comprehensive statistics about test duplication.", "204": "", "205": "        Args:", "206": "            threshold: Similarity threshold for analysis (default: 0.3)", "207": "", "208": "        Returns:", "209": "            Dictionary with all statistics", "210": "        \"\"\"", "211": "        exact = self.find_exact_duplicates()", "212": "        subsets = self.find_subset_duplicates()", "213": "        similar = self.find_similar_coverage(threshold)", "214": "", "215": "        return {", "216": "            'total_tests': len(self.tests),", "217": "            'exact_duplicate_groups': len(exact),", "218": "            'exact_duplicate_count': sum(len(g) - 1 for g in exact),", "219": "            'subset_duplicate_count': len(subsets),", "220": "            'similar_pair_count': len(similar),", "221": "            'total_removable_duplicates': sum(len(g) - 1 for g in exact) + len(subsets),", "222": "            'threshold': threshold", "223": "        }", "224": "", "225": "    def find_similar_coverage(self, threshold: float = 0.8) -> list[tuple[str, str, float]]:", "226": "        \"\"\"", "227": "        Find tests with similar (but not identical) coverage using Jaccard similarity.", "228": "", "229": "        Args:", "230": "            threshold: Minimum similarity ratio (0.0 to 1.0)", "231": "", "232": "        Returns:", "233": "            List of (test1, test2, similarity) tuples", "234": "", "235": "        Raises:", "236": "            ValidationError: If threshold is invalid", "237": "            AnalysisError: If analysis fails", "238": "        \"\"\"", "239": "        if not 0.0 <= threshold <= 1.0:", "240": "            raise ValidationError(f\"Threshold must be between 0.0 and 1.0, got {threshold}\")", "241": "", "242": "        if not self.tests:", "243": "            logger.warning(NO_TESTS_WARNING)", "244": "            return []", "245": "", "246": "        logger.info(f\"Finding similar tests (threshold={threshold}) among {len(self.tests)} tests\")", "247": "        start_time = time.time()", "248": "", "249": "        try:", "250": "            similar = []", "251": "            progress = ProgressTracker(len(self.tests), \"Similarity analysis\")", "252": "", "253": "            for i, test1 in enumerate(self.tests):", "254": "                for test2 in self.tests[i + 1 :]:", "255": "                    # Use cached similarity computation", "256": "                    similarity = compute_similarity(", "257": "                        frozenset(test1.covered_lines), frozenset(test2.covered_lines)", "258": "                    )", "259": "", "260": "                    if threshold <= similarity < 1.0:", "261": "                        similar.append((test1.test_name, test2.test_name, similarity))", "262": "", "263": "                if i % 10 == 0:", "264": "                    progress.update(10)", "265": "", "266": "            result = sorted(similar, key=lambda x: x[2], reverse=True)", "267": "            elapsed = time.time() - start_time", "268": "            logger.info(f\"Found {len(result)} similar test pairs in {elapsed:.2f}s\")", "269": "            return result", "270": "", "271": "        except Exception as e:", "272": "            logger.error(f\"Error finding similar coverage: {e}\")", "273": "            raise AnalysisError(f\"Failed to find similar coverage: {e}\")", "274": "", "275": "    def generate_report(self, threshold: float = 0.3) -> str:", "276": "        \"\"\"", "277": "        Generate a comprehensive duplicate report.", "278": "", "279": "        Args:", "280": "            threshold: Similarity threshold for analysis (default: 0.3 = 30%)", "281": "", "282": "        Returns:", "283": "            Markdown formatted report", "284": "        \"\"\"", "285": "        from testiq import __version__", "286": "        from datetime import datetime", "287": "", "288": "        report_lines = [\"# Test Duplication Report\\n\"]", "289": "        report_lines.append(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")", "290": "        report_lines.append(f\"**TestIQ Version:** {__version__}\")", "291": "        report_lines.append(f\"**Similarity Threshold:** {threshold:.1%}\\n\")", "292": "", "293": "        # Exact duplicates", "294": "        exact_dups = self.find_exact_duplicates()", "295": "        duplicate_count = self.get_duplicate_count()", "296": "        report_lines.append(\"## Exact Duplicates (Identical Coverage)\\n\")", "297": "        report_lines.append(f\"Found {len(exact_dups)} groups with {duplicate_count} duplicate tests:\\n\")", "298": "", "299": "        for i, group in enumerate(exact_dups, 1):", "300": "            report_lines.append(f\"\\n### Group {i} ({len(group)} tests):\")", "301": "            for test in group:", "302": "                report_lines.append(f\"  - {test}\")", "303": "            report_lines.append(", "304": "                f\"\\n  **Action**: Keep one test, remove {len(group) - 1} duplicates\\n\"", "305": "            )", "306": "", "307": "        # Subset duplicates (sorted by coverage ratio)", "308": "        subsets = self.get_sorted_subset_duplicates()", "309": "        report_lines.append(\"\\n## Subset Duplicates\\n\")", "310": "        report_lines.append(f\"Found {len(subsets)} tests that are subsets of others (showing top 20 by coverage ratio):\\n\")", "311": "", "312": "        for subset_test, superset_test, ratio in subsets[:20]:  # Top 20", "313": "            report_lines.append(", "314": "                f\"\\n  - `{subset_test}` is {ratio:.1%} covered by `{superset_test}`\"", "315": "            )", "316": "            report_lines.append(\"    **Action**: Consider removing if no unique edge cases\\n\")", "317": "", "318": "        if len(subsets) > 20:", "319": "            report_lines.append(f\"\\n  ... and {len(subsets) - 20} more subset duplicates\\n\")", "320": "", "321": "        # Similar coverage", "322": "        similar = self.find_similar_coverage(threshold)", "323": "        report_lines.append(f\"\\n## Similar Tests (\u2265{threshold:.0%} overlap)\\n\")", "324": "        report_lines.append(f\"Found {len(similar)} test pairs with \u2265{threshold:.0%} similarity (showing top 20):\\n\")", "325": "", "326": "        for test1, test2, similarity in similar[:20]:  # Top 20", "327": "            report_lines.append(f\"\\n  - `{test1}` \u2194 `{test2}`: {similarity:.1%} similar\")", "328": "            report_lines.append(\"    **Action**: Review for potential merge or refactoring\\n\")", "329": "", "330": "        if len(similar) > 20:", "331": "            report_lines.append(f\"\\n  ... and {len(similar) - 20} more similar test pairs\\n\")", "332": "", "333": "        # Summary statistics", "334": "        report_lines.append(\"\\n## Summary\\n\")", "335": "        report_lines.append(f\"- Total tests analyzed: {len(self.tests)}\")", "336": "        report_lines.append(", "337": "            f\"- Exact duplicates: {duplicate_count} tests can be removed\"", "338": "        )", "339": "        report_lines.append(f\"- Subset duplicates: {len(subsets)} tests may be redundant\")", "340": "        report_lines.append(f\"- Similar tests: {len(similar)} pairs need review\")", "341": "", "342": "        return \"\\n\".join(report_lines)"}, "src/testiq/analysis.py": {"1": "\"\"\"", "2": "Advanced analysis features for TestIQ.", "3": "Provides test quality scoring and intelligent recommendations.", "4": "\"\"\"", "5": "", "6": "from dataclasses import dataclass", "7": "from typing import Any, Optional", "8": "", "9": "from testiq.analyzer import CoverageDuplicateFinder", "10": "from testiq.logging_config import get_logger", "11": "", "12": "logger = get_logger(__name__)", "13": "", "14": "", "15": "@dataclass", "16": "class QualityScore:", "17": "    \"\"\"Quality score for test suite.\"\"\"", "18": "", "19": "    overall_score: float  # 0-100", "20": "    duplication_score: float  # 0-100 (100 = no duplicates)", "21": "    coverage_efficiency_score: float  # 0-100", "22": "    uniqueness_score: float  # 0-100", "23": "    grade: str  # A+, A, B, C, D, F", "24": "    recommendations: list[str]", "25": "", "26": "    def __str__(self) -> str:", "27": "        \"\"\"String representation.\"\"\"", "28": "        return f\"Quality Score: {self.overall_score:.1f}/100 (Grade: {self.grade})\"", "29": "", "30": "", "31": "class QualityAnalyzer:", "32": "    \"\"\"", "33": "    Analyze test suite quality and calculate quality scores.", "34": "", "35": "    Responsibilities:", "36": "    - Calculate quality metrics (duplication, efficiency, uniqueness)", "37": "    - Assign letter grades (A+ to F)", "38": "    - Generate text recommendations", "39": "", "40": "    For structured action items and detailed reports, use RecommendationEngine.", "41": "    \"\"\"", "42": "", "43": "    def __init__(self, finder: CoverageDuplicateFinder) -> None:", "44": "        \"\"\"Initialize quality analyzer.\"\"\"", "45": "        self.finder = finder", "46": "", "47": "    def calculate_score(self, threshold: float = 0.3) -> QualityScore:", "48": "        \"\"\"", "49": "        Calculate comprehensive quality score.", "50": "", "51": "        Args:", "52": "            threshold: Similarity threshold for analysis", "53": "", "54": "        Returns:", "55": "            QualityScore with detailed metrics", "56": "        \"\"\"", "57": "        logger.info(\"Calculating test quality score\")", "58": "", "59": "        exact_dups = self.finder.find_exact_duplicates()", "60": "        subset_dups = self.finder.find_subset_duplicates()", "61": "        similar = self.finder.find_similar_coverage(threshold)", "62": "", "63": "        total_tests = len(self.finder.tests)", "64": "        if total_tests == 0:", "65": "            return QualityScore(", "66": "                overall_score=0,", "67": "                duplication_score=0,", "68": "                coverage_efficiency_score=0,", "69": "                uniqueness_score=0,", "70": "                grade=\"F\",", "71": "                recommendations=[\"No tests found\"],", "72": "            )", "73": "", "74": "        duplicate_count = sum(len(g) - 1 for g in exact_dups)", "75": "", "76": "        # Calculate duplication score (100 = no duplicates)", "77": "        duplicate_percentage = (duplicate_count / total_tests) * 100", "78": "        duplication_score = max(0, 100 - (duplicate_percentage * 2))", "79": "", "80": "        # Calculate coverage efficiency score (penalize subsets)", "81": "        subset_percentage = (len(subset_dups) / total_tests) * 100 if total_tests > 0 else 0", "82": "        coverage_efficiency_score = max(0, 100 - subset_percentage)", "83": "", "84": "        # Calculate uniqueness score (penalize similar tests)", "85": "        similar_percentage = (len(similar) / (total_tests * (total_tests - 1) / 2)) * 100 if total_tests > 1 else 0", "86": "        uniqueness_score = max(0, 100 - (similar_percentage * 0.5))", "87": "", "88": "        # Overall score (weighted average)", "89": "        overall_score = (", "90": "            duplication_score * 0.5", "91": "            + coverage_efficiency_score * 0.3", "92": "            + uniqueness_score * 0.2", "93": "        )", "94": "", "95": "        # Determine grade", "96": "        if overall_score >= 95:", "97": "            grade = \"A+\"", "98": "        elif overall_score >= 90:", "99": "            grade = \"A\"", "100": "        elif overall_score >= 80:", "101": "            grade = \"B\"", "102": "        elif overall_score >= 70:", "103": "            grade = \"C\"", "104": "        elif overall_score >= 60:", "105": "            grade = \"D\"", "106": "        else:", "107": "            grade = \"F\"", "108": "", "109": "        # Generate recommendations", "110": "        recommendations = self._generate_recommendations(", "111": "            duplicate_count, len(subset_dups), len(similar), total_tests", "112": "        )", "113": "", "114": "        score = QualityScore(", "115": "            overall_score=overall_score,", "116": "            duplication_score=duplication_score,", "117": "            coverage_efficiency_score=coverage_efficiency_score,", "118": "            uniqueness_score=uniqueness_score,", "119": "            grade=grade,", "120": "            recommendations=recommendations,", "121": "        )", "122": "", "123": "        logger.info(f\"Quality score: {score.overall_score:.1f}/100 (Grade: {grade})\")", "124": "        return score", "125": "", "126": "    def _add_duplicate_recommendations(", "127": "        self, recommendations: list[str], duplicate_count: int, total_tests: int", "128": "    ) -> None:", "129": "        \"\"\"Add recommendations for exact duplicates.\"\"\"", "130": "        if duplicate_count <= 0:", "131": "            return", "132": "", "133": "        percentage = (duplicate_count / total_tests) * 100", "134": "        if percentage > 20:", "135": "            recommendations.append(", "136": "                f\"\u26a0\ufe0f CRITICAL: Remove {duplicate_count} exact duplicate tests ({percentage:.1f}% of total)\"", "137": "            )", "138": "        elif percentage > 10:", "139": "            recommendations.append(", "140": "                f\"\u26a0\ufe0f HIGH: Remove {duplicate_count} exact duplicate tests ({percentage:.1f}% of total)\"", "141": "            )", "142": "        else:", "143": "            recommendations.append(", "144": "                f\"\ud83d\udccb Remove {duplicate_count} exact duplicate tests to improve maintainability\"", "145": "            )", "146": "", "147": "    def _add_subset_recommendations(", "148": "        self, recommendations: list[str], subset_count: int, total_tests: int", "149": "    ) -> None:", "150": "        \"\"\"Add recommendations for subset duplicates.\"\"\"", "151": "        if subset_count <= 0:", "152": "            return", "153": "", "154": "        percentage = (subset_count / total_tests) * 100", "155": "        if percentage > 15:", "156": "            recommendations.append(", "157": "                f\"\u26a0\ufe0f Review {subset_count} subset duplicates - many tests may be redundant\"", "158": "            )", "159": "        else:", "160": "            recommendations.append(", "161": "                f\"\ud83d\udccb Review {subset_count} subset duplicates for potential consolidation\"", "162": "            )", "163": "", "164": "    def _add_similar_recommendations(", "165": "        self, recommendations: list[str], similar_count: int, total_tests: int", "166": "    ) -> None:", "167": "        \"\"\"Add recommendations for similar tests.\"\"\"", "168": "        if similar_count <= 0:", "169": "            return", "170": "", "171": "        if similar_count > total_tests:", "172": "            recommendations.append(", "173": "                f\"\u26a0\ufe0f Consider refactoring {similar_count} similar test pairs - high overlap detected\"", "174": "            )", "175": "        elif similar_count > total_tests // 2:", "176": "            recommendations.append(", "177": "                f\"\ud83d\udccb Review {similar_count} similar test pairs for possible merging\"", "178": "            )", "179": "", "180": "    def _add_best_practice_recommendations(", "181": "        self, recommendations: list[str], duplicate_count: int, subset_count: int,", "182": "        similar_count: int, total_tests: int", "183": "    ) -> None:", "184": "        \"\"\"Add best practice recommendations.\"\"\"", "185": "        # Positive feedback", "186": "        if not recommendations:", "187": "            recommendations.append(\"\u2705 Excellent! No significant test duplication detected\")", "188": "            recommendations.append(\"\ud83d\udca1 Continue maintaining this high quality standard\")", "189": "", "190": "        # Best practices", "191": "        if total_tests > 100 and duplicate_count == 0:", "192": "            recommendations.append(\"\ud83c\udf1f Great job maintaining a large test suite without duplicates!\")", "193": "", "194": "        if subset_count > 10:", "195": "            recommendations.append(", "196": "                \"\ud83d\udca1 Consider using test parametrization to reduce subset duplicates\"", "197": "            )", "198": "", "199": "        if similar_count > 20:", "200": "            recommendations.append(", "201": "                \"\ud83d\udca1 Use shared test fixtures and helper functions to reduce code duplication\"", "202": "            )", "203": "", "204": "    def _generate_recommendations(", "205": "        self,", "206": "        duplicate_count: int,", "207": "        subset_count: int,", "208": "        similar_count: int,", "209": "        total_tests: int,", "210": "    ) -> list[str]:", "211": "        \"\"\"Generate actionable recommendations based on analysis.\"\"\"", "212": "        recommendations = []", "213": "", "214": "        # Add all recommendation types", "215": "        self._add_duplicate_recommendations(recommendations, duplicate_count, total_tests)", "216": "        self._add_subset_recommendations(recommendations, subset_count, total_tests)", "217": "        self._add_similar_recommendations(recommendations, similar_count, total_tests)", "218": "        self._add_best_practice_recommendations(", "219": "            recommendations, duplicate_count, subset_count, similar_count, total_tests", "220": "        )", "221": "", "222": "        return recommendations", "223": "", "224": "", "225": "class RecommendationEngine:", "226": "    \"\"\"", "227": "    Generate intelligent, actionable recommendations for test improvement.", "228": "", "229": "    Responsibilities:", "230": "    - Create priority-based action items (high/medium/low)", "231": "    - Generate structured reports with statistics", "232": "    - Provide specific remediation steps", "233": "", "234": "    Uses QualityAnalyzer internally for score calculation.", "235": "    \"\"\"", "236": "", "237": "    def __init__(self, finder: CoverageDuplicateFinder) -> None:", "238": "        \"\"\"Initialize recommendation engine.\"\"\"", "239": "        self.finder = finder", "240": "        self.quality_analyzer = QualityAnalyzer(finder)", "241": "", "242": "    def generate_report(self, threshold: float = 0.3) -> dict[str, Any]:", "243": "        \"\"\"", "244": "        Generate comprehensive recommendation report.", "245": "", "246": "        Args:", "247": "            threshold: Similarity threshold", "248": "", "249": "        Returns:", "250": "            Dictionary with score, recommendations, and action items", "251": "        \"\"\"", "252": "        logger.info(\"Generating recommendations report\")", "253": "", "254": "        score = self.quality_analyzer.calculate_score(threshold)", "255": "        exact_dups = self.finder.find_exact_duplicates()", "256": "        subset_dups = self.finder.find_subset_duplicates()", "257": "        similar = self.finder.find_similar_coverage(threshold)", "258": "        total_tests = len(self.finder.tests)", "259": "", "260": "        # Priority action items", "261": "        action_items = []", "262": "", "263": "        # High priority: exact duplicates", "264": "        if exact_dups:", "265": "            for i, group in enumerate(exact_dups[:5], 1):", "266": "                action_items.append(", "267": "                    {", "268": "                        \"priority\": \"high\",", "269": "                        \"type\": \"remove_duplicate\",", "270": "                        \"description\": f\"Remove duplicate tests in group {i}\",", "271": "                        \"message\": f\"Remove duplicate tests in group {i}\",", "272": "                        \"tests\": group,", "273": "                        \"impact\": f\"Reduce test count by {len(group) - 1}\",", "274": "                    }", "275": "                )", "276": "", "277": "        # Medium priority: subset duplicates", "278": "        if subset_dups:", "279": "            for subset_test, superset_test, ratio in subset_dups[:5]:", "280": "                action_items.append(", "281": "                    {", "282": "                        \"priority\": \"medium\",", "283": "                        \"type\": \"review_subset\",", "284": "                        \"description\": f\"Review if '{subset_test}' is needed\",", "285": "                        \"message\": f\"Review if '{subset_test}' is needed\",", "286": "                        \"tests\": [subset_test, superset_test],", "287": "                        \"details\": f\"{ratio:.1%} covered by superset\",", "288": "                    }", "289": "                )", "290": "", "291": "        # Low priority: similar tests", "292": "        if similar and len(similar) > total_tests // 2:", "293": "            action_items.append(", "294": "                {", "295": "                    \"priority\": \"low\",", "296": "                    \"type\": \"refactor_similar\",", "297": "                    \"description\": \"Consider refactoring similar test pairs\",", "298": "                    \"message\": \"Consider refactoring similar test pairs\",", "299": "                    \"count\": len(similar),", "300": "                    \"suggestion\": \"Use test parametrization or shared fixtures\",", "301": "                }", "302": "            )", "303": "", "304": "        return {", "305": "            \"quality_score\": {", "306": "                \"overall\": score.overall_score,", "307": "                \"duplication\": score.duplication_score,", "308": "                \"efficiency\": score.coverage_efficiency_score,", "309": "                \"uniqueness\": score.uniqueness_score,", "310": "                \"grade\": score.grade,", "311": "            },", "312": "            \"recommendations\": action_items,  # Priority-based action items", "313": "            \"suggestions\": score.recommendations,  # Text recommendations", "314": "            \"action_items\": action_items,  # Keep for backward compatibility", "315": "            \"statistics\": {", "316": "                \"total_tests\": len(self.finder.tests),", "317": "                \"exact_duplicates\": sum(len(g) - 1 for g in exact_dups),", "318": "                \"subset_duplicates\": len(subset_dups),", "319": "                \"similar_pairs\": len(similar),", "320": "            },", "321": "        }"}, "src/testiq/source_reader.py": {"1": "\"\"\"", "2": "Source code reader for TestIQ reports.", "3": "Reads actual source files to display in coverage comparisons.", "4": "\"\"\"", "5": "", "6": "from pathlib import Path", "7": "from typing import Optional", "8": "", "9": "", "10": "class SourceCodeReader:", "11": "    \"\"\"Read and cache source code files for display in reports.\"\"\"", "12": "", "13": "    def __init__(self) -> None:", "14": "        \"\"\"Initialize the source code reader.\"\"\"", "15": "        self._cache: dict[str, dict[int, str]] = {}", "16": "", "17": "    def read_file(self, filepath: str) -> Optional[dict[int, str]]:", "18": "        \"\"\"", "19": "        Read a source file and return line-by-line content.", "20": "", "21": "        Args:", "22": "            filepath: Path to the source file", "23": "", "24": "        Returns:", "25": "            Dictionary mapping line numbers (1-indexed) to source code lines,", "26": "            or None if file cannot be read", "27": "        \"\"\"", "28": "        if filepath in self._cache:", "29": "            return self._cache[filepath]", "30": "", "31": "        try:", "32": "            file_path = Path(filepath)", "33": "            if file_path.exists() and file_path.is_file():", "34": "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:", "35": "                    lines = f.readlines()", "36": "                    result = {", "37": "                        i + 1: line.rstrip() for i, line in enumerate(lines)", "38": "                    }", "39": "                    self._cache[filepath] = result", "40": "                    return result", "41": "        except Exception:", "42": "            pass", "43": "", "44": "        return None", "45": "", "46": "    def read_multiple(self, filepaths: list[str]) -> dict[str, dict[int, str]]:", "47": "        \"\"\"", "48": "        Read multiple source files.", "49": "", "50": "        Args:", "51": "            filepaths: List of file paths to read", "52": "", "53": "        Returns:", "54": "            Dictionary mapping filepath to line content dictionary", "55": "        \"\"\"", "56": "        result = {}", "57": "        for filepath in filepaths:", "58": "            content = self.read_file(filepath)", "59": "            if content:", "60": "                result[filepath] = content", "61": "        return result"}, "tests/test_cicd.py": {"1": "\"\"\"", "2": "Tests for CI/CD integration module (quality gates, baselines, trends).", "3": "\"\"\"", "4": "", "5": "import json", "6": "import tempfile", "7": "from datetime import datetime", "8": "from pathlib import Path", "9": "", "10": "import pytest", "11": "", "12": "from testiq.analyzer import CoverageDuplicateFinder", "13": "from testiq.cicd import (", "14": "    AnalysisResult,", "15": "    BaselineManager,", "16": "    QualityGate,", "17": "    QualityGateChecker,", "18": "    TrendTracker,", "19": "    get_exit_code,", "20": ")", "21": "", "22": "", "23": "@pytest.fixture", "24": "def sample_finder():", "25": "    \"\"\"Create a finder with sample test data.\"\"\"", "26": "    finder = CoverageDuplicateFinder()", "27": "", "28": "    # Add 10 tests with 2 exact duplicates", "29": "    for i in range(10):", "30": "        coverage = {\"file.py\": [1, 2, 3, i + 10]}  # Each test is unique", "31": "        finder.add_test_coverage(f\"test_{i}\", coverage)", "32": "", "33": "    # Add exact duplicates (overwrite test_0 and test_1)", "34": "    finder.add_test_coverage(\"test_dup_0\", {\"file.py\": [1, 2, 3, 100]})", "35": "    finder.add_test_coverage(\"test_dup_1\", {\"file.py\": [1, 2, 3, 100]})", "36": "", "37": "    return finder", "38": "", "39": "", "40": "@pytest.fixture", "41": "def sample_result():", "42": "    \"\"\"Create a sample analysis result.\"\"\"", "43": "    return AnalysisResult(", "44": "        timestamp=datetime.now().isoformat(),", "45": "        total_tests=100,", "46": "        exact_duplicates=10,", "47": "        duplicate_groups=5,", "48": "        subset_duplicates=8,", "49": "        similar_pairs=15,", "50": "        duplicate_percentage=10.0,", "51": "        threshold=0.9,", "52": "    )", "53": "", "54": "", "55": "class TestQualityGate:", "56": "    \"\"\"Tests for QualityGate.\"\"\"", "57": "", "58": "    def test_default_gate(self):", "59": "        \"\"\"Test default quality gate settings.\"\"\"", "60": "        gate = QualityGate()", "61": "        assert gate.max_duplicates is None", "62": "        assert gate.max_duplicate_percentage is None", "63": "        assert gate.fail_on_increase is True  # Default is True", "64": "", "65": "    def test_custom_gate(self):", "66": "        \"\"\"Test custom quality gate settings.\"\"\"", "67": "        gate = QualityGate(", "68": "            max_duplicates=5,", "69": "            max_duplicate_percentage=10.0,", "70": "            fail_on_increase=True,", "71": "        )", "72": "        assert gate.max_duplicates == 5", "73": "        assert gate.max_duplicate_percentage == pytest.approx(10.0)", "74": "        assert gate.fail_on_increase is True", "75": "", "76": "", "77": "class TestQualityGateChecker:", "78": "    \"\"\"Tests for QualityGateChecker.\"\"\"", "79": "", "80": "    def test_quality_gate_all_scenarios(self, sample_finder, sample_result):", "81": "        \"\"\"Test all quality gate pass/fail scenarios including baseline comparisons.\"\"\"", "82": "        # Test passes with no limits", "83": "        gate_no_limits = QualityGate()", "84": "        checker_no_limits = QualityGateChecker(gate_no_limits)", "85": "        passed, details = checker_no_limits.check(sample_finder, 0.9)", "86": "        assert passed is True", "87": "        assert details[\"passed\"] is True", "88": "        assert len(details[\"failures\"]) == 0", "89": "", "90": "        # Test fails when exceeding max duplicates", "91": "        gate_fail_max = QualityGate(max_duplicates=0)", "92": "        checker_fail_max = QualityGateChecker(gate_fail_max)", "93": "        passed, details = checker_fail_max.check(sample_finder, 0.9)", "94": "        assert passed is False", "95": "        assert details[\"passed\"] is False", "96": "        assert len(details[\"failures\"]) > 0", "97": "        assert any(\"exact duplicates\" in f.lower() for f in details[\"failures\"])", "98": "", "99": "        # Test passes when under max duplicates", "100": "        gate_pass_max = QualityGate(max_duplicates=10)", "101": "        checker_pass_max = QualityGateChecker(gate_pass_max)", "102": "        passed, _ = checker_pass_max.check(sample_finder, 0.9)", "103": "        assert passed is True", "104": "", "105": "        # Test fails when exceeding max percentage", "106": "        gate_fail_pct = QualityGate(max_duplicate_percentage=5.0)", "107": "        checker_fail_pct = QualityGateChecker(gate_fail_pct)", "108": "        passed, _ = checker_fail_pct.check(sample_finder, 0.9)", "109": "        assert passed is False", "110": "", "111": "        # Test fails on increase from baseline", "112": "        gate_increase = QualityGate(fail_on_increase=True)", "113": "        checker_increase = QualityGateChecker(gate_increase)", "114": "        baseline = sample_result", "115": "        baseline.exact_duplicates = 0", "116": "        baseline.subset_duplicates = 0", "117": "        passed, details = checker_increase.check(sample_finder, 0.9, baseline)", "118": "        assert passed is False", "119": "        assert any(\"increased\" in f.lower() for f in details[\"failures\"])", "120": "", "121": "", "122": "class TestBaselineManager:", "123": "    \"\"\"Tests for BaselineManager.\"\"\"", "124": "", "125": "    def test_save_and_load_baseline(self, sample_result):", "126": "        \"\"\"Test saving and loading a baseline.\"\"\"", "127": "        with tempfile.TemporaryDirectory() as tmpdir:", "128": "            manager = BaselineManager(Path(tmpdir))", "129": "", "130": "            # Save baseline", "131": "            manager.save(sample_result, \"test_baseline\")", "132": "", "133": "            # Load baseline", "134": "            loaded = manager.load(\"test_baseline\")", "135": "", "136": "            assert loaded is not None", "137": "            assert loaded.total_tests == sample_result.total_tests", "138": "            assert loaded.exact_duplicates == sample_result.exact_duplicates", "139": "            assert loaded.duplicate_percentage == sample_result.duplicate_percentage", "140": "", "141": "    def test_load_nonexistent_baseline(self):", "142": "        \"\"\"Test loading a baseline that doesn't exist.\"\"\"", "143": "        with tempfile.TemporaryDirectory() as tmpdir:", "144": "            manager = BaselineManager(Path(tmpdir))", "145": "", "146": "            loaded = manager.load(\"nonexistent\")", "147": "            assert loaded is None", "148": "", "149": "    def test_list_baselines(self, sample_result):", "150": "        \"\"\"Test listing all baselines.\"\"\"", "151": "        with tempfile.TemporaryDirectory() as tmpdir:", "152": "            manager = BaselineManager(Path(tmpdir))", "153": "", "154": "            # Save multiple baselines", "155": "            manager.save(sample_result, \"baseline1\")", "156": "            manager.save(sample_result, \"baseline2\")", "157": "", "158": "            # List baselines", "159": "            baselines = manager.list_baselines()", "160": "", "161": "            assert len(baselines) == 2", "162": "            baseline_names = [b[\"name\"] for b in baselines]", "163": "            assert \"baseline1\" in baseline_names", "164": "            assert \"baseline2\" in baseline_names", "165": "            # Verify structure includes result objects", "166": "            assert all(\"result\" in b for b in baselines)", "167": "            assert all(isinstance(b[\"result\"], AnalysisResult) for b in baselines)", "168": "", "169": "    def test_baseline_file_format(self, sample_result):", "170": "        \"\"\"Test that baseline is saved in correct JSON format.\"\"\"", "171": "        with tempfile.TemporaryDirectory() as tmpdir:", "172": "            manager = BaselineManager(Path(tmpdir))", "173": "", "174": "            manager.save(sample_result, \"test\")", "175": "", "176": "            # Read raw JSON file", "177": "            baseline_file = Path(tmpdir) / \"test.json\"", "178": "            with open(baseline_file) as f:", "179": "                data = json.load(f)", "180": "", "181": "            assert \"timestamp\" in data", "182": "            assert \"total_tests\" in data", "183": "            assert \"exact_duplicates\" in data", "184": "", "185": "    def test_creates_baseline_directory(self):", "186": "        \"\"\"Test that baseline directory is created if it doesn't exist.\"\"\"", "187": "        with tempfile.TemporaryDirectory() as tmpdir:", "188": "            baseline_dir = Path(tmpdir) / \"new_dir\"", "189": "            assert not baseline_dir.exists()", "190": "", "191": "            _ = BaselineManager(baseline_dir)", "192": "            assert baseline_dir.exists()", "193": "", "194": "", "195": "class TestTrendTracker:", "196": "    \"\"\"Tests for TrendTracker.\"\"\"", "197": "", "198": "    def test_add_and_get_history(self, sample_result):", "199": "        \"\"\"Test adding results and getting history.\"\"\"", "200": "        with tempfile.TemporaryDirectory() as tmpdir:", "201": "            history_file = Path(tmpdir) / \"history.json\"", "202": "            tracker = TrendTracker(history_file)", "203": "", "204": "            # Add multiple results", "205": "            tracker.add_result(sample_result)", "206": "", "207": "            result2 = AnalysisResult(", "208": "                timestamp=datetime.now().isoformat(),", "209": "                total_tests=110,", "210": "                exact_duplicates=8,", "211": "                duplicate_groups=4,", "212": "                subset_duplicates=6,", "213": "                similar_pairs=12,", "214": "                duplicate_percentage=7.3,", "215": "                threshold=0.9,", "216": "            )", "217": "            tracker.add_result(result2)", "218": "", "219": "            # Get history", "220": "            history = tracker.load_history()", "221": "", "222": "            assert len(history) == 2", "223": "            assert history[0][\"total_tests\"] == 100", "224": "            assert history[1][\"total_tests\"] == 110", "225": "", "226": "    def test_calculate_trend_improving(self):", "227": "        \"\"\"Test trend calculation shows improvement.\"\"\"", "228": "        with tempfile.TemporaryDirectory() as tmpdir:", "229": "            history_file = Path(tmpdir) / \"history.json\"", "230": "            tracker = TrendTracker(history_file)", "231": "", "232": "            # Add results showing improvement (fewer duplicates)", "233": "            result1 = AnalysisResult(", "234": "                timestamp=datetime.now().isoformat(),", "235": "                total_tests=100,", "236": "                exact_duplicates=20,", "237": "                duplicate_groups=10,", "238": "                subset_duplicates=15,", "239": "                similar_pairs=25,", "240": "                duplicate_percentage=20.0,", "241": "                threshold=0.9,", "242": "            )", "243": "", "244": "            result2 = AnalysisResult(", "245": "                timestamp=datetime.now().isoformat(),", "246": "                total_tests=100,", "247": "                exact_duplicates=10,", "248": "                duplicate_groups=5,", "249": "                subset_duplicates=8,", "250": "                similar_pairs=15,", "251": "                duplicate_percentage=10.0,", "252": "                threshold=0.9,", "253": "            )", "254": "", "255": "            tracker.add_result(result1)", "256": "            tracker.add_result(result2)", "257": "", "258": "            # Check if improving", "259": "            assert tracker.is_improving(\"exact_duplicates\") is True", "260": "", "261": "    def test_calculate_trend_worsening(self):", "262": "        \"\"\"Test trend calculation shows worsening.\"\"\"", "263": "        with tempfile.TemporaryDirectory() as tmpdir:", "264": "            history_file = Path(tmpdir) / \"history.json\"", "265": "            tracker = TrendTracker(history_file)", "266": "", "267": "            # Add results showing worsening (more duplicates)", "268": "            result1 = AnalysisResult(", "269": "                timestamp=datetime.now().isoformat(),", "270": "                total_tests=100,", "271": "                exact_duplicates=5,", "272": "                duplicate_groups=3,", "273": "                subset_duplicates=4,", "274": "                similar_pairs=8,", "275": "                duplicate_percentage=5.0,", "276": "                threshold=0.9,", "277": "            )", "278": "", "279": "            result2 = AnalysisResult(", "280": "                timestamp=datetime.now().isoformat(),", "281": "                total_tests=100,", "282": "                exact_duplicates=15,", "283": "                duplicate_groups=8,", "284": "                subset_duplicates=12,", "285": "                similar_pairs=20,", "286": "                duplicate_percentage=15.0,", "287": "                threshold=0.9,", "288": "            )", "289": "", "290": "            tracker.add_result(result1)", "291": "            tracker.add_result(result2)", "292": "", "293": "            # Check if worsening (not improving)", "294": "            assert tracker.is_improving(\"exact_duplicates\") is False", "295": "", "296": "    def test_trend_with_insufficient_data(self):", "297": "        \"\"\"Test trend calculation with insufficient data.\"\"\"", "298": "        with tempfile.TemporaryDirectory() as tmpdir:", "299": "            history_file = Path(tmpdir) / \"history.json\"", "300": "            tracker = TrendTracker(history_file)", "301": "", "302": "            # Empty history should return True (improving)", "303": "            assert tracker.is_improving() is True", "304": "", "305": "", "306": "class TestGetExitCode:", "307": "    \"\"\"Tests for get_exit_code helper function.\"\"\"", "308": "", "309": "    def test_exit_code_success(self):", "310": "        \"\"\"Test exit code for successful run with no duplicates.\"\"\"", "311": "        # Success: no duplicates, gate passed", "312": "        assert get_exit_code(passed=True, duplicate_count=0, _total_tests=10) == 0", "313": "", "314": "    def test_exit_code_duplicates_found(self):", "315": "        \"\"\"Test exit code when duplicates are found.\"\"\"", "316": "        # Duplicates found but gate passed", "317": "        assert get_exit_code(passed=True, duplicate_count=5, _total_tests=10) == 1", "318": "", "319": "    def test_exit_code_gate_failed(self):", "320": "        \"\"\"Test exit code when quality gate fails.\"\"\"", "321": "        # Gate failed", "322": "        assert get_exit_code(passed=False, duplicate_count=10, _total_tests=10) == 2", "323": "", "324": "    def test_exit_code_gate_failed_priority(self):", "325": "        \"\"\"Test that gate failure takes priority over duplicates.\"\"\"", "326": "        # Gate failure (2) should override duplicates found (1)", "327": "        assert get_exit_code(passed=False, duplicate_count=5, _total_tests=10) == 2", "328": "        assert get_exit_code(passed=False, duplicate_count=0, _total_tests=10) == 2"}, "src/testiq/pytest_plugin.py": {"1": "\"\"\"", "2": "Pytest plugin for generating per-test coverage data compatible with TestIQ.", "3": "", "4": "This plugin tracks which lines each test executes and generates a JSON file", "5": "in the format TestIQ expects: {test_name: {filename: [line_numbers]}}", "6": "", "7": "Installation:", "8": "    pip install pytest-cov", "9": "", "10": "Usage:", "11": "    pytest --testiq-output=testiq_coverage.json", "12": "", "13": "Or in pytest.ini:", "14": "    [pytest]", "15": "    addopts = --testiq-output=testiq_coverage.json", "16": "\"\"\"", "17": "", "18": "import json", "19": "import sys", "20": "from pathlib import Path", "21": "from typing import Any, Dict, List, Set", "22": "", "23": "import pytest", "24": "from _pytest.config import Config", "25": "from _pytest.config.argparsing import Parser", "26": "from _pytest.nodes import Item", "27": "", "28": "", "29": "class TestIQPlugin:", "30": "    \"\"\"Pytest plugin to collect per-test coverage data for TestIQ.\"\"\"", "31": "", "32": "    def __init__(self, output_file: str) -> None:", "33": "        \"\"\"Initialize the plugin.\"\"\"", "34": "        self.output_file = output_file", "35": "        self.test_coverage: Dict[str, Dict[str, List[int]]] = {}", "36": "        self.current_test: str = \"\"", "37": "        self.traced_lines: Set[tuple[str, int]] = set()", "38": "        self.file_cache: Dict[str, Dict[int, str]] = {}  # Cache file contents", "39": "        self.docstring_lines_cache: Dict[str, Set[int]] = {}  # Cache docstring lines", "40": "", "41": "    def pytest_runtest_protocol(self, item: Item) -> None:", "42": "        \"\"\"Called for each test item.\"\"\"", "43": "        # Get full test name (module::class::test)", "44": "        self.current_test = item.nodeid", "45": "        self.traced_lines = set()", "46": "", "47": "        # Set up trace function for this test", "48": "        sys.settrace(self._trace_lines)", "49": "", "50": "    def _trace_lines(self, frame: Any, event: str, arg: Any) -> Any:", "51": "        \"\"\"Trace function to record line execution.\"\"\"", "52": "        if event == \"line\":", "53": "            filename = frame.f_code.co_filename", "54": "            lineno = frame.f_lineno", "55": "", "56": "            # Filter to only project files (not libraries)", "57": "            if self._is_project_file(filename):", "58": "                # Skip if this line is part of a docstring", "59": "                if not self._is_docstring_line(filename, lineno):", "60": "                    self.traced_lines.add((filename, lineno))", "61": "", "62": "        return self._trace_lines", "63": "", "64": "    def _is_project_file(self, filename: str) -> bool:", "65": "        \"\"\"Check if file is part of the project (not a library).\"\"\"", "66": "        # Exclude standard library and site-packages", "67": "        if \"/site-packages/\" in filename or \"/lib/python\" in filename:", "68": "            return False", "69": "        if filename.startswith(\"<\"):  # <string>, <stdin>, etc.", "70": "            return False", "71": "", "72": "        # Include files in current working directory", "73": "        try:", "74": "            Path(filename).relative_to(Path.cwd())", "75": "            return True", "76": "        except ValueError:", "77": "            return False", "78": "", "79": "    def _is_docstring_line(self, filename: str, lineno: int) -> bool:", "80": "        \"\"\"Check if a line is part of a docstring.\"\"\"", "81": "        # Use cached result if available", "82": "        if filename in self.docstring_lines_cache:", "83": "            return lineno in self.docstring_lines_cache[filename]", "84": "", "85": "        # Read and cache the file if not already cached", "86": "        if filename not in self.file_cache:", "87": "            try:", "88": "                with open(filename, 'r', encoding='utf-8') as f:", "89": "                    lines = f.readlines()", "90": "                    self.file_cache[filename] = {i + 1: line for i, line in enumerate(lines)}", "91": "            except Exception:", "92": "                # If we can't read the file, assume no docstrings", "93": "                self.docstring_lines_cache[filename] = set()", "94": "                return False", "95": "", "96": "        # Find all docstring lines in this file", "97": "        docstring_lines = set()", "98": "        file_lines = self.file_cache[filename]", "99": "        in_docstring = False", "100": "        docstring_delimiter = ''", "101": "", "102": "        for line_num in sorted(file_lines.keys()):", "103": "            line = file_lines[line_num]", "104": "            trimmed = line.strip()", "105": "", "106": "            # Check for docstring delimiters", "107": "            if '\"\"\"' in trimmed or \"'''\" in trimmed:", "108": "                if '\"\"\"' in trimmed:", "109": "                    delimiter = '\"\"\"'", "110": "                else:", "111": "                    delimiter = \"'''\"", "112": "", "113": "                if not in_docstring:", "114": "                    # Starting a docstring", "115": "                    in_docstring = True", "116": "                    docstring_delimiter = delimiter", "117": "                    docstring_lines.add(line_num)", "118": "", "119": "                    # Check if it's a single-line docstring", "120": "                    first_idx = trimmed.find(delimiter)", "121": "                    after_first = trimmed[first_idx + 3:]", "122": "                    if delimiter in after_first:", "123": "                        # Single-line docstring", "124": "                        in_docstring = False", "125": "                        docstring_delimiter = ''", "126": "                elif in_docstring and delimiter == docstring_delimiter:", "127": "                    # Ending a docstring", "128": "                    docstring_lines.add(line_num)", "129": "                    in_docstring = False", "130": "                    docstring_delimiter = ''", "131": "            elif in_docstring:", "132": "                # Inside a docstring", "133": "                docstring_lines.add(line_num)", "134": "", "135": "        self.docstring_lines_cache[filename] = docstring_lines", "136": "        return lineno in docstring_lines", "137": "", "138": "    def pytest_runtest_teardown(self, item: Item) -> None:", "139": "        \"\"\"Called after each test finishes.\"\"\"", "140": "        # Stop tracing", "141": "        sys.settrace(None)", "142": "", "143": "        # Convert traced lines to TestIQ format", "144": "        if self.current_test and self.traced_lines:", "145": "            coverage: Dict[str, List[int]] = {}", "146": "", "147": "            for filename, lineno in self.traced_lines:", "148": "                # Make path relative to project root", "149": "                try:", "150": "                    rel_path = str(Path(filename).relative_to(Path.cwd()))", "151": "                except ValueError:", "152": "                    rel_path = filename", "153": "", "154": "                if rel_path not in coverage:", "155": "                    coverage[rel_path] = []", "156": "                coverage[rel_path].append(lineno)", "157": "", "158": "            # Add function/class definition lines for better context", "159": "            self._add_definition_lines(coverage)", "160": "", "161": "            # Sort line numbers and remove duplicates", "162": "            for file_path in coverage:", "163": "                coverage[file_path] = sorted(set(coverage[file_path]))", "164": "", "165": "            self.test_coverage[self.current_test] = coverage", "166": "", "167": "    def _add_definition_lines(self, coverage: Dict[str, List[int]]) -> None:", "168": "        \"\"\"", "169": "        Add function/class definition lines to coverage.", "170": "", "171": "        If a function body is executed, include the def line.", "172": "        If a class method is executed, include the class line.", "173": "        \"\"\"", "174": "        for file_path, lines in coverage.items():", "175": "            if not lines:", "176": "                continue", "177": "", "178": "            # Get file contents", "179": "            try:", "180": "                abs_path = str(Path.cwd() / file_path)", "181": "                if abs_path not in self.file_cache:", "182": "                    with open(abs_path, 'r', encoding='utf-8') as f:", "183": "                        file_lines = f.readlines()", "184": "                        self.file_cache[abs_path] = {i + 1: line for i, line in enumerate(file_lines)}", "185": "", "186": "                file_content = self.file_cache[abs_path]", "187": "            except Exception:", "188": "                continue", "189": "", "190": "            # Find definition lines to add", "191": "            definition_lines = set()", "192": "", "193": "            for line_num in lines:", "194": "                # Look backwards from this line to find the nearest def/class", "195": "                for check_line in range(line_num - 1, 0, -1):", "196": "                    if check_line not in file_content:", "197": "                        break", "198": "", "199": "                    line_text = file_content[check_line].strip()", "200": "", "201": "                    # Found a function or class definition", "202": "                    if line_text.startswith('def ') or line_text.startswith('class '):", "203": "                        # Check indentation - make sure we're in the same scope", "204": "                        if check_line < line_num:", "205": "                            # Add this definition line", "206": "                            definition_lines.add(check_line)", "207": "                            # Only add the immediate parent definition", "208": "                            break", "209": "", "210": "                    # Stop if we hit another statement at the same or lower indentation", "211": "                    if line_text and not line_text.startswith('#'):", "212": "                        # Check if this is a decorator", "213": "                        if line_text.startswith('@'):", "214": "                            continue", "215": "                        # If we hit something else, stop looking", "216": "                        if check_line < line_num - 50:  # Don't search too far", "217": "                            break", "218": "", "219": "            # Add the definition lines to coverage", "220": "            coverage[file_path].extend(definition_lines)", "221": "", "222": "    def pytest_sessionfinish(self, session: Any) -> None:", "223": "        \"\"\"Called after all tests complete.\"\"\"", "224": "        if self.test_coverage:", "225": "            output_path = Path(self.output_file)", "226": "            output_path.parent.mkdir(parents=True, exist_ok=True)", "227": "", "228": "            with open(output_path, \"w\") as f:", "229": "                json.dump(self.test_coverage, f, indent=2)", "230": "", "231": "            print(f\"\\n\u2713 TestIQ coverage data saved to: {output_path}\")", "232": "            print(f\"  {len(self.test_coverage)} tests tracked\")", "233": "", "234": "", "235": "def pytest_addoption(parser: Parser) -> None:", "236": "    \"\"\"Add command-line options for TestIQ plugin.\"\"\"", "237": "    group = parser.getgroup(\"testiq\")", "238": "    group.addoption(", "239": "        \"--testiq-output\",", "240": "        action=\"store\",", "241": "        default=None,", "242": "        help=\"Output file for TestIQ per-test coverage data (JSON format)\",", "243": "    )", "244": "", "245": "", "246": "def pytest_configure(config: Config) -> None:", "247": "    \"\"\"Register the TestIQ plugin if --testiq-output is specified.\"\"\"", "248": "    output_file = config.getoption(\"--testiq-output\")", "249": "    if output_file:", "250": "        plugin = TestIQPlugin(output_file)", "251": "        config.pluginmanager.register(plugin, \"testiq_plugin\")", "252": "        config.addinivalue_line(\"markers\", \"testiq: mark test for TestIQ analysis\")"}, "tests/test_enterprise.py": {"1": "\"\"\"Tests for enterprise features: security, logging, config, exceptions.\"\"\"", "2": "", "3": "from pathlib import Path", "4": "", "5": "import pytest", "6": "", "7": "from testiq.config import Config, load_config, load_config_from_env", "8": "from testiq.exceptions import (", "9": "    AnalysisError,", "10": "    ConfigurationError,", "11": "    SecurityError,", "12": "    ValidationError,", "13": ")", "14": "from testiq.security import (", "15": "    check_file_size,", "16": "    sanitize_output_path,", "17": "    validate_coverage_data,", "18": "    validate_file_path,", "19": ")", "20": "", "21": "", "22": "class TestExceptions:", "23": "    \"\"\"Test custom exceptions.\"\"\"", "24": "", "25": "    def test_testiq_error(self):", "26": "        \"\"\"Test base TestIQ error.\"\"\"", "27": "        error = ValidationError(\"Test message\")", "28": "        assert \"VALIDATION_ERROR\" in str(error)", "29": "        assert \"Test message\" in str(error)", "30": "", "31": "    def test_error_codes(self):", "32": "        \"\"\"Test different error codes.\"\"\"", "33": "        errors = [", "34": "            (ConfigurationError(\"test\"), \"CONFIG_ERROR\"),", "35": "            (ValidationError(\"test\"), \"VALIDATION_ERROR\"),", "36": "            (SecurityError(\"test\"), \"SECURITY_ERROR\"),", "37": "            (AnalysisError(\"test\"), \"ANALYSIS_ERROR\"),", "38": "        ]", "39": "", "40": "        for error, expected_code in errors:", "41": "            assert error.error_code == expected_code", "42": "            assert expected_code in str(error)", "43": "", "44": "", "45": "class TestSecurity:", "46": "    \"\"\"Test security features integration.\"\"\"", "47": "", "48": "    def test_validate_coverage_data_invalid_structure(self):", "49": "        \"\"\"Test rejecting invalid data structures (integration check).\"\"\"", "50": "        with pytest.raises(ValidationError, match=\"must be a dictionary\"):", "51": "            validate_coverage_data([])", "52": "", "53": "        with pytest.raises(ValidationError, match=\"must be a dictionary\"):", "54": "            validate_coverage_data({\"test1\": []})", "55": "", "56": "        with pytest.raises(ValidationError, match=\"must be a list\"):", "57": "            validate_coverage_data({\"test1\": {\"file.py\": \"not a list\"}})", "58": "", "59": "", "60": "class TestConfig:", "61": "    \"\"\"Test configuration management.\"\"\"", "62": "", "63": "    def test_default_config(self):", "64": "        \"\"\"Test default configuration values.\"\"\"", "65": "        config = Config()", "66": "", "67": "        assert config.log.level == \"INFO\"", "68": "        assert config.security.max_file_size == 100 * 1024 * 1024", "69": "        assert config.performance.enable_parallel is True", "70": "        assert config.analysis.similarity_threshold == 0.3", "71": "", "72": "    def test_config_from_dict(self):", "73": "        \"\"\"Test creating config from dictionary.\"\"\"", "74": "        data = {", "75": "            \"log\": {\"level\": \"DEBUG\"},", "76": "            \"security\": {\"max_tests\": 1000},", "77": "            \"performance\": {\"max_workers\": 8},", "78": "            \"analysis\": {\"similarity_threshold\": 0.8},", "79": "        }", "80": "", "81": "        config = Config.from_dict(data)", "82": "", "83": "        assert config.log.level == \"DEBUG\"", "84": "        assert config.security.max_tests == 1000", "85": "        assert config.performance.max_workers == 8", "86": "        assert config.analysis.similarity_threshold == 0.8", "87": "", "88": "    def test_config_to_dict(self):", "89": "        \"\"\"Test converting config to dictionary.\"\"\"", "90": "        config = Config()", "91": "        config.log.level = \"DEBUG\"", "92": "", "93": "        data = config.to_dict()", "94": "", "95": "        assert data[\"log\"][\"level\"] == \"DEBUG\"", "96": "        assert \"security\" in data", "97": "        assert \"performance\" in data", "98": "        assert \"analysis\" in data", "99": "", "100": "    def test_load_config_yaml(self, tmp_path):", "101": "        \"\"\"Test loading YAML config file.\"\"\"", "102": "        config_file = tmp_path / \".testiq.yaml\"", "103": "        config_file.write_text(", "104": "            \"\"\"", "105": "log:", "106": "  level: DEBUG", "107": "security:", "108": "  max_tests: 5000", "109": "\"\"\"", "110": "        )", "111": "", "112": "        config = load_config(config_file)", "113": "", "114": "        assert config.log.level == \"DEBUG\"", "115": "        assert config.security.max_tests == 5000", "116": "", "117": "    def test_load_config_invalid_file(self):", "118": "        \"\"\"Test loading non-existent config file.\"\"\"", "119": "        with pytest.raises(ConfigurationError):", "120": "            from testiq.config import load_config_file", "121": "", "122": "            load_config_file(Path(\"/nonexistent/config.yaml\"))", "123": "", "124": "    def test_load_config_from_env(self, monkeypatch):", "125": "        \"\"\"Test loading config from environment variables.\"\"\"", "126": "        monkeypatch.setenv(\"TESTIQ_LOG_LEVEL\", \"ERROR\")", "127": "        monkeypatch.setenv(\"TESTIQ_MAX_TESTS\", \"10000\")", "128": "        monkeypatch.setenv(\"TESTIQ_ENABLE_PARALLEL\", \"false\")", "129": "", "130": "        env_config = load_config_from_env()", "131": "", "132": "        assert env_config[\"log\"][\"level\"] == \"ERROR\"", "133": "        assert env_config[\"security\"][\"max_tests\"] == 10000", "134": "        assert env_config[\"performance\"][\"enable_parallel\"] is False", "135": "", "136": "", "137": "class TestPerformance:", "138": "    \"\"\"Test performance features integration.\"\"\"", "139": "", "140": "    def test_compute_similarity(self):", "141": "        \"\"\"Test cached similarity computation.\"\"\"", "142": "        from testiq.performance import compute_similarity", "143": "", "144": "        lines1 = frozenset([(\"file.py\", 1), (\"file.py\", 2), (\"file.py\", 3)])", "145": "        lines2 = frozenset([(\"file.py\", 2), (\"file.py\", 3), (\"file.py\", 4)])", "146": "", "147": "        similarity = compute_similarity(lines1, lines2)", "148": "", "149": "        # Jaccard: intersection=2, union=4, similarity=0.5", "150": "        assert similarity == 0.5", "151": "", "152": "", "153": "class TestLogging:", "154": "    \"\"\"Test logging configuration.\"\"\"", "155": "", "156": "    def test_setup_logging(self):", "157": "        \"\"\"Test setting up logging.\"\"\"", "158": "        from testiq.logging_config import setup_logging", "159": "", "160": "        logger = setup_logging(level=\"DEBUG\")", "161": "", "162": "        assert logger.name == \"testiq\"", "163": "        assert logger.level == 10  # DEBUG", "164": "", "165": "    def test_setup_logging_with_file(self, tmp_path):", "166": "        \"\"\"Test setting up logging with file.\"\"\"", "167": "        from testiq.logging_config import setup_logging", "168": "", "169": "        log_file = tmp_path / \"test.log\"", "170": "        logger = setup_logging(level=\"INFO\", log_file=log_file)", "171": "", "172": "        logger.info(\"Test message\")", "173": "", "174": "        assert log_file.exists()", "175": "        assert \"Test message\" in log_file.read_text()", "176": "", "177": "    def test_get_logger(self):", "178": "        \"\"\"Test getting logger instance.\"\"\"", "179": "        from testiq.logging_config import get_logger", "180": "", "181": "        logger = get_logger(\"testiq.test\")", "182": "", "183": "        assert logger.name == \"testiq.test\""}, "tests/test_coverage_converter.py": {"1": "\"\"\"Tests for coverage converter module.\"\"\"", "2": "", "3": "import json", "4": "import tempfile", "5": "from pathlib import Path", "6": "", "7": "import pytest", "8": "", "9": "from testiq.coverage_converter import (", "10": "    convert_pytest_contexts,", "11": "    convert_pytest_coverage,", "12": ")", "13": "", "14": "", "15": "class TestConvertPytestCoverage:", "16": "    \"\"\"Tests for convert_pytest_coverage function.\"\"\"", "17": "", "18": "    def test_basic_conversion(self):", "19": "        \"\"\"Test basic conversion of pytest coverage format.\"\"\"", "20": "        coverage_data = {", "21": "            \"files\": {", "22": "                \"src/module.py\": {", "23": "                    \"executed_lines\": [1, 2, 3, 5, 10]", "24": "                },", "25": "                \"src/other.py\": {", "26": "                    \"executed_lines\": [1, 4, 7]", "27": "                }", "28": "            }", "29": "        }", "30": "", "31": "        result = convert_pytest_coverage(coverage_data)", "32": "", "33": "        assert \"all_tests_aggregated\" in result", "34": "        assert len(result[\"all_tests_aggregated\"]) == 2", "35": "        assert sorted(result[\"all_tests_aggregated\"][\"src/module.py\"]) == [1, 2, 3, 5, 10]", "36": "        assert sorted(result[\"all_tests_aggregated\"][\"src/other.py\"]) == [1, 4, 7]", "37": "", "38": "    def test_missing_files_key(self):", "39": "        \"\"\"Test error handling for missing 'files' key.\"\"\"", "40": "        coverage_data = {}", "41": "", "42": "        with pytest.raises(ValueError, match=\"missing 'files' key\"):", "43": "            convert_pytest_coverage(coverage_data)", "44": "", "45": "    def test_missing_executed_lines(self):", "46": "        \"\"\"Test handling of files without executed_lines.\"\"\"", "47": "        coverage_data = {", "48": "            \"files\": {", "49": "                \"src/module.py\": {", "50": "                    \"something_else\": [1, 2, 3]", "51": "                }", "52": "            }", "53": "        }", "54": "", "55": "        result = convert_pytest_coverage(coverage_data)", "56": "        assert result == {}", "57": "", "58": "    def test_invalid_executed_lines_type(self):", "59": "        \"\"\"Test handling of non-list executed_lines.\"\"\"", "60": "        coverage_data = {", "61": "            \"files\": {", "62": "                \"src/module.py\": {", "63": "                    \"executed_lines\": \"not a list\"", "64": "                }", "65": "            }", "66": "        }", "67": "", "68": "        result = convert_pytest_coverage(coverage_data)", "69": "        assert result == {}", "70": "", "71": "    def test_empty_coverage(self):", "72": "        \"\"\"Test handling of empty coverage data.\"\"\"", "73": "        coverage_data = {\"files\": {}}", "74": "", "75": "        result = convert_pytest_coverage(coverage_data)", "76": "        assert result == {}", "77": "", "78": "    def test_line_sorting(self):", "79": "        \"\"\"Test that lines are sorted in output.\"\"\"", "80": "        coverage_data = {", "81": "            \"files\": {", "82": "                \"src/module.py\": {", "83": "                    \"executed_lines\": [10, 5, 1, 3, 2]", "84": "                }", "85": "            }", "86": "        }", "87": "", "88": "        result = convert_pytest_coverage(coverage_data)", "89": "        assert result[\"all_tests_aggregated\"][\"src/module.py\"] == [1, 2, 3, 5, 10]", "90": "", "91": "", "92": "class TestConvertPytestContexts:", "93": "    \"\"\"Tests for convert_pytest_contexts function.\"\"\"", "94": "", "95": "    def test_with_contexts(self):", "96": "        \"\"\"Test conversion with test contexts available.\"\"\"", "97": "        coverage_data = {", "98": "            \"meta\": {", "99": "                \"show_contexts\": True", "100": "            },", "101": "            \"files\": {", "102": "                \"src/module.py\": {", "103": "                    \"contexts\": {", "104": "                        \"test_foo\": [1, 2, 3],", "105": "                        \"test_bar\": [4, 5, 6]", "106": "                    }", "107": "                }", "108": "            }", "109": "        }", "110": "", "111": "        result = convert_pytest_contexts(coverage_data)", "112": "", "113": "        assert \"test_foo\" in result", "114": "        assert \"test_bar\" in result", "115": "        assert result[\"test_foo\"][\"src/module.py\"] == [1, 2, 3]", "116": "        assert result[\"test_bar\"][\"src/module.py\"] == [4, 5, 6]", "117": "", "118": "    def test_without_contexts(self):", "119": "        \"\"\"Test fallback to aggregated format when no contexts.\"\"\"", "120": "        coverage_data = {", "121": "            \"meta\": {", "122": "                \"show_contexts\": False", "123": "            },", "124": "            \"files\": {", "125": "                \"src/module.py\": {", "126": "                    \"executed_lines\": [1, 2, 3]", "127": "                }", "128": "            }", "129": "        }", "130": "", "131": "        result = convert_pytest_contexts(coverage_data)", "132": "", "133": "        # Should fall back to aggregated format", "134": "        assert \"all_tests_aggregated\" in result", "135": "", "136": "    def test_empty_context_name(self):", "137": "        \"\"\"Test handling of empty context names.\"\"\"", "138": "        coverage_data = {", "139": "            \"meta\": {", "140": "                \"show_contexts\": True", "141": "            },", "142": "            \"files\": {", "143": "                \"src/module.py\": {", "144": "                    \"contexts\": {", "145": "                        \"\": [1, 2, 3],", "146": "                        \"test_foo\": [4, 5]", "147": "                    }", "148": "                }", "149": "            }", "150": "        }", "151": "", "152": "        result = convert_pytest_contexts(coverage_data)", "153": "", "154": "        # Empty context should be skipped", "155": "        assert \"\" not in result", "156": "        assert \"test_foo\" in result", "157": "", "158": "    def test_no_contexts_field(self):", "159": "        \"\"\"Test fallback when contexts field is missing.\"\"\"", "160": "        coverage_data = {", "161": "            \"meta\": {", "162": "                \"show_contexts\": True", "163": "            },", "164": "            \"files\": {", "165": "                \"src/module.py\": {", "166": "                    \"executed_lines\": [1, 2, 3]", "167": "                }", "168": "            }", "169": "        }", "170": "", "171": "        result = convert_pytest_contexts(coverage_data)", "172": "", "173": "        # Should fall back to aggregated format", "174": "        assert \"all_tests_aggregated\" in result", "175": "", "176": "", "177": "class TestCLI:", "178": "    \"\"\"Tests for coverage_converter CLI.\"\"\"", "179": "", "180": "    def test_basic_conversion_cli(self, tmp_path):", "181": "        \"\"\"Test CLI basic conversion.\"\"\"", "182": "        # Create test coverage file", "183": "        coverage_file = tmp_path / \"coverage.json\"", "184": "        coverage_data = {", "185": "            \"files\": {", "186": "                \"src/module.py\": {", "187": "                    \"executed_lines\": [1, 2, 3]", "188": "                }", "189": "            }", "190": "        }", "191": "        coverage_file.write_text(json.dumps(coverage_data))", "192": "", "193": "        # Import and run CLI", "194": "        from testiq.coverage_converter import main", "195": "        from click.testing import CliRunner", "196": "", "197": "        runner = CliRunner()", "198": "        output_file = tmp_path / \"output.json\"", "199": "        result = runner.invoke(main, [str(coverage_file), \"-o\", str(output_file)])", "200": "", "201": "        assert result.exit_code == 0", "202": "        assert output_file.exists()", "203": "", "204": "        # Verify output", "205": "        output_data = json.loads(output_file.read_text())", "206": "        assert \"all_tests_aggregated\" in output_data", "207": "", "208": "    def test_with_contexts_flag(self, tmp_path):", "209": "        \"\"\"Test CLI with --with-contexts flag.\"\"\"", "210": "        # Create test coverage file with contexts", "211": "        coverage_file = tmp_path / \"coverage.json\"", "212": "        coverage_data = {", "213": "            \"meta\": {\"show_contexts\": True},", "214": "            \"files\": {", "215": "                \"src/module.py\": {", "216": "                    \"contexts\": {", "217": "                        \"test_foo\": [1, 2, 3]", "218": "                    }", "219": "                }", "220": "            }", "221": "        }", "222": "        coverage_file.write_text(json.dumps(coverage_data))", "223": "", "224": "        from testiq.coverage_converter import main", "225": "        from click.testing import CliRunner", "226": "", "227": "        runner = CliRunner()", "228": "        output_file = tmp_path / \"output.json\"", "229": "        result = runner.invoke(", "230": "            main,", "231": "            [str(coverage_file), \"-o\", str(output_file), \"--with-contexts\"]", "232": "        )", "233": "", "234": "        assert result.exit_code == 0", "235": "        assert output_file.exists()", "236": "", "237": "        # Verify output has contexts", "238": "        output_data = json.loads(output_file.read_text())", "239": "        assert \"test_foo\" in output_data", "240": "", "241": "    def test_default_output_filename(self, tmp_path, monkeypatch):", "242": "        \"\"\"Test CLI uses default output filename.\"\"\"", "243": "        # Create test coverage file", "244": "        coverage_file = tmp_path / \"coverage.json\"", "245": "        coverage_data = {", "246": "            \"files\": {", "247": "                \"src/module.py\": {", "248": "                    \"executed_lines\": [1, 2, 3]", "249": "                }", "250": "            }", "251": "        }", "252": "        coverage_file.write_text(json.dumps(coverage_data))", "253": "", "254": "        # Change to tmp directory", "255": "        monkeypatch.chdir(tmp_path)", "256": "", "257": "        from testiq.coverage_converter import main", "258": "        from click.testing import CliRunner", "259": "", "260": "        runner = CliRunner()", "261": "        result = runner.invoke(main, [str(coverage_file)])", "262": "", "263": "        assert result.exit_code == 0", "264": "        assert (tmp_path / \"testiq_coverage.json\").exists()", "265": "", "266": "    def test_invalid_json(self, tmp_path):", "267": "        \"\"\"Test CLI handles invalid JSON.\"\"\"", "268": "        coverage_file = tmp_path / \"invalid.json\"", "269": "        coverage_file.write_text(\"not valid json\")", "270": "", "271": "        from testiq.coverage_converter import main", "272": "        from click.testing import CliRunner", "273": "", "274": "        runner = CliRunner()", "275": "        result = runner.invoke(main, [str(coverage_file)])", "276": "", "277": "        assert result.exit_code == 1", "278": "        assert \"Error\" in result.output"}, "tests/test_cli.py": {"1": "\"\"\"Tests for TestIQ CLI module.\"\"\"", "2": "", "3": "import json", "4": "", "5": "import pytest", "6": "from click.testing import CliRunner", "7": "", "8": "from testiq.cli import main", "9": "", "10": "", "11": "@pytest.fixture", "12": "def runner():", "13": "    \"\"\"Create a CLI runner for testing.\"\"\"", "14": "    return CliRunner()", "15": "", "16": "", "17": "@pytest.fixture", "18": "def sample_coverage_data(tmp_path):", "19": "    \"\"\"Create a sample coverage data file.\"\"\"", "20": "    coverage_data = {", "21": "        \"test_user_login_1\": {\"auth.py\": [10, 11, 12, 15, 20], \"user.py\": [5, 6, 7]},", "22": "        \"test_user_login_2\": {\"auth.py\": [10, 11, 12, 15, 20], \"user.py\": [5, 6, 7]},", "23": "        \"test_admin_login\": {\"auth.py\": [10, 11, 12, 15, 20, 30], \"admin.py\": [50]},", "24": "    }", "25": "", "26": "    coverage_file = tmp_path / \"coverage.json\"", "27": "    coverage_file.write_text(json.dumps(coverage_data, indent=2))", "28": "", "29": "    return coverage_file", "30": "", "31": "", "32": "class TestCLI:", "33": "    \"\"\"Test suite for CLI commands.\"\"\"", "34": "", "35": "    def test_version(self, runner):", "36": "        \"\"\"Test --version flag.\"\"\"", "37": "        result = runner.invoke(main, [\"--version\"])", "38": "", "39": "        assert result.exit_code == 0", "40": "        assert \"testiq\" in result.output.lower() or \"version\" in result.output.lower()", "41": "", "42": "    def test_help(self, runner):", "43": "        \"\"\"Test --help flag.\"\"\"", "44": "        result = runner.invoke(main, [\"--help\"])", "45": "", "46": "        assert result.exit_code == 0", "47": "        assert \"TestIQ\" in result.output", "48": "        assert \"Intelligent Test Analysis\" in result.output", "49": "", "50": "    def test_demo_command(self, runner):", "51": "        \"\"\"Test demo command.\"\"\"", "52": "        result = runner.invoke(main, [\"demo\"])", "53": "", "54": "        assert result.exit_code == 0", "55": "        assert \"demo\" in result.output.lower() or \"Exact Duplicates\" in result.output", "56": "", "57": "    def test_analyze_command_variations(self, runner, sample_coverage_data, tmp_path):", "58": "        \"\"\"Test analyze command with various options and configurations.\"\"\"", "59": "        # Test basic analyze command", "60": "        result = runner.invoke(main, [\"analyze\", str(sample_coverage_data)])", "61": "        assert result.exit_code == 0", "62": "", "63": "        # Test with custom threshold", "64": "        result = runner.invoke(main, [\"analyze\", str(sample_coverage_data), \"--threshold\", \"0.8\"])", "65": "        assert result.exit_code == 0", "66": "", "67": "        # Test with log level option", "68": "        result = runner.invoke(main, [\"--log-level\", \"DEBUG\", \"analyze\", str(sample_coverage_data)])", "69": "        assert result.exit_code == 0", "70": "", "71": "        # Test text format ignores output file", "72": "        output_file = tmp_path / \"ignored.txt\"", "73": "        result = runner.invoke(", "74": "            main,", "75": "            [\"analyze\", str(sample_coverage_data), \"--format\", \"text\", \"--output\", str(output_file)],", "76": "        )", "77": "        assert result.exit_code == 0", "78": "        assert not output_file.exists() or output_file.stat().st_size == 0", "79": "", "80": "        # Test with log file", "81": "        log_file = tmp_path / \"testiq.log\"", "82": "        result = runner.invoke(", "83": "            main, [\"--log-file\", str(log_file), \"analyze\", str(sample_coverage_data)]", "84": "        )", "85": "        assert result.exit_code == 0", "86": "", "87": "        # Test save baseline", "88": "        baseline_file = tmp_path / \"test_baseline\"", "89": "        result = runner.invoke(", "90": "            main, [\"analyze\", str(sample_coverage_data), \"--save-baseline\", str(baseline_file)]", "91": "        )", "92": "        assert result.exit_code == 0", "93": "        assert \"saved\" in result.output.lower()", "94": "", "95": "        # Test custom config file", "96": "        config_file = tmp_path / \"testiq.yaml\"", "97": "        config_file.write_text(\"\"\"", "98": "analysis:", "99": "  similarity_threshold: 0.95", "100": "", "101": "performance:", "102": "  enable_parallel: false", "103": "\"\"\")", "104": "        coverage_data = {\"test_a\": {\"file.py\": [1, 2]}}", "105": "        coverage_file = tmp_path / \"coverage.json\"", "106": "        coverage_file.write_text(json.dumps(coverage_data))", "107": "        result = runner.invoke(", "108": "            main, [\"--config\", str(config_file), \"analyze\", str(coverage_file)]", "109": "        )", "110": "        assert result.exit_code == 0", "111": "", "112": "    def test_analyze_output_formats(self, runner, sample_coverage_data, tmp_path):", "113": "        \"\"\"Test analyze command with various output formats and options.\"\"\"", "114": "        # Test JSON format with output file", "115": "        output_file = tmp_path / \"output.json\"", "116": "        result = runner.invoke(", "117": "            main,", "118": "            [", "119": "                \"analyze\",", "120": "                str(sample_coverage_data),", "121": "                \"--format\",", "122": "                \"json\",", "123": "                \"--output\",", "124": "                str(output_file),", "125": "            ],", "126": "        )", "127": "        assert result.exit_code == 0", "128": "        assert output_file.exists()", "129": "        with open(output_file) as f:", "130": "            data = json.load(f)", "131": "            assert \"exact_duplicates\" in data", "132": "            assert \"subset_duplicates\" in data", "133": "            assert \"similar_tests\" in data", "134": "", "135": "        # Test JSON to stdout", "136": "        result = runner.invoke(main, [\"analyze\", str(sample_coverage_data), \"--format\", \"json\"])", "137": "        assert result.exit_code == 0", "138": "        output = result.output", "139": "        json_start = output.find(\"{\")", "140": "        if json_start >= 0:", "141": "            json_text = output[json_start:]", "142": "            try:", "143": "                data = json.loads(json_text)", "144": "                assert \"exact_duplicates\" in data", "145": "            except json.JSONDecodeError:", "146": "                assert \"{\" in output and \"}\" in output", "147": "        else:", "148": "            assert len(output) > 0", "149": "", "150": "        # Test with all options", "151": "        output_file2 = tmp_path / \"full_report.json\"", "152": "        result = runner.invoke(", "153": "            main,", "154": "            [", "155": "                \"analyze\",", "156": "                str(sample_coverage_data),", "157": "                \"--threshold\",", "158": "                \"0.75\",", "159": "                \"--format\",", "160": "                \"json\",", "161": "                \"--output\",", "162": "                str(output_file2),", "163": "            ],", "164": "        )", "165": "        assert result.exit_code == 0", "166": "        assert output_file2.exists()", "167": "", "168": "        # Test CSV format", "169": "        output_file3 = tmp_path / \"report.csv\"", "170": "        result = runner.invoke(", "171": "            main,", "172": "            [\"analyze\", str(sample_coverage_data), \"--format\", \"csv\", \"--output\", str(output_file3)],", "173": "        )", "174": "        assert result.exit_code == 0", "175": "        assert output_file3.exists()", "176": "", "177": "    def test_analyze_markdown_format(self, runner, sample_coverage_data, tmp_path):", "178": "        \"\"\"Test analyze command with markdown output.\"\"\"", "179": "        output_file = tmp_path / \"report.md\"", "180": "", "181": "        result = runner.invoke(", "182": "            main,", "183": "            [", "184": "                \"analyze\",", "185": "                str(sample_coverage_data),", "186": "                \"--format\",", "187": "                \"markdown\",", "188": "                \"--output\",", "189": "                str(output_file),", "190": "            ],", "191": "        )", "192": "", "193": "        assert result.exit_code == 0", "194": "        assert output_file.exists()", "195": "", "196": "        content = output_file.read_text()", "197": "        assert \"# Test Duplication Report\" in content", "198": "", "199": "    def test_cli_error_handling_scenarios(self, runner, tmp_path):", "200": "        \"\"\"Test comprehensive CLI error handling scenarios.\"\"\"", "201": "        # Test 1: Non-existent file", "202": "        result = runner.invoke(main, [\"analyze\", \"nonexistent.json\"])", "203": "        assert result.exit_code != 0", "204": "", "205": "        # Test 2: Invalid JSON", "206": "        bad_file = tmp_path / \"bad.json\"", "207": "        bad_file.write_text(\"not valid json {\")", "208": "        result = runner.invoke(main, [\"analyze\", str(bad_file)])", "209": "        assert result.exit_code != 0", "210": "        assert \"error\" in result.output.lower() or \"invalid\" in result.output.lower()", "211": "", "212": "        # Test 3: Invalid coverage data structure", "213": "        bad_data = {\"test1\": \"not a dict\"}", "214": "        coverage_file = tmp_path / \"bad_structure.json\"", "215": "        coverage_file.write_text(json.dumps(bad_data))", "216": "        result = runner.invoke(main, [\"analyze\", str(coverage_file)])", "217": "        assert result.exit_code != 0", "218": "", "219": "        # Test 4: Security violation (invalid file extension)", "220": "        bad_ext = tmp_path / \"test.exe\"", "221": "        bad_ext.write_text(\"{}\")", "222": "        result = runner.invoke(main, [\"analyze\", str(bad_ext)])", "223": "        assert result.exit_code != 0", "224": "", "225": "", "226": "class TestCLIIntegration:", "227": "    \"\"\"Integration tests for CLI.\"\"\"", "228": "", "229": "    def test_cli_workflow_with_thresholds(self, runner, sample_coverage_data, tmp_path):", "230": "        \"\"\"Test complete CLI workflow with duplicate detection and threshold variations.\"\"\"", "231": "        # Test 1: Full workflow with duplicate detection", "232": "        coverage_data = {", "233": "            \"test_a\": {\"file.py\": [1, 2, 3]},", "234": "            \"test_b\": {\"file.py\": [1, 2, 3]},", "235": "            \"test_c\": {\"file.py\": [10, 20]},", "236": "        }", "237": "        input_file = tmp_path / \"coverage.json\"", "238": "        input_file.write_text(json.dumps(coverage_data))", "239": "        output_file = tmp_path / \"report.json\"", "240": "        result = runner.invoke(", "241": "            main, [\"analyze\", str(input_file), \"--format\", \"json\", \"--output\", str(output_file)]", "242": "        )", "243": "        assert result.exit_code == 0", "244": "        assert output_file.exists()", "245": "        with open(output_file) as f:", "246": "            data = json.load(f)", "247": "            assert len(data[\"exact_duplicates\"]) >= 1", "248": "            duplicates = data[\"exact_duplicates\"][0]", "249": "            assert set(duplicates) == {\"test_a\", \"test_b\"}", "250": "", "251": "        # Test 2: Threshold parameter affects similarity results", "252": "        output_low = tmp_path / \"low_threshold.json\"", "253": "        output_high = tmp_path / \"high_threshold.json\"", "254": "        runner.invoke(", "255": "            main,", "256": "            [", "257": "                \"analyze\",", "258": "                str(sample_coverage_data),", "259": "                \"--threshold\",", "260": "                \"0.5\",", "261": "                \"--format\",", "262": "                \"json\",", "263": "                \"--output\",", "264": "                str(output_low),", "265": "            ],", "266": "        )", "267": "        runner.invoke(", "268": "            main,", "269": "            [", "270": "                \"analyze\",", "271": "                str(sample_coverage_data),", "272": "                \"--threshold\",", "273": "                \"0.9\",", "274": "                \"--format\",", "275": "                \"json\",", "276": "                \"--output\",", "277": "                str(output_high),", "278": "            ],", "279": "        )", "280": "        with open(output_low) as f:", "281": "            low_data = json.load(f)", "282": "        with open(output_high) as f:", "283": "            high_data = json.load(f)", "284": "        assert len(low_data[\"similar_tests\"]) >= len(high_data[\"similar_tests\"])", "285": "", "286": "", "287": "class TestCLIFormats:", "288": "    \"\"\"Test different output formats.\"\"\"", "289": "", "290": "    def test_html_format(self, runner, sample_coverage_data, tmp_path):", "291": "        \"\"\"Test HTML output format.\"\"\"", "292": "        output_file = tmp_path / \"report.html\"", "293": "        result = runner.invoke(", "294": "            main,", "295": "            [\"analyze\", str(sample_coverage_data), \"--format\", \"html\", \"--output\", str(output_file)],", "296": "        )", "297": "        assert result.exit_code == 0", "298": "        assert output_file.exists()", "299": "        content = output_file.read_text()", "300": "        assert \"<html\" in content.lower()", "301": "", "302": "    def test_formats_requiring_output(self, runner, sample_coverage_data):", "303": "        \"\"\"Test formats that require output file specification.\"\"\"", "304": "        # HTML format requires output file", "305": "        result = runner.invoke(main, [\"analyze\", str(sample_coverage_data), \"--format\", \"html\"])", "306": "        assert result.exit_code != 0", "307": "        assert \"requires --output\" in result.output.lower()", "308": "", "309": "        # CSV format requires output file", "310": "        result = runner.invoke(main, [\"analyze\", str(sample_coverage_data), \"--format\", \"csv\"])", "311": "        assert result.exit_code != 0", "312": "        assert \"requires --output\" in result.output.lower()", "313": "", "314": "", "315": "", "316": "class TestCLIQualityGate:", "317": "    \"\"\"Test quality gate functionality.\"\"\"", "318": "", "319": "    def test_quality_gate_pass_and_fail(self, runner, tmp_path):", "320": "        \"\"\"Test quality gate pass and fail scenarios.\"\"\"", "321": "        # Test passing case with unique tests", "322": "        coverage_data_pass = {", "323": "            \"test_a\": {\"file.py\": [1, 2]},", "324": "            \"test_b\": {\"file.py\": [3, 4]},", "325": "            \"test_c\": {\"file.py\": [5, 6]},", "326": "        }", "327": "        coverage_file_pass = tmp_path / \"coverage_pass.json\"", "328": "        coverage_file_pass.write_text(json.dumps(coverage_data_pass))", "329": "        result = runner.invoke(", "330": "            main, [\"analyze\", str(coverage_file_pass), \"--quality-gate\", \"--max-duplicates\", \"0\"]", "331": "        )", "332": "        assert result.exit_code == 0", "333": "        assert \"PASSED\" in result.output", "334": "", "335": "        # Test failing case with duplicates", "336": "        coverage_data_fail = {", "337": "            \"test_a\": {\"file.py\": [1, 2, 3]},", "338": "            \"test_b\": {\"file.py\": [1, 2, 3]},", "339": "        }", "340": "        coverage_file_fail = tmp_path / \"coverage_fail.json\"", "341": "        coverage_file_fail.write_text(json.dumps(coverage_data_fail))", "342": "        result = runner.invoke(", "343": "            main, [\"analyze\", str(coverage_file_fail), \"--quality-gate\", \"--max-duplicates\", \"0\"]", "344": "        )", "345": "        assert result.exit_code == 2", "346": "        assert \"FAILED\" in result.output", "347": "", "348": "", "349": "class TestCLIQualityScore:", "350": "    \"\"\"Test quality-score command.\"\"\"", "351": "", "352": "    def test_quality_score_with_output(self, runner, sample_coverage_data, tmp_path):", "353": "        \"\"\"Test quality score with output file.\"\"\"", "354": "        output_file = tmp_path / \"quality.json\"", "355": "        result = runner.invoke(", "356": "            main, [\"quality-score\", str(sample_coverage_data), \"--output\", str(output_file)]", "357": "        )", "358": "        # May fail due to Rich formatting issues, but we test file output", "359": "        if result.exit_code == 0:", "360": "            assert output_file.exists()", "361": "            with open(output_file) as f:", "362": "                data = json.load(f)", "363": "                assert \"score\" in data", "364": "                assert \"recommendations\" in data", "365": "                assert \"statistics\" in data", "366": "", "367": "", "368": "class TestCLIBaseline:", "369": "    \"\"\"Test baseline management commands.\"\"\"", "370": "", "371": "    def test_baseline_operations(self, runner, tmp_path, monkeypatch):", "372": "        \"\"\"Test baseline management operations.\"\"\"", "373": "        baseline_dir = tmp_path / \".testiq\" / \"baselines\"", "374": "        baseline_dir.mkdir(parents=True, exist_ok=True)", "375": "        monkeypatch.setenv(\"HOME\", str(tmp_path))", "376": "", "377": "        # Test listing when empty", "378": "        result = runner.invoke(main, [\"baseline\", \"list\"])", "379": "        assert \"No baselines\" in result.output or len(result.output) > 0", "380": "", "381": "        # Test showing non-existent baseline", "382": "        result = runner.invoke(main, [\"baseline\", \"show\", \"nonexistent\"])", "383": "        assert result.exit_code != 0 or \"not found\" in result.output.lower()", "384": "", "385": "        # Test deleting non-existent baseline", "386": "        result = runner.invoke(main, [\"baseline\", \"delete\", \"nonexistent\", \"--force\"])", "387": "        assert result.exit_code != 0 or \"not found\" in result.output.lower()", "388": "", "389": "", "390": "class TestCLIConfig:", "391": "    \"\"\"Test configuration handling.\"\"\"", "392": "", "393": "    def test_custom_config_file(self, runner, tmp_path):", "394": "        \"\"\"Test loading custom config file.\"\"\"", "395": "        config_file = tmp_path / \"testiq.yaml\"", "396": "        config_file.write_text(\"\"\"", "397": "analysis:", "398": "  similarity_threshold: 0.95", "399": "", "400": "performance:", "401": "  enable_parallel: false", "402": "\"\"\")", "403": "        coverage_data = {\"test_a\": {\"file.py\": [1, 2]}}", "404": "        coverage_file = tmp_path / \"coverage.json\"", "405": "        coverage_file.write_text(json.dumps(coverage_data))", "406": "", "407": "        result = runner.invoke(", "408": "            main, [\"--config\", str(config_file), \"analyze\", str(coverage_file)]", "409": "        )", "410": "        assert result.exit_code == 0", "411": "", "412": "", "413": "class TestCLIErrorHandling:", "414": "    \"\"\"Test error handling in CLI.\"\"\"", "415": "", "416": "    def test_config_error(self, runner, tmp_path):", "417": "        \"\"\"Test configuration error handling.\"\"\"", "418": "        bad_config = tmp_path / \"bad_config.yaml\"", "419": "        bad_config.write_text(\"invalid: yaml: content: [unclosed\")", "420": "", "421": "        coverage_data = {\"test\": {\"file.py\": [1]}}", "422": "        coverage_file = tmp_path / \"coverage.json\"", "423": "        coverage_file.write_text(json.dumps(coverage_data))", "424": "", "425": "        result = runner.invoke(", "426": "            main, [\"--config\", str(bad_config), \"analyze\", str(coverage_file)]", "427": "        )", "428": "        assert result.exit_code != 0"}, "src/testiq/cli.py": {"1": "\"\"\"", "2": "Command-line interface for TestIQ.", "3": "\"\"\"", "4": "", "5": "import json", "6": "import sys", "7": "from datetime import datetime", "8": "from pathlib import Path", "9": "from typing import Optional", "10": "", "11": "import click", "12": "from rich import box", "13": "from rich.console import Console", "14": "from rich.panel import Panel", "15": "from rich.table import Table", "16": "from rich.text import Text", "17": "", "18": "from testiq import __version__", "19": "from testiq.analysis import QualityAnalyzer, RecommendationEngine", "20": "from testiq.analyzer import CoverageDuplicateFinder", "21": "from testiq.cicd import BaselineManager, QualityGate, QualityGateChecker, TrendTracker, get_exit_code", "22": "from testiq.config import Config, load_config", "23": "from testiq.exceptions import TestIQError", "24": "from testiq.logging_config import get_logger, setup_logging", "25": "from testiq.reporting import CSVReportGenerator, HTMLReportGenerator", "26": "from testiq.security import (", "27": "    check_file_size,", "28": "    sanitize_output_path,", "29": "    validate_coverage_data,", "30": "    validate_file_path,", "31": ")", "32": "", "33": "console = Console()", "34": "logger = get_logger(__name__)", "35": "", "36": "# Constants", "37": "TESTIQ_CONFIG_DIR = \".testiq\"", "38": "SAMPLE_AUTH_FILE = \"auth.py\"", "39": "SAMPLE_USER_FILE = \"user.py\"", "40": "", "41": "", "42": "def _get_grade_color(grade: str) -> str:", "43": "    \"\"\"Get the color for a grade letter.\"\"\"", "44": "    first_letter = grade[0] if grade else 'F'", "45": "    if first_letter == 'A':", "46": "        return 'green'", "47": "    elif first_letter in ('B', 'C'):", "48": "        return 'yellow'", "49": "    else:", "50": "        return 'red'", "51": "", "52": "", "53": "@click.group()", "54": "@click.version_option(version=__version__, prog_name=\"testiq\")", "55": "@click.option(", "56": "    \"--config\",", "57": "    \"-c\",", "58": "    type=click.Path(exists=True, path_type=Path),", "59": "    help=\"Path to configuration file (.yaml, .yml, .toml)\",", "60": ")", "61": "@click.option(", "62": "    \"--log-level\",", "63": "    type=click.Choice([\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]),", "64": "    help=\"Set logging level\",", "65": ")", "66": "@click.option(", "67": "    \"--log-file\",", "68": "    type=click.Path(path_type=Path),", "69": "    help=\"Log file path\",", "70": ")", "71": "@click.pass_context", "72": "def main(", "73": "    ctx: click.Context, config: Optional[Path], log_level: Optional[str], log_file: Optional[Path]", "74": ") -> None:", "75": "    \"\"\"", "76": "    TestIQ - Intelligent Test Analysis", "77": "", "78": "    Find duplicate and redundant tests using coverage analysis.", "79": "    \"\"\"", "80": "    # Load configuration", "81": "    try:", "82": "        cfg = load_config(config)", "83": "", "84": "        # Override with CLI options", "85": "        if log_level:", "86": "            cfg.log.level = log_level", "87": "        if log_file:", "88": "            cfg.log.file = str(log_file)", "89": "", "90": "        # Setup logging", "91": "        setup_logging(", "92": "            level=cfg.log.level,", "93": "            log_file=Path(cfg.log.file) if cfg.log.file else None,", "94": "            enable_rotation=cfg.log.enable_rotation,", "95": "            max_bytes=cfg.log.max_bytes,", "96": "            backup_count=cfg.log.backup_count,", "97": "        )", "98": "", "99": "        # Store config in context", "100": "        ctx.ensure_object(dict)", "101": "        ctx.obj[\"config\"] = cfg", "102": "", "103": "        logger.debug(f\"TestIQ v{__version__} initialized\")", "104": "        if config:", "105": "            logger.info(f\"Loaded configuration from: {config}\")", "106": "", "107": "    except Exception as e:", "108": "        console.print(f\"[red]Configuration error: {e}[/red]\")", "109": "        sys.exit(1)", "110": "", "111": "", "112": "@main.command()", "113": "@click.argument(\"coverage_file\", type=click.Path(exists=True, path_type=Path))", "114": "@click.option(", "115": "    \"--threshold\",", "116": "    \"-t\",", "117": "    type=float,", "118": "    help=\"Similarity threshold (0.0-1.0). Default: 0.3 (30%). Tests with \u226530%% overlap are considered similar.\",", "119": ")", "120": "@click.option(", "121": "    \"--output\",", "122": "    \"-o\",", "123": "    type=click.Path(path_type=Path),", "124": "    help=\"Output file for the report (default: stdout)\",", "125": ")", "126": "@click.option(", "127": "    \"--format\",", "128": "    \"-f\",", "129": "    type=click.Choice([\"markdown\", \"json\", \"text\", \"html\", \"csv\"]),", "130": "    default=\"text\",", "131": "    help=\"Output format (html and csv require --output)\",", "132": ")", "133": "@click.option(", "134": "    \"--quality-gate\",", "135": "    is_flag=True,", "136": "    help=\"Enable quality gate checking (exits with code 2 if failed)\",", "137": ")", "138": "@click.option(", "139": "    \"--max-duplicates\",", "140": "    type=int,", "141": "    default=0,", "142": "    help=\"Maximum allowed exact duplicates. Default: 0 (no duplicates allowed)\",", "143": ")", "144": "@click.option(", "145": "    \"--baseline\",", "146": "    type=click.Path(path_type=Path),", "147": "    help=\"Baseline file for comparison (for quality gate)\",", "148": ")", "149": "@click.option(", "150": "    \"--save-baseline\",", "151": "    type=click.Path(path_type=Path),", "152": "    help=\"Save results as baseline for future comparisons\",", "153": ")", "154": "@click.pass_context", "155": "def analyze(", "156": "    ctx: click.Context,", "157": "    coverage_file: Path,", "158": "    threshold: Optional[float],", "159": "    output: Optional[Path],", "160": "    format: str,", "161": "    quality_gate: bool,", "162": "    max_duplicates: int,", "163": "    baseline: Optional[Path],", "164": "    save_baseline: Optional[Path],", "165": ") -> None:", "166": "    \"\"\"", "167": "    Analyze test coverage data to find duplicates.", "168": "", "169": "    COVERAGE_FILE: JSON file containing per-test coverage data", "170": "    \"\"\"", "171": "    cfg: Config = ctx.obj[\"config\"]", "172": "", "173": "    # Use config threshold if not provided", "174": "    if threshold is None:", "175": "        threshold = cfg.analysis.similarity_threshold", "176": "", "177": "    console.print(\"[cyan]TestIQ Analysis Starting...[/cyan]\")", "178": "    console.print(f\"  \u2022 Coverage file: {coverage_file}\")", "179": "    console.print(f\"  \u2022 Similarity threshold: {threshold:.1%} (tests with \u2265{threshold:.1%} overlap are flagged)\")", "180": "    console.print(f\"  \u2022 Max duplicates allowed: {max_duplicates}\")", "181": "    console.print(f\"  \u2022 Output format: {format}\")", "182": "    if output:", "183": "        console.print(f\"  \u2022 Output file: {output}\")", "184": "    console.print()", "185": "", "186": "    try:", "187": "        # Security: Validate and check file", "188": "        validated_path = validate_file_path(coverage_file)", "189": "        check_file_size(validated_path, cfg.security.max_file_size)", "190": "", "191": "        # Load coverage data", "192": "        with open(validated_path) as f:", "193": "            coverage_data = json.load(f)", "194": "", "195": "        # Security: Validate coverage data", "196": "        validate_coverage_data(coverage_data, cfg.security.max_tests)", "197": "", "198": "        logger.info(f\"Loaded {len(coverage_data)} tests from coverage file\")", "199": "", "200": "        # Create analyzer with config", "201": "        finder = CoverageDuplicateFinder(", "202": "            enable_parallel=cfg.performance.enable_parallel,", "203": "            max_workers=cfg.performance.max_workers,", "204": "            enable_caching=cfg.performance.enable_caching,", "205": "            cache_dir=cfg.performance.cache_dir,", "206": "        )", "207": "", "208": "        # Add test coverage", "209": "        for test_name, test_coverage in coverage_data.items():", "210": "            finder.add_test_coverage(test_name, test_coverage)", "211": "", "212": "        # Quality gate checking", "213": "        exit_code = 0", "214": "        if quality_gate:", "215": "            gate = QualityGate(", "216": "                max_duplicates=max_duplicates,", "217": "                fail_on_increase=baseline is not None,", "218": "            )", "219": "            checker = QualityGateChecker(gate)", "220": "", "221": "            # Load baseline if provided", "222": "            baseline_result = None", "223": "            if baseline:", "224": "                baseline_mgr = BaselineManager(Path.home() / TESTIQ_CONFIG_DIR / \"baselines\")", "225": "                baseline_result = baseline_mgr.load(baseline.stem)", "226": "", "227": "            passed, details = checker.check(finder, threshold, baseline_result)", "228": "", "229": "            if not passed:", "230": "                console.print(\"\\n[red]\u2717 Quality Gate FAILED[/red]\")", "231": "                for failure in details[\"failures\"]:", "232": "                    console.print(f\"  \u2022 {failure}\")", "233": "                exit_code = 2", "234": "            else:", "235": "                console.print(\"\\n[green]\u2713 Quality Gate PASSED[/green]\")", "236": "", "237": "        # Save baseline if requested", "238": "        if save_baseline:", "239": "            from testiq.cicd import AnalysisResult", "240": "", "241": "            exact_dups = finder.find_exact_duplicates()", "242": "            duplicate_count = sum(len(g) - 1 for g in exact_dups)", "243": "            total_tests = len(finder.tests)", "244": "", "245": "            result = AnalysisResult(", "246": "                timestamp=datetime.now().isoformat(),", "247": "                total_tests=total_tests,", "248": "                exact_duplicates=duplicate_count,", "249": "                duplicate_groups=len(exact_dups),", "250": "                subset_duplicates=len(finder.find_subset_duplicates()),", "251": "                similar_pairs=len(finder.find_similar_coverage(threshold)),", "252": "                duplicate_percentage=(duplicate_count / total_tests * 100) if total_tests > 0 else 0,", "253": "                threshold=threshold,", "254": "            )", "255": "", "256": "            baseline_mgr = BaselineManager(Path.home() / TESTIQ_CONFIG_DIR / \"baselines\")", "257": "            baseline_mgr.save(result, save_baseline.stem)", "258": "            console.print(f\"[green]\u2713 Baseline saved: {save_baseline}[/green]\")", "259": "", "260": "        # Generate output based on format", "261": "        if format == \"html\":", "262": "            if not output:", "263": "                console.print(\"[red]Error: HTML format requires --output[/red]\")", "264": "                sys.exit(1)", "265": "            html_gen = HTMLReportGenerator(finder)", "266": "            html_gen.generate(output, threshold=threshold)", "267": "            console.print(f\"[green]\u2713 HTML report saved to {output}[/green]\")", "268": "", "269": "        elif format == \"csv\":", "270": "            if not output:", "271": "                console.print(\"[red]Error: CSV format requires --output[/red]\")", "272": "                sys.exit(1)", "273": "            csv_gen = CSVReportGenerator(finder)", "274": "            csv_gen.generate_summary(output, threshold=threshold)", "275": "            console.print(f\"[green]\u2713 CSV report saved to {output}[/green]\")", "276": "", "277": "        elif format == \"json\":", "278": "            # Get statistics for comprehensive output", "279": "            stats = finder.get_statistics(threshold)", "280": "", "281": "            result = {", "282": "                \"metadata\": {", "283": "                    \"timestamp\": datetime.now().isoformat(),", "284": "                    \"testiq_version\": __version__,", "285": "                    \"threshold\": threshold,", "286": "                    \"total_tests\": len(finder.tests),", "287": "                    \"total_removable_duplicates\": stats[\"total_removable_duplicates\"],", "288": "                },", "289": "                \"exact_duplicates\": finder.find_exact_duplicates(),", "290": "                \"subset_duplicates\": [", "291": "                    {\"subset\": s, \"superset\": sup, \"ratio\": r}", "292": "                    for s, sup, r in finder.get_sorted_subset_duplicates()", "293": "                ],", "294": "                \"similar_tests\": [", "295": "                    {\"test1\": t1, \"test2\": t2, \"similarity\": sim}", "296": "                    for t1, t2, sim in finder.find_similar_coverage(threshold)", "297": "                ],", "298": "                \"statistics\": stats,", "299": "            }", "300": "            output_text = json.dumps(result, indent=2)", "301": "", "302": "            if output:", "303": "                validated_output = sanitize_output_path(output)", "304": "                validated_output.write_text(output_text)", "305": "                console.print(f\"[green]\u2713 JSON report saved to {validated_output}[/green]\")", "306": "            else:", "307": "                console.print(output_text)", "308": "", "309": "        elif format == \"markdown\":", "310": "            output_text = finder.generate_report(threshold)", "311": "", "312": "            if output:", "313": "                validated_output = sanitize_output_path(output)", "314": "                validated_output.write_text(output_text)", "315": "                console.print(f\"[green]\u2713 Report saved to {validated_output}[/green]\")", "316": "            else:", "317": "                console.print(output_text)", "318": "", "319": "        else:  # text format with rich", "320": "            if output:", "321": "                console.print(\"[yellow]Warning: --output ignored for text format[/yellow]\")", "322": "            display_results(finder, threshold)", "323": "", "324": "        sys.exit(exit_code)", "325": "", "326": "    except TestIQError as e:", "327": "        console.print(f\"[red]Error: {e}[/red]\")", "328": "        logger.error(str(e))", "329": "        sys.exit(1)", "330": "    except json.JSONDecodeError as e:", "331": "        console.print(f\"[red]Invalid JSON in {coverage_file}: {e}[/red]\")", "332": "        logger.error(f\"JSON decode error: {e}\")", "333": "        sys.exit(1)", "334": "    except Exception as e:", "335": "        console.print(f\"[red]Unexpected error: {e}[/red]\")", "336": "        logger.exception(\"Unexpected error during analysis\")", "337": "        sys.exit(1)", "338": "", "339": "", "340": "def display_results(finder: CoverageDuplicateFinder, threshold: float) -> None:", "341": "    \"\"\"Display results in rich formatted text.\"\"\"", "342": "    console.print(", "343": "        Panel(", "344": "            \"[bold cyan]TestIQ Analysis Results[/bold cyan]\",", "345": "            box=box.DOUBLE,", "346": "        )", "347": "    )", "348": "", "349": "    # Exact duplicates", "350": "    exact_dups = finder.find_exact_duplicates()", "351": "    if exact_dups:", "352": "        table = Table(title=\"\ud83c\udfaf Exact Duplicates (Identical Coverage)\", show_header=True)", "353": "        table.add_column(\"Group\", style=\"cyan\", width=10)", "354": "        table.add_column(\"Tests\", style=\"yellow\", no_wrap=False, overflow=\"fold\")", "355": "        table.add_column(\"Action\", style=\"green\", width=20)", "356": "", "357": "        for i, group in enumerate(exact_dups, 1):", "358": "            tests_str = \"\\n\".join(group)", "359": "            action = f\"Remove {len(group) - 1} duplicate(s)\"", "360": "            table.add_row(f\"Group {i}\", tests_str, action)", "361": "", "362": "        console.print(table)", "363": "        console.print()", "364": "", "365": "    # Subset duplicates (sorted by ratio)", "366": "    subsets = finder.get_sorted_subset_duplicates()", "367": "    if subsets:", "368": "        table = Table(title=\"\ud83d\udcca Subset Duplicates (Sorted by Coverage Ratio)\", show_header=True)", "369": "        table.add_column(\"Subset Test\", style=\"yellow\", no_wrap=False, overflow=\"fold\")", "370": "        table.add_column(\"Superset Test\", style=\"cyan\", no_wrap=False, overflow=\"fold\")", "371": "        table.add_column(\"Coverage Ratio\", style=\"magenta\", width=15)", "372": "", "373": "        for subset_test, superset_test, ratio in subsets[:10]:", "374": "            table.add_row(subset_test, superset_test, f\"{ratio:.1%}\")", "375": "", "376": "        if len(subsets) > 10:", "377": "            console.print(table)", "378": "            console.print(f\"[dim]... and {len(subsets) - 10} more subset duplicates[/dim]\\n\")", "379": "        else:", "380": "            console.print(table)", "381": "            console.print()", "382": "", "383": "    # Similar tests", "384": "    similar = finder.find_similar_coverage(threshold)", "385": "    if similar:", "386": "        table = Table(title=f\"\ud83d\udd0d Similar Tests (\u2265{threshold:.1%} overlap)\", show_header=True)", "387": "        table.add_column(\"Test 1\", style=\"yellow\", no_wrap=False, overflow=\"fold\")", "388": "        table.add_column(\"Test 2\", style=\"cyan\", no_wrap=False, overflow=\"fold\")", "389": "        table.add_column(\"Similarity\", style=\"magenta\", width=12)", "390": "", "391": "        for test1, test2, similarity in similar[:10]:", "392": "            table.add_row(test1, test2, f\"{similarity:.1%}\")", "393": "", "394": "        if len(similar) > 10:", "395": "            console.print(table)", "396": "            console.print(f\"[dim]... and {len(similar) - 10} more similar test pairs[/dim]\\n\")", "397": "        else:", "398": "            console.print(table)", "399": "            console.print()", "400": "", "401": "    # Summary with statistics", "402": "    stats = finder.get_statistics(threshold)", "403": "    summary_table = Table(title=\"\ud83d\udcc8 Summary\", show_header=True, box=box.ROUNDED)", "404": "    summary_table.add_column(\"Metric\", style=\"cyan\")", "405": "    summary_table.add_column(\"Count\", style=\"bold green\")", "406": "", "407": "    summary_table.add_row(\"Total tests analyzed\", str(len(finder.tests)))", "408": "    summary_table.add_row(\"Exact duplicates (can remove)\", str(stats[\"exact_duplicate_count\"]))", "409": "    summary_table.add_row(\"Subset duplicates (can optimize)\", str(stats[\"subset_duplicate_count\"]))", "410": "    summary_table.add_row(\"Similar test pairs\", str(stats[\"similar_pair_count\"]))", "411": "    summary_table.add_row(\"Total removable duplicates\", str(stats[\"total_removable_duplicates\"]))", "412": "", "413": "    console.print(summary_table)", "414": "", "415": "", "416": "@main.command()", "417": "def demo() -> None:", "418": "    \"\"\"Run a demonstration with sample data.\"\"\"", "419": "    console.print(\"[cyan]Running TestIQ demo with sample data...[/cyan]\\n\")", "420": "", "421": "    finder = CoverageDuplicateFinder()", "422": "", "423": "    # Add sample test data", "424": "    finder.add_test_coverage(", "425": "        \"test_user_login_success_1\",", "426": "        {SAMPLE_AUTH_FILE: [10, 11, 12, 15, 20, 25], SAMPLE_USER_FILE: [5, 6, 7]},", "427": "    )", "428": "", "429": "    finder.add_test_coverage(", "430": "        \"test_user_login_success_2\",", "431": "        {SAMPLE_AUTH_FILE: [10, 11, 12, 15, 20, 25], SAMPLE_USER_FILE: [5, 6, 7]},", "432": "    )", "433": "", "434": "    finder.add_test_coverage(\"test_user_login_minimal\", {SAMPLE_AUTH_FILE: [10, 11, 12]})", "435": "", "436": "    finder.add_test_coverage(", "437": "        \"test_user_login_complete\",", "438": "        {", "439": "            SAMPLE_AUTH_FILE: [10, 11, 12, 15, 20, 25, 30, 35],", "440": "            SAMPLE_USER_FILE: [5, 6, 7],", "441": "            \"db.py\": [100, 101],", "442": "        },", "443": "    )", "444": "", "445": "    finder.add_test_coverage(", "446": "        \"test_admin_login\",", "447": "        {SAMPLE_AUTH_FILE: [10, 11, 12, 15, 20, 25, 40], SAMPLE_USER_FILE: [5, 6, 7], \"admin.py\": [50]},", "448": "    )", "449": "", "450": "    finder.add_test_coverage(", "451": "        \"test_password_reset\", {\"password.py\": [1, 2, 3, 4, 5], \"email.py\": [10, 20]}", "452": "    )", "453": "", "454": "    display_results(finder, threshold=0.3)", "455": "", "456": "", "457": "if __name__ == \"__main__\":", "458": "    main()", "459": "", "460": "", "461": "@main.command(name=\"quality-score\")", "462": "@click.argument(\"coverage_file\", type=click.Path(exists=True, path_type=Path))", "463": "@click.option(", "464": "    \"--threshold\",", "465": "    \"-t\",", "466": "    type=float,", "467": "    help=\"Similarity threshold (0.0-1.0) for detecting similar tests\",", "468": ")", "469": "@click.option(", "470": "    \"--output\",", "471": "    \"-o\",", "472": "    type=click.Path(path_type=Path),", "473": "    help=\"Output file for the report (default: stdout)\",", "474": ")", "475": "@click.pass_context", "476": "def quality_score(", "477": "    ctx: click.Context,", "478": "    coverage_file: Path,", "479": "    threshold: Optional[float],", "480": "    output: Optional[Path],", "481": ") -> None:", "482": "    \"\"\"", "483": "    Analyze test quality and get actionable recommendations.", "484": "", "485": "    COVERAGE_FILE: JSON file containing per-test coverage data", "486": "    \"\"\"", "487": "    cfg: Config = ctx.obj[\"config\"]", "488": "", "489": "    if threshold is None:", "490": "        threshold = cfg.analysis.similarity_threshold", "491": "", "492": "    try:", "493": "        # Load and validate coverage data", "494": "        validated_path = validate_file_path(coverage_file)", "495": "        check_file_size(validated_path, cfg.security.max_file_size)", "496": "", "497": "        with open(validated_path) as f:", "498": "            coverage_data = json.load(f)", "499": "", "500": "        validate_coverage_data(coverage_data, cfg.security.max_tests)", "501": "", "502": "        # Create analyzer", "503": "        finder = CoverageDuplicateFinder(", "504": "            enable_parallel=cfg.performance.enable_parallel,", "505": "            max_workers=cfg.performance.max_workers,", "506": "        )", "507": "", "508": "        for test_name, test_coverage in coverage_data.items():", "509": "            finder.add_test_coverage(test_name, test_coverage)", "510": "", "511": "        # Calculate quality score", "512": "        analyzer = QualityAnalyzer(finder)", "513": "        score = analyzer.calculate_score(threshold)", "514": "", "515": "        # Display score with rich formatting", "516": "        grade_color = _get_grade_color(score.grade)", "517": "        console.print(", "518": "            Panel(", "519": "                f\"[bold cyan]Test Quality Score[/bold cyan]\\n\\n\"", "520": "                f\"Overall Score: [bold yellow]{score.overall_score:.1f}/100[/bold yellow]\\n\"", "521": "                f\"Grade: [bold][{grade_color}]{score.grade}[/{grade_color}][/bold]\\n\\n\"", "522": "                f\"Duplication Score: {score.duplication_score:.1f}/100\\n\"", "523": "                f\"Coverage Efficiency: {score.coverage_efficiency_score:.1f}/100\\n\"", "524": "                f\"Uniqueness Score: {score.uniqueness_score:.1f}/100\",", "525": "                box=box.DOUBLE,", "526": "            )", "527": "        )", "528": "", "529": "        # Generate recommendations", "530": "        engine = RecommendationEngine(finder)", "531": "        report = engine.generate_report(threshold)", "532": "", "533": "        # Display recommendations", "534": "        if report[\"recommendations\"]:", "535": "            console.print(\"\\n[bold cyan]\ud83d\udccb Recommendations:[/bold cyan]\\n\")", "536": "            for rec in report[\"recommendations\"]:", "537": "                priority_color = {\"high\": \"red\", \"medium\": \"yellow\", \"low\": \"green\"}[rec[\"priority\"]]", "538": "                console.print(f\"[{priority_color}]\u2022 [{rec['priority'].upper()}][/{priority_color}] {rec['message']}\")", "539": "", "540": "        # Save to file if requested", "541": "        if output:", "542": "            validated_output = sanitize_output_path(output)", "543": "            output_data = {", "544": "                \"score\": {", "545": "                    \"overall\": score.overall_score,", "546": "                    \"grade\": score.grade,", "547": "                    \"duplication\": score.duplication_score,", "548": "                    \"efficiency\": score.coverage_efficiency_score,", "549": "                    \"uniqueness\": score.uniqueness_score,", "550": "                },", "551": "                \"recommendations\": report[\"recommendations\"],", "552": "                \"statistics\": report[\"statistics\"],", "553": "            }", "554": "            validated_output.write_text(json.dumps(output_data, indent=2))", "555": "            console.print(f\"\\n[green]\u2713 Quality report saved to {validated_output}[/green]\")", "556": "", "557": "    except TestIQError as e:", "558": "        console.print(f\"[red]Error: {e}[/red]\")", "559": "        sys.exit(1)", "560": "    except Exception as e:", "561": "        console.print(f\"[red]Unexpected error: {e}[/red]\")", "562": "        logger.exception(\"Error calculating quality score\")", "563": "        sys.exit(1)", "564": "", "565": "", "566": "@main.group(name=\"baseline\")", "567": "def baseline() -> None:", "568": "    \"\"\"Manage analysis baselines for comparison.\"\"\"", "569": "    pass", "570": "", "571": "", "572": "@baseline.command(name=\"list\")", "573": "def baseline_list() -> None:", "574": "    \"\"\"List all saved baselines.\"\"\"", "575": "    baseline_mgr = BaselineManager(Path.home() / TESTIQ_CONFIG_DIR / \"baselines\")", "576": "    baselines = baseline_mgr.list_baselines()", "577": "", "578": "    if not baselines:", "579": "        console.print(\"[yellow]No baselines found[/yellow]\")", "580": "        return", "581": "", "582": "    table = Table(title=\"Saved Baselines\", show_header=True)", "583": "    table.add_column(\"Name\", style=\"cyan\")", "584": "    table.add_column(\"Tests\", style=\"yellow\")", "585": "    table.add_column(\"Duplicates\", style=\"red\")", "586": "    table.add_column(\"Date\", style=\"green\")", "587": "", "588": "    for bl in baselines:", "589": "        table.add_row(", "590": "            bl[\"name\"],", "591": "            str(bl[\"result\"].total_tests),", "592": "            str(bl[\"result\"].exact_duplicates),", "593": "            bl[\"result\"].timestamp[:10],", "594": "        )", "595": "", "596": "    console.print(table)", "597": "", "598": "", "599": "@baseline.command(name=\"show\")", "600": "@click.argument(\"name\")", "601": "def baseline_show(name: str) -> None:", "602": "    \"\"\"Show details of a specific baseline.\"\"\"", "603": "    baseline_mgr = BaselineManager(Path.home() / TESTIQ_CONFIG_DIR / \"baselines\")", "604": "    result = baseline_mgr.load(name)", "605": "", "606": "    if not result:", "607": "        console.print(f\"[red]Baseline '{name}' not found[/red]\")", "608": "        sys.exit(1)", "609": "", "610": "    console.print(", "611": "        Panel(", "612": "            f\"[bold cyan]Baseline: {name}[/bold cyan]\\n\\n\"", "613": "            f\"Date: {result.timestamp[:10]}\\n\"", "614": "            f\"Total Tests: {result.total_tests}\\n\"", "615": "            f\"Exact Duplicates: {result.exact_duplicates}\\n\"", "616": "            f\"Duplicate Groups: {result.duplicate_groups}\\n\"", "617": "            f\"Subset Duplicates: {result.subset_duplicates}\\n\"", "618": "            f\"Similar Pairs: {result.similar_pairs}\\n\"", "619": "            f\"Duplicate %: {result.duplicate_percentage:.2f}%\\n\"", "620": "            f\"Threshold: {result.threshold}\",", "621": "            box=box.DOUBLE,", "622": "        )", "623": "    )", "624": "", "625": "", "626": "@baseline.command(name=\"delete\")", "627": "@click.argument(\"name\")", "628": "@click.option(\"--force\", \"-f\", is_flag=True, help=\"Don't ask for confirmation\")", "629": "def baseline_delete(name: str, force: bool) -> None:", "630": "    \"\"\"Delete a baseline.\"\"\"", "631": "    if not force and not click.confirm(f\"Delete baseline '{name}'?\"):", "632": "        console.print(\"[yellow]Cancelled[/yellow]\")", "633": "        return", "634": "", "635": "    baseline_mgr = BaselineManager(Path.home() / TESTIQ_CONFIG_DIR / \"baselines\")", "636": "    baseline_dir = baseline_mgr.baseline_dir / f\"{name}.json\"", "637": "", "638": "    if baseline_dir.exists():", "639": "        baseline_dir.unlink()", "640": "        console.print(f\"[green]\u2713 Baseline '{name}' deleted[/green]\")", "641": "    else:", "642": "        console.print(f\"[red]Baseline '{name}' not found[/red]\")", "643": "        sys.exit(1)"}, "tests/test_analysis.py": {"1": "\"\"\"", "2": "Tests for analysis module (quality scoring and recommendations).", "3": "\"\"\"", "4": "", "5": "import pytest", "6": "", "7": "from testiq.analysis import QualityAnalyzer, RecommendationEngine, QualityScore", "8": "from testiq.analyzer import CoverageDuplicateFinder", "9": "", "10": "", "11": "@pytest.fixture", "12": "def high_quality_finder():", "13": "    \"\"\"Create a finder with high-quality tests (few duplicates).\"\"\"", "14": "    finder = CoverageDuplicateFinder()", "15": "", "16": "    # 20 unique tests with good coverage", "17": "    for i in range(20):", "18": "        coverage = {", "19": "            f\"file{i % 5}.py\": list(range(i * 10 + 1, (i + 1) * 10 + 1)),  # Start from 1", "20": "            \"common.py\": [1, 2],  # Small overlap", "21": "        }", "22": "        finder.add_test_coverage(f\"test_unique_{i}\", coverage)", "23": "", "24": "    return finder", "25": "", "26": "", "27": "@pytest.fixture", "28": "def low_quality_finder():", "29": "    \"\"\"Create a finder with low-quality tests (many duplicates).\"\"\"", "30": "    finder = CoverageDuplicateFinder()", "31": "", "32": "    # 10 exact duplicates", "33": "    for i in range(10):", "34": "        finder.add_test_coverage(f\"test_duplicate_{i}\", {\"file.py\": [1, 2, 3]})", "35": "", "36": "    # 5 subset duplicates", "37": "    finder.add_test_coverage(\"test_short_1\", {\"utils.py\": [1, 2]})", "38": "    finder.add_test_coverage(\"test_long_1\", {\"utils.py\": [1, 2, 3, 4, 5]})", "39": "", "40": "    return finder", "41": "", "42": "", "43": "@pytest.fixture", "44": "def medium_quality_finder():", "45": "    \"\"\"Create a finder with medium-quality tests.\"\"\"", "46": "    finder = CoverageDuplicateFinder()", "47": "", "48": "    # Mix of unique and duplicate tests", "49": "    for i in range(5):", "50": "        lines = [1, 2, 3, i + 10] if i > 0 else [1, 2, 3, 10]  # Avoid line 0", "51": "        finder.add_test_coverage(f\"test_unique_{i}\", {f\"file{i}.py\": lines})", "52": "", "53": "    # A few duplicates", "54": "    finder.add_test_coverage(\"test_dup_1\", {\"common.py\": [10, 11, 12]})", "55": "    finder.add_test_coverage(\"test_dup_2\", {\"common.py\": [10, 11, 12]})", "56": "", "57": "    return finder", "58": "", "59": "", "60": "class TestQualityScoreClass:", "61": "    \"\"\"Tests for QualityScore dataclass.\"\"\"", "62": "", "63": "    def test_score_initialization(self):", "64": "        \"\"\"Test creating a quality score.\"\"\"", "65": "        score = QualityScore(", "66": "            overall_score=85.0,", "67": "            duplication_score=90.0,", "68": "            coverage_efficiency_score=80.0,", "69": "            uniqueness_score=85.0,", "70": "            grade=\"B+\",", "71": "            recommendations=[\"Sample recommendation\"],", "72": "        )", "73": "", "74": "        assert score.overall_score == 85.0", "75": "        assert score.grade == \"B+\"", "76": "", "77": "    def test_score_perfect(self):", "78": "        \"\"\"Test perfect quality score.\"\"\"", "79": "        score = QualityScore(", "80": "            overall_score=100.0,", "81": "            duplication_score=100.0,", "82": "            coverage_efficiency_score=100.0,", "83": "            uniqueness_score=100.0,", "84": "            grade=\"A+\",", "85": "            recommendations=[],", "86": "        )", "87": "", "88": "        assert score.overall_score == 100.0", "89": "        assert score.grade == \"A+\"", "90": "", "91": "", "92": "class TestQualityAnalyzer:", "93": "    \"\"\"Tests for QualityAnalyzer.\"\"\"", "94": "", "95": "    def test_quality_scores_across_all_grades(self, high_quality_finder, low_quality_finder, medium_quality_finder):", "96": "        \"\"\"Test quality scoring across high, medium, and low quality test suites with component validation.\"\"\"", "97": "        # High quality tests", "98": "        high_analyzer = QualityAnalyzer(high_quality_finder)", "99": "        high_score = high_analyzer.calculate_score(threshold=0.9)", "100": "        assert high_score.overall_score >= 80.0", "101": "        assert high_score.duplication_score >= 80.0", "102": "        assert high_score.grade in [\"A+\", \"A\", \"A-\", \"B+\", \"B\"]", "103": "        assert 0 <= high_score.overall_score <= 100", "104": "        assert 0 <= high_score.duplication_score <= 100", "105": "        assert 0 <= high_score.coverage_efficiency_score <= 100", "106": "        assert 0 <= high_score.uniqueness_score <= 100", "107": "", "108": "        # Low quality tests", "109": "        low_analyzer = QualityAnalyzer(low_quality_finder)", "110": "        low_score = low_analyzer.calculate_score(threshold=0.9)", "111": "        assert low_score.overall_score < 60.0", "112": "        assert low_score.duplication_score < 60.0", "113": "        assert low_score.grade in [\"D\", \"D+\", \"D-\", \"F\"]", "114": "", "115": "        # Medium quality with threshold variations", "116": "        med_analyzer = QualityAnalyzer(medium_quality_finder)", "117": "        med_score = med_analyzer.calculate_score(threshold=0.9)", "118": "        assert 60.0 <= med_score.overall_score <= 90.0", "119": "        assert med_score.grade in [\"B\", \"B+\", \"B-\", \"C+\", \"C\"]", "120": "        score_high_threshold = med_analyzer.calculate_score(threshold=0.95)", "121": "        score_low_threshold = med_analyzer.calculate_score(threshold=0.5)", "122": "        assert score_high_threshold is not None", "123": "        assert score_low_threshold is not None", "124": "", "125": "        # Verify score ordering", "126": "        assert high_score.overall_score > med_score.overall_score > low_score.overall_score", "127": "", "128": "    def test_empty_finder_score(self):", "129": "        \"\"\"Test quality score for empty test suite.\"\"\"", "130": "        finder = CoverageDuplicateFinder()", "131": "        analyzer = QualityAnalyzer(finder)", "132": "        score = analyzer.calculate_score(threshold=0.9)", "133": "", "134": "        # Empty suite gets F grade with message", "135": "        assert score.overall_score == 0", "136": "        assert score.duplication_score == 0", "137": "        assert score.grade == \"F\"", "138": "        assert \"No tests found\" in score.recommendations", "139": "", "140": "", "141": "class TestRecommendationEngine:", "142": "    \"\"\"Tests for RecommendationEngine.\"\"\"", "143": "", "144": "    def test_recommendation_engine_scenarios(self, low_quality_finder):", "145": "        \"\"\"Test comprehensive recommendation generation across quality scenarios.\"\"\"", "146": "        # Test 1: Low quality test suite with duplicates", "147": "        engine = RecommendationEngine(low_quality_finder)", "148": "        report = engine.generate_report(threshold=0.9)", "149": "", "150": "        # Should have recommendations", "151": "        assert len(report[\"recommendations\"]) > 0", "152": "", "153": "        # Should have high-priority recommendations due to low quality", "154": "        priorities = [r[\"priority\"] for r in report[\"recommendations\"]]", "155": "        assert \"high\" in priorities", "156": "", "157": "        # All recommendations should have valid priorities", "158": "        for rec in report[\"recommendations\"]:", "159": "            assert rec[\"priority\"] in [\"high\", \"medium\", \"low\"]", "160": "            assert \"message\" in rec", "161": "", "162": "        # Should recommend removing duplicates", "163": "        messages = [r[\"message\"].lower() for r in report[\"recommendations\"]]", "164": "        assert any(\"duplicate\" in msg for msg in messages)", "165": "", "166": "        # Test 2: Mixed quality scenarios", "167": "        finder_mixed = CoverageDuplicateFinder()", "168": "        finder_mixed.add_test_coverage(\"test_unique_1\", {\"file1.py\": [1, 2, 3]})", "169": "        finder_mixed.add_test_coverage(\"test_unique_2\", {\"file2.py\": [5, 6, 7]})", "170": "        finder_mixed.add_test_coverage(\"test_dup_1\", {\"common.py\": [10, 11]})", "171": "        finder_mixed.add_test_coverage(\"test_dup_2\", {\"common.py\": [10, 11]})", "172": "", "173": "        engine_mixed = RecommendationEngine(finder_mixed)", "174": "        report_mixed = engine_mixed.generate_report(threshold=0.9)", "175": "", "176": "        assert \"recommendations\" in report_mixed", "177": "        assert \"statistics\" in report_mixed", "178": "        assert len(report_mixed[\"recommendations\"]) > 0", "179": "", "180": "    def test_comprehensive_recommendation_workflow(self, high_quality_finder, medium_quality_finder):", "181": "        \"\"\"Test complete recommendation engine workflow including quality scoring, statistics, and priority handling.\"\"\"", "182": "        # Test 1: High quality suite with minimal recommendations", "183": "        finder = CoverageDuplicateFinder()", "184": "        for i in range(10):", "185": "            finder.add_test_coverage(", "186": "                f\"test_{i}\",", "187": "                {f\"file{i}.py\": list(range(i * 100 + 1, (i + 1) * 100 + 1))},", "188": "            )", "189": "        analyzer = QualityAnalyzer(finder)", "190": "        score = analyzer.calculate_score(threshold=0.9)", "191": "        engine = RecommendationEngine(finder)", "192": "        report = engine.generate_report(threshold=0.9)", "193": "        assert len(report[\"recommendations\"]) <= 1", "194": "        for rec in report[\"recommendations\"]:", "195": "            assert rec[\"priority\"] == \"low\"", "196": "", "197": "        # Test 2: Medium quality workflow with statistics", "198": "        analyzer2 = QualityAnalyzer(medium_quality_finder)", "199": "        score2 = analyzer2.calculate_score(threshold=0.9)", "200": "        assert score2 is not None", "201": "        assert 0 <= score2.overall_score <= 100", "202": "        engine2 = RecommendationEngine(medium_quality_finder)", "203": "        report2 = engine2.generate_report(threshold=0.9)", "204": "        assert \"recommendations\" in report2", "205": "        assert \"statistics\" in report2", "206": "        stats = report2[\"statistics\"]", "207": "        assert stats[\"total_tests\"] == len(medium_quality_finder.tests)", "208": "        exact_dups = medium_quality_finder.find_exact_duplicates()", "209": "        expected_dup_count = sum(len(g) - 1 for g in exact_dups)", "210": "        assert stats[\"exact_duplicates\"] == expected_dup_count", "211": "        for rec in report2[\"recommendations\"]:", "212": "            assert \"priority\" in rec", "213": "            assert \"message\" in rec", "214": "            assert len(rec[\"message\"]) > 0", "215": "", "216": "        # Test 3: Score influences recommendation priorities", "217": "        high_finder = CoverageDuplicateFinder()", "218": "        for i in range(10):", "219": "            high_finder.add_test_coverage(f\"test_high_{i}\", {f\"file{i}.py\": [i + 1, i + 2, i + 3]})", "220": "        high_engine = RecommendationEngine(high_finder)", "221": "        high_report = high_engine.generate_report(threshold=0.9)", "222": "", "223": "        low_finder = CoverageDuplicateFinder()", "224": "        for i in range(10):", "225": "            low_finder.add_test_coverage(f\"test_low_{i}\", {\"file.py\": [1, 2, 3]})", "226": "        low_engine = RecommendationEngine(low_finder)", "227": "        low_report = low_engine.generate_report(threshold=0.9)", "228": "        assert len(low_report[\"recommendations\"]) >= len(high_report[\"recommendations\"])", "229": "", "230": "    def test_empty_finder_recommendations(self):", "231": "        \"\"\"Test recommendations for empty finder.\"\"\"", "232": "        finder = CoverageDuplicateFinder()", "233": "        engine = RecommendationEngine(finder)", "234": "        report = engine.generate_report(threshold=0.9)", "235": "", "236": "        # Should handle empty finder gracefully", "237": "        assert \"recommendations\" in report", "238": "        assert \"statistics\" in report", "239": "        assert report[\"statistics\"][\"total_tests\"] == 0", "240": "", "241": "", "242": "class TestIntegration:", "243": "    \"\"\"Integration tests for analysis workflow.\"\"\"", "244": "", "245": ""}, "tests/test_source_reader.py": {"1": "\"\"\"Tests for source_reader module.\"\"\"", "2": "", "3": "from pathlib import Path", "4": "import tempfile", "5": "", "6": "import pytest", "7": "", "8": "from testiq.source_reader import SourceCodeReader", "9": "", "10": "", "11": "class TestSourceCodeReader:", "12": "    \"\"\"Tests for SourceCodeReader class.\"\"\"", "13": "", "14": "    def test_init(self):", "15": "        \"\"\"Test reader initialization.\"\"\"", "16": "        reader = SourceCodeReader()", "17": "        assert reader._cache == {}", "18": "", "19": "    def test_read_simple_file(self, tmp_path):", "20": "        \"\"\"Test reading a simple source file.\"\"\"", "21": "        # Create test file", "22": "        test_file = tmp_path / \"test.py\"", "23": "        test_file.write_text(\"line 1\\nline 2\\nline 3\\n\")", "24": "", "25": "        reader = SourceCodeReader()", "26": "        result = reader.read_file(str(test_file))", "27": "", "28": "        assert result is not None", "29": "        assert result[1] == \"line 1\"", "30": "        assert result[2] == \"line 2\"", "31": "        assert result[3] == \"line 3\"", "32": "        assert len(result) == 3", "33": "", "34": "    def test_read_file_with_trailing_whitespace(self, tmp_path):", "35": "        \"\"\"Test that trailing whitespace is stripped.\"\"\"", "36": "        test_file = tmp_path / \"test.py\"", "37": "        test_file.write_text(\"line 1   \\nline 2\\t\\t\\n\")", "38": "", "39": "        reader = SourceCodeReader()", "40": "        result = reader.read_file(str(test_file))", "41": "", "42": "        assert result[1] == \"line 1\"  # Trailing spaces removed", "43": "        assert result[2] == \"line 2\"  # Trailing tabs removed", "44": "", "45": "    def test_read_nonexistent_file(self):", "46": "        \"\"\"Test reading a non-existent file returns None.\"\"\"", "47": "        reader = SourceCodeReader()", "48": "        result = reader.read_file(\"/nonexistent/file.py\")", "49": "", "50": "        assert result is None", "51": "", "52": "    def test_caching(self, tmp_path):", "53": "        \"\"\"Test that files are cached after first read.\"\"\"", "54": "        test_file = tmp_path / \"test.py\"", "55": "        test_file.write_text(\"line 1\\nline 2\\n\")", "56": "", "57": "        reader = SourceCodeReader()", "58": "", "59": "        # First read", "60": "        result1 = reader.read_file(str(test_file))", "61": "        assert str(test_file) in reader._cache", "62": "", "63": "        # Modify file (shouldn't affect cached result)", "64": "        test_file.write_text(\"modified\\n\")", "65": "", "66": "        # Second read (should return cached version)", "67": "        result2 = reader.read_file(str(test_file))", "68": "        assert result2 == result1", "69": "        assert result2[1] == \"line 1\"  # Original content", "70": "", "71": "    def test_empty_file(self, tmp_path):", "72": "        \"\"\"Test reading an empty file.\"\"\"", "73": "        test_file = tmp_path / \"empty.py\"", "74": "        test_file.write_text(\"\")", "75": "", "76": "        reader = SourceCodeReader()", "77": "        result = reader.read_file(str(test_file))", "78": "", "79": "        assert result == {}", "80": "", "81": "    def test_unicode_content(self, tmp_path):", "82": "        \"\"\"Test reading file with unicode characters.\"\"\"", "83": "        test_file = tmp_path / \"unicode.py\"", "84": "        test_file.write_text(\"# \u4e2d\u6587\u6ce8\u91ca\\ndef foo():\\n    pass\\n\", encoding='utf-8')", "85": "", "86": "        reader = SourceCodeReader()", "87": "        result = reader.read_file(str(test_file))", "88": "", "89": "        assert result is not None", "90": "        assert \"\u4e2d\u6587\" in result[1]", "91": "", "92": "    def test_invalid_utf8_handled(self, tmp_path):", "93": "        \"\"\"Test that invalid UTF-8 is handled gracefully.\"\"\"", "94": "        test_file = tmp_path / \"binary.dat\"", "95": "        # Write some binary data that's not valid UTF-8", "96": "        test_file.write_bytes(b'\\x80\\x81\\x82\\x83')", "97": "", "98": "        reader = SourceCodeReader()", "99": "        result = reader.read_file(str(test_file))", "100": "", "101": "        # Should not crash, may return content with replacement chars", "102": "        assert result is not None or result is None", "103": "", "104": "    def test_one_indexed_lines(self, tmp_path):", "105": "        \"\"\"Test that line numbers are 1-indexed.\"\"\"", "106": "        test_file = tmp_path / \"test.py\"", "107": "        test_file.write_text(\"first\\nsecond\\nthird\\n\")", "108": "", "109": "        reader = SourceCodeReader()", "110": "        result = reader.read_file(str(test_file))", "111": "", "112": "        assert 1 in result", "113": "        assert 2 in result", "114": "        assert 3 in result", "115": "        assert 0 not in result", "116": "", "117": "    def test_read_multiple_files(self, tmp_path):", "118": "        \"\"\"Test read_multiple method.\"\"\"", "119": "        file1 = tmp_path / \"file1.py\"", "120": "        file2 = tmp_path / \"file2.py\"", "121": "        file1.write_text(\"file 1 line 1\\nfile 1 line 2\\n\")", "122": "        file2.write_text(\"file 2 line 1\\nfile 2 line 2\\n\")", "123": "", "124": "        reader = SourceCodeReader()", "125": "        result = reader.read_multiple([str(file1), str(file2)])", "126": "", "127": "        assert len(result) == 2", "128": "        assert result[str(file1)][1] == \"file 1 line 1\"", "129": "        assert result[str(file2)][1] == \"file 2 line 1\"", "130": "", "131": "    def test_read_multiple_with_nonexistent(self, tmp_path):", "132": "        \"\"\"Test read_multiple skips non-existent files.\"\"\"", "133": "        file1 = tmp_path / \"file1.py\"", "134": "        file1.write_text(\"file 1 line 1\\n\")", "135": "", "136": "        reader = SourceCodeReader()", "137": "        result = reader.read_multiple([str(file1), \"/nonexistent/file.py\"])", "138": "", "139": "        # Should only include the existing file", "140": "        assert len(result) == 1", "141": "        assert str(file1) in result", "142": "", "143": "    def test_read_multiple_empty_list(self):", "144": "        \"\"\"Test read_multiple with empty list.\"\"\"", "145": "        reader = SourceCodeReader()", "146": "        result = reader.read_multiple([])", "147": "", "148": "        assert result == {}", "149": "", "150": "    def test_multiple_files_cached(self, tmp_path):", "151": "        \"\"\"Test that multiple files can be cached separately.\"\"\"", "152": "        file1 = tmp_path / \"file1.py\"", "153": "        file2 = tmp_path / \"file2.py\"", "154": "        file1.write_text(\"file 1 content\\n\")", "155": "        file2.write_text(\"file 2 content\\n\")", "156": "", "157": "        reader = SourceCodeReader()", "158": "", "159": "        result1 = reader.read_file(str(file1))", "160": "        result2 = reader.read_file(str(file2))", "161": "", "162": "        assert len(reader._cache) == 2", "163": "        assert result1[1] == \"file 1 content\"", "164": "        assert result2[1] == \"file 2 content\"", "165": "", "166": "    def test_file_read_error_handling(self, tmp_path):", "167": "        \"\"\"Test that read errors return None gracefully.\"\"\"", "168": "        # Create a directory (not a file) to trigger error", "169": "        dir_path = tmp_path / \"not_a_file\"", "170": "        dir_path.mkdir()", "171": "", "172": "        reader = SourceCodeReader()", "173": "        result = reader.read_file(str(dir_path))", "174": "", "175": "        assert result is None"}, "src/testiq/logging_config.py": {"1": "\"\"\"", "2": "Structured logging configuration for TestIQ.", "3": "Provides enterprise-grade logging with rotation, levels, and multiple handlers.", "4": "\"\"\"", "5": "", "6": "import logging", "7": "import logging.handlers", "8": "import sys", "9": "from pathlib import Path", "10": "from typing import Optional", "11": "", "12": "", "13": "class StructuredFormatter(logging.Formatter):", "14": "    \"\"\"Custom formatter with structured output.\"\"\"", "15": "", "16": "    COLORS = {", "17": "        \"DEBUG\": \"\\033[36m\",  # Cyan", "18": "        \"INFO\": \"\\033[32m\",  # Green", "19": "        \"WARNING\": \"\\033[33m\",  # Yellow", "20": "        \"ERROR\": \"\\033[31m\",  # Red", "21": "        \"CRITICAL\": \"\\033[35m\",  # Magenta", "22": "        \"RESET\": \"\\033[0m\",", "23": "    }", "24": "", "25": "    def format(self, record: logging.LogRecord) -> str:", "26": "        \"\"\"Format log record with colors and structure.\"\"\"", "27": "        if hasattr(sys.stderr, \"isatty\") and sys.stderr.isatty():", "28": "            color = self.COLORS.get(record.levelname, self.COLORS[\"RESET\"])", "29": "            reset = self.COLORS[\"RESET\"]", "30": "            record.levelname = f\"{color}{record.levelname}{reset}\"", "31": "", "32": "        # Add context if available", "33": "        if hasattr(record, \"test_name\"):", "34": "            record.msg = f\"[{record.test_name}] {record.msg}\"", "35": "        if hasattr(record, \"file_path\"):", "36": "            record.msg = f\"[{record.file_path}] {record.msg}\"", "37": "", "38": "        return super().format(record)", "39": "", "40": "", "41": "def setup_logging(", "42": "    level: str = \"INFO\",", "43": "    log_file: Optional[Path] = None,", "44": "    enable_rotation: bool = True,", "45": "    max_bytes: int = 10 * 1024 * 1024,  # 10MB", "46": "    backup_count: int = 5,", "47": ") -> logging.Logger:", "48": "    \"\"\"", "49": "    Setup structured logging for TestIQ.", "50": "", "51": "    Args:", "52": "        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)", "53": "        log_file: Path to log file (if None, logs only to console)", "54": "        enable_rotation: Enable log file rotation", "55": "        max_bytes: Maximum size of log file before rotation", "56": "        backup_count: Number of backup log files to keep", "57": "", "58": "    Returns:", "59": "        Configured logger instance", "60": "    \"\"\"", "61": "    logger = logging.getLogger(\"testiq\")", "62": "    logger.setLevel(getattr(logging, level.upper()))", "63": "    logger.propagate = False", "64": "", "65": "    # Remove existing handlers", "66": "    logger.handlers.clear()", "67": "", "68": "    # Console handler with colors", "69": "    console_handler = logging.StreamHandler(sys.stderr)", "70": "    console_handler.setLevel(logging.INFO)", "71": "    console_formatter = StructuredFormatter(", "72": "        \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",", "73": "        datefmt=\"%Y-%m-%d %H:%M:%S\",", "74": "    )", "75": "    console_handler.setFormatter(console_formatter)", "76": "    logger.addHandler(console_handler)", "77": "", "78": "    # File handler with rotation", "79": "    if log_file:", "80": "        log_file.parent.mkdir(parents=True, exist_ok=True)", "81": "", "82": "        if enable_rotation:", "83": "            file_handler: logging.Handler = logging.handlers.RotatingFileHandler(", "84": "                log_file,", "85": "                maxBytes=max_bytes,", "86": "                backupCount=backup_count,", "87": "                encoding=\"utf-8\",", "88": "            )", "89": "        else:", "90": "            file_handler = logging.FileHandler(log_file, encoding=\"utf-8\")", "91": "", "92": "        file_handler.setLevel(logging.DEBUG)", "93": "        file_formatter = logging.Formatter(", "94": "            \"%(asctime)s | %(levelname)-8s | %(name)s | %(funcName)s:%(lineno)d | %(message)s\",", "95": "            datefmt=\"%Y-%m-%d %H:%M:%S\",", "96": "        )", "97": "        file_handler.setFormatter(file_formatter)", "98": "        logger.addHandler(file_handler)", "99": "", "100": "    return logger", "101": "", "102": "", "103": "def get_logger(name: str = \"testiq\") -> logging.Logger:", "104": "    \"\"\"Get or create a logger instance.\"\"\"", "105": "    return logging.getLogger(name)"}, "src/testiq/exceptions.py": {"1": "\"\"\"", "2": "Custom exceptions for TestIQ.", "3": "Provides detailed error codes and categories for better error handling.", "4": "\"\"\"", "5": "", "6": "", "7": "class TestIQError(Exception):", "8": "    \"\"\"Base exception for all TestIQ errors.\"\"\"", "9": "", "10": "    def __init__(self, message: str, error_code: str = \"TESTIQ_ERROR\") -> None:", "11": "        self.message = message", "12": "        self.error_code = error_code", "13": "        super().__init__(message)", "14": "", "15": "    def __str__(self) -> str:", "16": "        return f\"[{self.error_code}] {self.message}\"", "17": "", "18": "", "19": "class ConfigurationError(TestIQError):", "20": "    \"\"\"Configuration-related errors.\"\"\"", "21": "", "22": "    def __init__(self, message: str) -> None:", "23": "        super().__init__(message, \"CONFIG_ERROR\")", "24": "", "25": "", "26": "class ValidationError(TestIQError):", "27": "    \"\"\"Input validation errors.\"\"\"", "28": "", "29": "    def __init__(self, message: str) -> None:", "30": "        super().__init__(message, \"VALIDATION_ERROR\")", "31": "", "32": "", "33": "class SecurityError(TestIQError):", "34": "    \"\"\"Security-related errors.\"\"\"", "35": "", "36": "    def __init__(self, message: str) -> None:", "37": "        super().__init__(message, \"SECURITY_ERROR\")", "38": "", "39": "", "40": "class FileOperationError(TestIQError):", "41": "    \"\"\"File operation errors.\"\"\"", "42": "", "43": "    def __init__(self, message: str, filepath: str = \"\") -> None:", "44": "        self.filepath = filepath", "45": "        super().__init__(message, \"FILE_ERROR\")", "46": "", "47": "", "48": "class ParseError(TestIQError):", "49": "    \"\"\"Data parsing errors.\"\"\"", "50": "", "51": "    def __init__(self, message: str) -> None:", "52": "        super().__init__(message, \"PARSE_ERROR\")", "53": "", "54": "", "55": "class AnalysisError(TestIQError):", "56": "    \"\"\"Analysis operation errors.\"\"\"", "57": "", "58": "    def __init__(self, message: str) -> None:", "59": "        super().__init__(message, \"ANALYSIS_ERROR\")", "60": "", "61": "", "62": "class ResourceLimitError(TestIQError):", "63": "    \"\"\"Resource limit exceeded errors.\"\"\"", "64": "", "65": "    def __init__(self, message: str, limit_type: str = \"\") -> None:", "66": "        self.limit_type = limit_type", "67": "        super().__init__(message, \"RESOURCE_LIMIT_ERROR\")"}};
        let currentData = null;
        let syncEnabled = true;
        let isScrolling = false;
        
        function showComparison(index) {{
            const data = coverageData[index];
            if (!data) return;
            
            currentData = data;
            
            document.getElementById('subsetName').innerHTML = formatTestName(data.subsetName);
            document.getElementById('supersetName').innerHTML = formatTestName(data.supersetName);
            document.getElementById('coverageRatio').textContent = (data.ratio * 100).toFixed(1) + '%';
            
            // Populate file filter
            const allFiles = new Set([...Object.keys(data.subset), ...Object.keys(data.superset)]);
            const fileFilter = document.getElementById('fileFilter');
            fileFilter.innerHTML = '<option value="">All Files</option>';
            Array.from(allFiles).sort().forEach(file => {{
                const option = document.createElement('option');
                option.value = file;
                option.textContent = file;
                fileFilter.appendChild(option);
            }});
            
            renderBothPanels();
            
            document.getElementById('comparisonModal').style.display = 'block';
            
            // Scroll to top of the modal
            const splitView = document.querySelector('.split-view');
            if (splitView) {{
                splitView.scrollTop = 0;
            }}
        }}
        
        function renderBothPanels() {{
            const selectedFile = document.getElementById('fileFilter').value;
            
            // Get all unique files from both sides
            const subsetFiles = Object.keys(currentData.subset).sort();
            const supersetFiles = Object.keys(currentData.superset).sort();
            const allFiles = [...new Set([...subsetFiles, ...supersetFiles])].sort();
            
            // Apply file filter
            const filesToRender = selectedFile ? [selectedFile] : allFiles;
            
            let subsetHtml = '';
            let supersetHtml = '';
            
            for (const file of filesToRender) {{
                const subsetLines = currentData.subset[file] || [];
                const supersetLines = currentData.superset[file] || [];
                
                // Get all unique line numbers from both sides
                let allLineNums = [...new Set([...subsetLines, ...supersetLines])].sort((a, b) => a - b);
                
                if (allLineNums.length === 0) continue;
                
                const subsetLineSet = new Set(subsetLines);
                const supersetLineSet = new Set(supersetLines);
                const fileSource = sourceCode[file] || {};
                
                // Find and include method/class definitions for context
                const minLine = Math.min(...allLineNums);
                const maxLine = Math.max(...allLineNums);
                const contextLines = new Set(allLineNums);
                
                // Scan backwards from each covered line to find def/class
                for (const lineNum of allLineNums) {{
                    for (let i = lineNum - 1; i >= Math.max(1, minLine - 20); i--) {{
                        const line = fileSource[i] || '';
                        const trimmed = line.trim();
                        if (trimmed.startsWith('def ') || trimmed.startsWith('class ') || trimmed.startsWith('async def ')) {{
                            contextLines.add(i);
                            break;
                        }}
                        // Stop if we hit another definition or empty line followed by def
                        if (trimmed === '' && i < lineNum - 5) break;
                    }}
                }}
                
                // Convert back to sorted array
                allLineNums = Array.from(contextLines).sort((a, b) => a - b);
                
                // Add file headers
                subsetHtml += '<div class="file-section" style="margin-bottom: 30px;">';
                subsetHtml += '<div class="file-path">üìÑ ' + escapeHtml(file) + '</div>';
                
                supersetHtml += '<div class="file-section" style="margin-bottom: 30px;">';
                supersetHtml += '<div class="file-path">üìÑ ' + escapeHtml(file) + '</div>';
                
                // Render each line with gap detection
                let prevLineNum = null;
                let inDocstring = false;
                let docstringDelimiter = '';
                
                for (let idx = 0; idx < allLineNums.length; idx++) {{
                    const lineNum = allLineNums[idx];
                    const sourceLine = fileSource[lineNum] || '';
                    const trimmed = sourceLine.trim();
                    
                    // Track docstring state
                    if (trimmed.startsWith('"""') || trimmed.startsWith("'''")) {{
                        const delimiter = trimmed.startsWith('"""') ? '"""' : "'''";
                        if (!inDocstring) {{
                            // Starting a docstring
                            inDocstring = true;
                            docstringDelimiter = delimiter;
                            // Check if it's a single-line docstring
                            const afterDelimiter = trimmed.substring(3);
                            if (afterDelimiter.includes(delimiter)) {{
                                inDocstring = false; // Single-line docstring
                            }}
                            continue; // Skip docstring opening line
                        }} else if (inDocstring && delimiter === docstringDelimiter) {{
                            // Ending a docstring
                            inDocstring = false;
                            docstringDelimiter = '';
                            continue; // Skip docstring closing line
                        }}
                    }} else if (inDocstring) {{
                        // Skip content inside docstring
                        continue;
                    }}
                    
                    // Handle gap between lines
                    if (prevLineNum !== null && lineNum - prevLineNum > 1) {{
                        const gap = lineNum - prevLineNum - 1;
                        const gapStart = prevLineNum + 1;
                        const gapEnd = lineNum - 1;
                        
                        if (gap > 3) {{
                            // Show collapsible gap for >3 lines
                            const gapId = 'gap_' + file.replace(/[^a-zA-Z0-9]/g, '_') + '_' + gapStart + '_' + gapEnd;
                            const gapText = '... (' + gap + ' line' + (gap > 1 ? 's' : '') + ')';
                            
                            subsetHtml += '<div class="code-line gap-line" style="color: #000000; text-align: center; font-style: normal; background: transparent; cursor: pointer; padding: 2px 8px;" ';
                            subsetHtml += 'data-gap-id="' + gapId + '" data-gap-start="' + gapStart + '" data-gap-end="' + gapEnd + '" data-file="' + escapeHtml(file) + '" ';
                            subsetHtml += 'onclick="toggleGap(this, \'subset\')" title="Click to expand">';
                            subsetHtml += '<strong>' + gapText + '</strong>';
                            subsetHtml += '</div>';
                            
                            supersetHtml += '<div class="code-line gap-line" style="color: #000000; text-align: center; font-style: normal; background: transparent; cursor: pointer; padding: 2px 8px;" ';
                            supersetHtml += 'data-gap-id="' + gapId + '" data-gap-start="' + gapStart + '" data-gap-end="' + gapEnd + '" data-file="' + escapeHtml(file) + '" ';
                            supersetHtml += 'onclick="toggleGap(this, \'superset\')" title="Click to expand">';
                            supersetHtml += '<strong>' + gapText + '</strong>';
                            supersetHtml += '</div>';
                        }} else {{
                            // Show lines if gap is 3 or less
                            for (let gapLine = gapStart; gapLine <= gapEnd; gapLine++) {{
                                const gapSource = fileSource[gapLine] || '';
                                const gapLineNumStr = String(gapLine).padStart(4, ' ');
                                
                                subsetHtml += '<div class="code-line" style="opacity: 0.4; background: #fafafa;">';
                                subsetHtml += '<span style="color: #bbb; margin-right: 10px;">' + gapLineNumStr + '</span>';
                                subsetHtml += '<span style="color: #aaa;">' + escapeHtml(gapSource) + '</span>';
                                subsetHtml += '</div>';
                                
                                supersetHtml += '<div class="code-line" style="opacity: 0.4; background: #fafafa;">';
                                supersetHtml += '<span style="color: #bbb; margin-right: 10px;">' + gapLineNumStr + '</span>';
                                supersetHtml += '<span style="color: #aaa;">' + escapeHtml(gapSource) + '</span>';
                                supersetHtml += '</div>';
                            }}
                        }}
                    }}
                    
                    prevLineNum = lineNum;
                    const lineNumStr = String(lineNum).padStart(4, ' ');
                    const isDefLine = trimmed.startsWith('def ') || trimmed.startsWith('class ') || trimmed.startsWith('async def ');
                    
                    // Determine coverage status
                    const inSubset = subsetLineSet.has(lineNum);
                    const inSuperset = supersetLineSet.has(lineNum);
                    const inBoth = inSubset && inSuperset;
                    
                    // Render left side (subset)
                    if (inBoth) {{
                        // Both tests executed this line - GREEN
                        subsetHtml += '<div class="code-line covered-both">';
                        subsetHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        subsetHtml += escapeHtml(sourceLine) || '&nbsp;';
                        subsetHtml += '</div>';
                    }} else if (inSubset) {{
                        // Only subset executed this line - YELLOW
                        subsetHtml += '<div class="code-line covered-single">';
                        subsetHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        subsetHtml += escapeHtml(sourceLine) || '&nbsp;';
                        subsetHtml += '</div>';
                    }} else if (isDefLine) {{
                        // Show def/class lines as context
                        subsetHtml += '<div class="code-line" style="background: #e8f4f8; font-weight: 600;">';
                        subsetHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        subsetHtml += escapeHtml(sourceLine);
                        subsetHtml += '</div>';
                    }} else {{
                        // Show actual code dimmed for non-executed lines
                        subsetHtml += '<div class="code-line" style="opacity: 0.4; background: #fafafa;">';
                        subsetHtml += '<span style="color: #bbb; margin-right: 10px;">' + lineNumStr + '</span>';
                        subsetHtml += '<span style="color: #999;">' + escapeHtml(sourceLine) + '</span>';
                        subsetHtml += '</div>';
                    }}
                    
                    // Render right side (superset)
                    if (inBoth) {{
                        // Both tests executed this line - GREEN
                        supersetHtml += '<div class="code-line covered-both">';
                        supersetHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        supersetHtml += escapeHtml(sourceLine) || '&nbsp;';
                        supersetHtml += '</div>';
                    }} else if (inSuperset) {{
                        // Only superset executed this line - YELLOW
                        supersetHtml += '<div class="code-line covered-single">';
                        supersetHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        supersetHtml += escapeHtml(sourceLine) || '&nbsp;';
                        supersetHtml += '</div>';
                    }} else if (isDefLine) {{
                        // Show def/class lines as context
                        supersetHtml += '<div class="code-line" style="background: #e8f4f8; font-weight: 600;">';
                        supersetHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        supersetHtml += escapeHtml(sourceLine);
                        supersetHtml += '</div>';
                    }} else {{
                        // Show actual code dimmed for non-executed lines
                        supersetHtml += '<div class="code-line" style="opacity: 0.4; background: #fafafa;">';
                        supersetHtml += '<span style="color: #bbb; margin-right: 10px;">' + lineNumStr + '</span>';
                        supersetHtml += '<span style="color: #999;">' + escapeHtml(sourceLine) + '</span>';
                        supersetHtml += '</div>';
                    }}
                }}
                
                subsetHtml += '</div>';
                supersetHtml += '</div>';
            }}
            
            document.getElementById('subsetContent').innerHTML = subsetHtml || '<p style="padding: 20px; color: #7f8c8d;">No coverage data</p>';
            document.getElementById('supersetContent').innerHTML = supersetHtml || '<p style="padding: 20px; color: #7f8c8d;">No coverage data</p>';
        }}
        
        function applyFileFilter() {{
            renderBothPanels();
        }}
        
        function toggleSync() {{
            syncEnabled = !syncEnabled;
            const checkbox = document.getElementById('syncCheckbox');
            const toggle = document.getElementById('syncToggle');
            const splitView = document.querySelector('.split-view');
            const filePanels = document.querySelectorAll('.file-panel');
            
            checkbox.checked = syncEnabled;
            if (syncEnabled) {{
                toggle.classList.add('active');
                // Use single scroll - both panels scroll together
                splitView.classList.remove('independent');
                filePanels.forEach(panel => panel.classList.remove('independent'));
            }} else {{
                toggle.classList.remove('active');
                // Enable independent scrolling for each panel
                splitView.classList.add('independent');
                filePanels.forEach(panel => panel.classList.add('independent'));
            }}
        }}
        
        function toggleGap(element, side) {{
            const gapId = element.getAttribute('data-gap-id');
            const gapStart = parseInt(element.getAttribute('data-gap-start'));
            const gapEnd = parseInt(element.getAttribute('data-gap-end'));
            const file = element.getAttribute('data-file');
            
            // Find the corresponding gap in the other panel
            const otherSide = side === 'subset' ? 'superset' : 'subset';
            const otherContent = document.getElementById(otherSide + 'Content');
            const otherGap = otherContent.querySelector('.gap-line[data-gap-id="' + gapId + '"]');
            
            // Check if already expanded
            const isExpanded = element.classList.contains('expanded');
            
            if (isExpanded) {{
                // Collapse both sides - show gap element again
                const expandedLines = element.parentElement.querySelectorAll('.expanded-line[data-gap-id="' + gapId + '"]');
                expandedLines.forEach(line => line.remove());
                element.classList.remove('expanded');
                element.style.display = 'block';
                const gap = gapEnd - gapStart + 1;
                element.innerHTML = '<strong>... (' + gap + ' line' + (gap > 1 ? 's' : '') + ')</strong>';
                
                // Collapse other side
                if (otherGap) {{
                    const otherExpandedLines = otherGap.parentElement.querySelectorAll('.expanded-line[data-gap-id="' + gapId + '"]');
                    otherExpandedLines.forEach(line => line.remove());
                    otherGap.classList.remove('expanded');
                    otherGap.style.display = 'block';
                    otherGap.innerHTML = '<strong>... (' + gap + ' line' + (gap > 1 ? 's' : '') + ')</strong>';
                }}
            }} else {{
                // Expand both sides - hide gap element completely
                element.classList.add('expanded');
                element.style.display = 'none';
                
                const fileSource = sourceCode[file] || {};
                const subsetLineSet = new Set(currentData.subset[file] || []);
                const supersetLineSet = new Set(currentData.superset[file] || []);
                
                let subsetInsertHtml = '';
                let supersetInsertHtml = '';
                
                // Build in ASCENDING order from gapStart to gapEnd
                // Show ALL lines when gap is expanded (including docstrings, comments, etc.)
                for (let lineNum = gapStart; lineNum <= gapEnd; lineNum++) {{
                    const sourceLine = fileSource[lineNum] || '';
                    const lineNumStr = String(lineNum).padStart(4, ' ');
                    
                    const inSubset = subsetLineSet.has(lineNum);
                    const inSuperset = supersetLineSet.has(lineNum);
                    const inBoth = inSubset && inSuperset;
                    
                    // Make all expanded lines clickable to collapse
                    const clickHandler = 'onclick="toggleGap(document.querySelector(\'.gap-line[data-gap-id=\\\'' + gapId + '\\\']\'), \'subset\')" style="cursor: pointer;" title="Click to collapse"';
                    
                    // Always show actual source code, with color coding for execution status
                    // Build subset side HTML
                    if (inBoth) {{
                        subsetInsertHtml += '<div class="code-line expanded-line covered-both" data-gap-id="' + gapId + '" ' + clickHandler + '>';
                        subsetInsertHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        subsetInsertHtml += escapeHtml(sourceLine) || '&nbsp;';
                        subsetInsertHtml += '</div>';
                    }} else if (inSubset) {{
                        subsetInsertHtml += '<div class="code-line expanded-line covered-single" data-gap-id="' + gapId + '" ' + clickHandler + '>';
                        subsetInsertHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        subsetInsertHtml += escapeHtml(sourceLine) || '&nbsp;';
                        subsetInsertHtml += '</div>';
                    }} else {{
                        // Show actual code even when not executed, just dimmed
                        subsetInsertHtml += '<div class="code-line expanded-line" data-gap-id="' + gapId + '" style="opacity: 0.4; background: #fafafa; cursor: pointer;" onclick="toggleGap(document.querySelector(\'.gap-line[data-gap-id=\\\'' + gapId + '\\\']\'), \'subset\')" title="Click to collapse">';
                        subsetInsertHtml += '<span style="color: #bbb; margin-right: 10px;">' + lineNumStr + '</span>';
                        subsetInsertHtml += '<span style="color: #999;">' + escapeHtml(sourceLine) + '</span>';
                        subsetInsertHtml += '</div>';
                    }}
                    
                    // Build superset side HTML
                    if (inBoth) {{
                        supersetInsertHtml += '<div class="code-line expanded-line covered-both" data-gap-id="' + gapId + '" ' + clickHandler + '>';
                        supersetInsertHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        supersetInsertHtml += escapeHtml(sourceLine) || '&nbsp;';
                        supersetInsertHtml += '</div>';
                    }} else if (inSuperset) {{
                        supersetInsertHtml += '<div class="code-line expanded-line covered-single" data-gap-id="' + gapId + '" ' + clickHandler + '>';
                        supersetInsertHtml += '<span style="color: #999; margin-right: 10px;">' + lineNumStr + '</span>';
                        supersetInsertHtml += escapeHtml(sourceLine) || '&nbsp;';
                        supersetInsertHtml += '</div>';
                    }} else {{
                        // Show actual code even when not executed, just dimmed
                        supersetInsertHtml += '<div class="code-line expanded-line" data-gap-id="' + gapId + '" style="opacity: 0.4; background: #fafafa; cursor: pointer;" onclick="toggleGap(document.querySelector(\'.gap-line[data-gap-id=\\\'' + gapId + '\\\']\'), \'superset\')" title="Click to collapse">';
                        supersetInsertHtml += '<span style="color: #bbb; margin-right: 10px;">' + lineNumStr + '</span>';
                        supersetInsertHtml += '<span style="color: #999;">' + escapeHtml(sourceLine) + '</span>';
                        supersetInsertHtml += '</div>';
                    }}
                }}
                
                // Insert in correct order: create elements array first, then insert in REVERSE
                if (side === 'subset') {{
                    const tempDiv = document.createElement('div');
                    tempDiv.innerHTML = subsetInsertHtml;
                    const elementsArray = Array.from(tempDiv.children);
                    // Insert in REVERSE order to maintain ascending line numbers
                    for (let i = elementsArray.length - 1; i >= 0; i--) {{
                        element.parentNode.insertBefore(elementsArray[i], element.nextSibling);
                    }}
                }} else {{
                    const tempDiv = document.createElement('div');
                    tempDiv.innerHTML = supersetInsertHtml;
                    const elementsArray = Array.from(tempDiv.children);
                    // Insert in REVERSE order to maintain ascending line numbers
                    for (let i = elementsArray.length - 1; i >= 0; i--) {{
                        element.parentNode.insertBefore(elementsArray[i], element.nextSibling);
                    }}
                }}
                
                // Expand other side - hide it too
                if (otherGap) {{
                    otherGap.classList.add('expanded');
                    otherGap.style.display = 'none';
                    
                    const tempDiv2 = document.createElement('div');
                    if (otherSide === 'subset') {{
                        tempDiv2.innerHTML = subsetInsertHtml;
                    }} else {{
                        tempDiv2.innerHTML = supersetInsertHtml;
                    }}
                    const elementsArray2 = Array.from(tempDiv2.children);
                    // Insert in REVERSE order to maintain ascending line numbers
                    for (let i = elementsArray2.length - 1; i >= 0; i--) {{
                        otherGap.parentNode.insertBefore(elementsArray2[i], otherGap.nextSibling);
                    }}
                }}
            }}
        }}
        
        function closeModal() {{
            document.getElementById('comparisonModal').style.display = 'none';
            currentData = null;
        }}
        
        window.onclick = function(event) {{
            const modal = document.getElementById('comparisonModal');
            if (event.target == modal) {{
                closeModal();
            }}
        }}
        
        document.addEventListener('keydown', function(event) {{
            if (event.key === 'Escape') {{
                closeModal();
            }}
        }});
        
        // Format all test names on page load
        document.addEventListener('DOMContentLoaded', function() {{
            const testNames = document.querySelectorAll('.test-name');
            testNames.forEach(el => {{
                const originalText = el.textContent;
                if (originalText.includes('::')) {{
                    el.innerHTML = formatTestName(originalText);
                }}
            }});
            
            // Initial render after DOM is loaded and all functions are defined
            renderExactDuplicates(1);
            renderSimilarTests(1);
            renderSubsetDuplicates(1);
        }});
        </script>
        
        <div class="footer">
            <p>Generated by <strong>TestIQ v{version}</strong> on {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p>Analysis threshold: {threshold:.1%} | üîó <a href="https://github.com/pydevtools/TestIQ" style="color: #00c6ff;">github.com/pydevtools/TestIQ</a></p>
        </div>
    </div>
</body>
</html>
